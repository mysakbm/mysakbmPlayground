{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feed0f6e",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357b2e5",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "## Obsah\n",
    "- [Co je Spark a PySpark](#Co-je-Spark-a-PySpark)\n",
    "- [Instalace](#Instalace)\n",
    "- [PySpark sešny](#PySpark-sešny)\n",
    "- [Práce s daty](#Práce-s-daty)\n",
    "  - [Výroba tabulkoidního objektu](#Výroba-tabulkoidního-objektu) \n",
    "  - [Dataframový přístup](#Dataframový-přístup)\n",
    "    - [Informační metody](#Informační-metody)\n",
    "    - [Manipulace se sloupci](#Manipulace-se-sloupci)\n",
    "    - [PySpark a datumy](#PySpark-a-datumy)\n",
    "    - [Window funkce](#Window-funkce)\n",
    "  - [SQL přístup](#SQL-přístup)\n",
    "  - [Cachování](#Cachování)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c06718",
   "metadata": {},
   "outputs": [],
   "source": [
    "- něco o Spark UI\n",
    "- pysparková vizualizace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa2ddb4",
   "metadata": {},
   "source": [
    "## Co je Spark a PySpark\n",
    "Jeden počítač sice z hlediska výpočetní síly určitou práci zastane, více počítačů najednou je ale mocnějších. Přitom zde nemluvíme pouze o nějakých složitých algoritmech, takovýto poznatek je možná ještě očividnější při \"tupém\" procházení velkého množství dat řádek po řádku. Nicméně hromada počítačů/serverů (alias cluster) sama o sobě tolik nezmůže - musí být ošetřena komunikace a rozdělování práce mezi nimi. Tuto úlohu může krom jiných zastávat právě Spark. Alternativně lze cluster počítačů řídit nečím jiným, třeba [YARNem](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html), přičemž na tuto věc se Spark napojí.  \n",
    "Aplikace běžící ve Sparku se skládá z jednoho driver procesu a několika executor procesů. Driver proces má za úkol spravovat informace o sparkovské aplikaci, komunikovat s uživatelem a řídit a distribuovat práci executor procesů. Executory pak vykonávají práci jim zadanou a výsledky svých výpočtů vrací na driver. V případě našeho povídání, kdy si vše budeme ukazovat na lokálu, budou driver i executory de facto obyčejné procesy. V reálném prostředí by pravda šlo též o obyčejné procesy, ty by ale běžely na odlišných mašinách.  \n",
    "Spark je napsán ve Scale, což není úplně nejsnazší jazyk na pochopení. Nicméně byla by škoda o sílu tohoto frameworku přijít. Proto vznikl PySpark jako svého druhu pythoní obal nad Sparkem. Z hodně naivního pohledu připomíná PySpark Pandas (balíček pro práci s tabulkami) zkřížený s balíčkem Scikit-learn (balíček pro strojové učení). Velký rozdíl ale tkví ve způsobu vyhodnocování příkazů. V Pandách je vyhodnocování \"eager\" (asi nemá cenu tento termín překládat - výsledek by čtenáře jen mátl). To znamená, že když spustíme jakoukoli pandí funkci či metodu, provede se okamžitě. Oproti tomu PySpark je lazy. Některé příkazy - tzv. transformace - se při odklepnutí v IDE fakticky nespustí, pouze se přidají do grafu vyhodnocování transformací. Tento graf, resp. operace v něm, odstartuje až když je to potřeba, obvykle kvůli příkazům typu akce. Příkladem transformace mohou být všelikaké úpravy pysparkového dataframu, zatímco příkladem akce by bylo zobrazení tohoto dataframu uživateli. Důvod takovéhoto chování je redukce míry načítání obřích dataframů do paměti a zefektivnění práce s clustery serverů. Člověk musí mít toto chování pořád na paměti, jinak zjistí, že mu aplikace padá. Proč? Jelikož samotné výpočty (obvykle řetěz transformací s obříma datama) probíhá na executor procesech, driver procesy nemají alokovány tolik paměti. Pakliže ale na tyto velká data vypustíme nevhodnou akci (třeba akci \"vytiskni celou obří tabulku\"), tak všechny executory svá data pošlou na driver a ten to nerozchodí.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb774bf",
   "metadata": {},
   "source": [
    "## Instalace\n",
    "V praxi Pyspark buďto bydlí na Linuxu a je napojen na HDFS, anebo ho uživatel najde v cloudu, dejme tomu v Azure jako Databrics. Nicméně první by pro potřeby tohoto výukového textu znamenalo složitejší instalaci, druhé zase vytažení platební karty. Proto si zde povíme, jak PySpark nainstalovat na obyčejný počítač, kde jako operační systém slouží MS Windows.  \n",
    "Při instalaci jsem postupoval podle tohoto [návodu](https://sparkbyexamples.com/pyspark/install-pyspark-in-anaconda-jupyter-notebook/). Pokud by odkaz nebyl v budoucnu funkční, shrnu zde hlavní body:  \n",
    "- instalace [Anacondy](https://www.anaconda.com/)  \n",
    "- instalace Javy (tu jsem měl nainstalovanou již dříve, nicméně podle návodu by to šlo udělat i přes anacondí repozitář pomocí \"conda install openjdk\")  \n",
    "- instalace PySparku pomocí \"conda install pyspark\"  \n",
    "- instalace balíčku findspark  \n",
    "\n",
    "V mém případě toto bohužel nestačilo. Nejprve se při otestování funkčnosti (napsání \"pyspark\" do anacondí konzole) objevila hláška na způsob\n",
    "```\n",
    "Python was not found; run without arguments to install from the Microsoft Store\n",
    "```\n",
    "Bylo třeba klepnout na Start a psát (přesněji začít psát) \"Manage app execution aliases\". Po kliknutí na odpovídající položku ve Startu se v okně, které se objevilo, musely vypnout všechny věci vypadající, že mají nějakou spojitost s Pythonem.  \n",
    "Při následovném pokusu o spuštění PySparku se v konzoli vypsala zpráva\n",
    "```\n",
    "Missing Python executable 'python3', defaulting to 'C:\\Users\\ThereWouldBeUserName\\miniconda3\\Scripts\\..' for SPARK_HOME environment variable. Please install Python or specify the correct Python executable in PYSPARK_DRIVER_PYTHON or PYSPARK_PYTHON environment variable to detect SPARK_HOME safely.\n",
    "```\n",
    "Řešení spočívalo v nastavení proměnné PYSPARK_PYTHON. To se realizuje následujícím příkazem napsaným do konzole\n",
    "```\n",
    "set PYSPARK_PYTHON=python\n",
    "```\n",
    "Zdůrazněme, že takto nastavená proměnná existuje jen do té doby, dokud není konzole zavřena. Pokud bychom na tento krok zapomenuli a spustily Jupyter Notebook, projeví se to v nekonečném vytváření sparkové sešny (obvykle první věc, kterou budeme dělat po importování balíčků). Nicméně v konzoli, ve které jsme Jupyter pustili, by se objevila výše uvedená \"missing python executable\" chyba.  \n",
    "Zmiňme nakonec, že v konzoli jak při použití Jupyteru, tak při spuštění PySpaarku napřímo uvidíme chybu spojenou s hadoopem:\n",
    "```\n",
    "java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
    "```\n",
    "Ta naši práci neovlivní (žádný Hadoop u sebe nainstalovaný nemáme), takže ji můžeme ignorovat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542fb87",
   "metadata": {},
   "source": [
    "## PySpark sešny\n",
    "Pro komunikaci se sparkovským enginem, bez kterého se neobejdeme, je třeba vytvořit sparkovskou sešnu. To provedeme následujícím kódem (vnější sada závorek tu je jen proto, abychom mohli provádět odřádkování a neměli tak jeden nekonečný řádek):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22be7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"toy_application\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a080e",
   "metadata": {},
   "source": [
    "V tomto minimálním příkladu jsem sešnu pojmenovali (*appName*) a poté jsme zavolali metodu *getOrCreate*. Ta zajistí, že šesna bude singleton. Tj. pokud sešna neexistuje, bude vytvořena, ale pokud už nějaká sešna předtím vznikla, *getorCreate* vrátí právě tu. Toto chování si demonstrujme na zobrazení spark objektu a na vypsání jeho IDčka.  \n",
    "Když vložíme do buňka samostojící jméno sešny, dostaneme odkaz na Spark UI - de facto webový inteface, ve kterém můžeme běh sešny kontrolovat. Pod labelem \"Version\" se skrývá verze Sparku, \"Master\" nám zase řekne, kde vlasntě Spark bydlí. Důležité pro nás v tomto bodě je \"AppName\", kde vidíme jméno naší aplikace - toy_application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f21acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>toy_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x126a7660340>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602efb7",
   "metadata": {},
   "source": [
    "Vypišme si idčko spark objektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c4525b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881290530624"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f351a2",
   "metadata": {},
   "source": [
    "Když se nyní pokusíme vytvořit novou sparkovsou sešnu, můžeme se přesvědčit, že nám *getOrCreate* opravdu vrátil sešnu původní."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4113550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881290530624"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_2 = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"toy_application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "id(spark_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3107bd",
   "metadata": {},
   "source": [
    "Dokonce ani snaha o nastavení odlišného appNamu nepovede k vytvoření opravdu nové sešny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3caf020d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881290530624"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_3 = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"another_toy_application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "id(spark_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f267840",
   "metadata": {},
   "source": [
    "Vidíme, že nové jméno ani nedokázalo přepsat jméno staré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "035556e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>toy_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29eda4c6f40>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d119b10",
   "metadata": {},
   "source": [
    "Chceme-li z nějakého důvodu vytvořit sešnu novou, musíme na sešně staré zavolat *newSession*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "183bae48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881290526784"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_4 = spark.newSession()\n",
    "id(spark_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb526f",
   "metadata": {},
   "source": [
    "Nicméně takto vytvořená sešna není úplně oddělená od sešny původní. Reálně se hodí jen pokud bychom opravdu potřebovali mít dva od sebe oddělené namespacy, tj. pokud bychom potřebovali, aby se pod stejným jménem ve dvou sešnách nacházely jiné objekty. Koneckonců i v dokumentaci se píše, že \n",
    "```\n",
    "Returns a new SparkSession as new session, that has separate SQLConf,\n",
    "registered temporary views and UDFs, but shared SparkContext and\n",
    "table cache.\n",
    "```\n",
    "Zde se mluví o jakémsi SparkContextu. Pokud bychom na internetu narazili na staré tutoriály (před Spark 2.0), viděli bychom, že se v nich SparkContext používá jako vstup do sparkovského enginu namísto SparkSession. Jaký je mezi těmito entitami rozdíl? SparkContext se musí složitěji (a explicitněji) nastavovat a na celém JVM (Java Virtual Machine alias to, na čem jede Spark) může existovat jen jeden. Nicméně i když ho v našem uživatelském kódu nikde nevytváříme, v pozadí pořád existuje. Když například chceme sešnu ukončit a provoláme na ní metodu *stop*, zastavíme i všechny ostatní sešny - koneckonců zkume si poté, co jsme ukončili sešnu spark_4, otevřít odkazy na Spark UI z předešlých sešen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c1b41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_4.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76fb47f",
   "metadata": {},
   "source": [
    "Docstring v metodě *stop* totiž zmiňuje právě SparkContext:\n",
    "```\n",
    "Stop the underlying :class:`SparkContext`.\n",
    "```\n",
    "Jinak co se týče zastavování sešen, v našem případě, kdy pracujeme na lokálu, to potřeba není. Celý Spark se totiž vypne s vypnutím Jupyteru. Nicméně na normálním prostředí už by zastavování sešen po skončení práce bylo životně důležité, aby se neblokovaly hardwarové prostředky.  \n",
    "Tyto hardwarové prostředky, které bude naše aplikace používat, bychom mohli specifikovat při vytváření sešny explicitně\n",
    "```\n",
    "spark_example = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.executors.cores\", \"5\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"4G\")\n",
    "    .config(\"spark.driver.memory\",\"2G\")\n",
    "    .appName(\"another_toy_application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "```\n",
    "Asi častější ale bude, že někdo z infrastruktury pro nás vytvoří uživatelský pool s dedikovanými prostčedky, do kterého se přihlásíme variací na\n",
    "```\n",
    "spark_example = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.yaml.queue\", \"pool_data_scientist\")\n",
    "    .appName(\"another_toy_application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064993f",
   "metadata": {},
   "source": [
    "## Práce s daty\n",
    "Základní stavební jednotkou dat, se kterým Spark pracuje, je tzv. Resilient Distributed Dataset (RDD). Můžeme si ho představit jako svého druhu tuple, na kterém lze efektivně práci paralelizovat. Zdůrazněme, že RDD má blíž k tuplu než k listu - je immutable, tj. operace na něm proběhla ho nemění, nýbrž vrací zeditovanou kopii. Na každý pád dnes není moc důvodů s RDD pracovat - vznikl totiž datový objekt, se kterým se snadněji pracuje - DataFrame. Jméno připomíná základní stavební jednotku balíčku Pandas a podobnost není čistě náhodná - opravdu se jedná o reprezentaci tabulkoidního objektu. Naše povídání se tak bude celou dobu a zejména v této kapitole týkat tohoto typu objektu. Pro úplnost ještě poznamenejme, že ve Sparku existují tzv. Datasety, které ale nejsou v pySparku implementovány.\n",
    "\n",
    "### Výroba tabulkoidního objektu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e984a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"toy_application\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc067c1b",
   "metadata": {},
   "source": [
    "Asi nejpřímočařejším, byť ne úplně využívaným postupem na vytvoření pysparkového dataframu, je jeho konstrukce z dat zadaných v programu v podobě listu. Tento list se skládá z podlistů či z tuplů. Ty v budoucí tabulce budou reprezentovat jednotlivé řádky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d97971",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_list = [\n",
    "    [\"Švejk\",40,\"Praha\"], \n",
    "    [\"Vyskočil\",50,\"Horní Dolní\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f515358",
   "metadata": {},
   "source": [
    "List vložíme jako parametr do funkce *createDataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72081b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_list = spark.createDataFrame(data_in_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36432c",
   "metadata": {},
   "source": [
    "Výsledný dataframe si zobrazíme pomocí metody *show* (a ano, *show* je akce). Všimněme si, že jelikož jsme nespecifikovali hlavičku tabulky, objevila se nám posloupnost čísel \\_1, \\_2 atd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba541cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+\n",
      "|      _1| _2|         _3|\n",
      "+--------+---+-----------+\n",
      "|   Švejk| 40|      Praha|\n",
      "|Vyskočil| 50|Horní Dolní|\n",
      "+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_list.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d2fed",
   "metadata": {},
   "source": [
    "Pro vytvoření dataframu s hlavičkou je speciální parametr v *createDataFrame* - *schema* s listem obsahujícím jména sloupců."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1942bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+\n",
      "|    name|age|       city|\n",
      "+--------+---+-----------+\n",
      "|   Švejk| 40|      Praha|\n",
      "|Vyskočil| 50|Horní Dolní|\n",
      "+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_list = spark.createDataFrame(\n",
    "    data_in_list,\n",
    "    schema=[\"name\", \"age\", \"city\"]\n",
    ")\n",
    "table_from_list.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea0a87",
   "metadata": {},
   "source": [
    "Pysparkový dataframe lze vytvořit i z pandího dataframu - zkrátka se do *createDataFrame* namísto listu vloží právě onen pandí dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22de6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+\n",
      "|    name|age|       city|\n",
      "+--------+---+-----------+\n",
      "|   Švejk| 40|      Praha|\n",
      "|Vyskočil| 50|Horní Dolní|\n",
      "+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_dataframe = pd.DataFrame({\n",
    "    \"name\": [\"Švejk\", \"Vyskočil\"],\n",
    "    \"age\": [40, 50],\n",
    "    \"city\": [\"Praha\", \"Horní Dolní\"]\n",
    "})\n",
    "table_from_pandas = spark.createDataFrame(pandas_dataframe)\n",
    "table_from_pandas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5ef22",
   "metadata": {},
   "source": [
    "Pokud v *createDataFrame* uvedeme jména sloupců, mají tyto jména oproti těm v pandím dataframu prioritu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b5d0b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+\n",
      "|last_name|age|    address|\n",
      "+---------+---+-----------+\n",
      "|    Švejk| 40|      Praha|\n",
      "| Vyskočil| 50|Horní Dolní|\n",
      "+---------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_dataframe = pd.DataFrame({\n",
    "    \"name\": [\"Švejk\", \"Vyskočil\"],\n",
    "    \"age\": [40, 50],\n",
    "    \"city\": [\"Praha\", \"Horní Dolní\"]\n",
    "})\n",
    "table_from_pandas = spark.createDataFrame(\n",
    "    pandas_dataframe,\n",
    "    schema=[\"last_name\", \"age\", \"address\"]\n",
    ")\n",
    "table_from_pandas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bea49f",
   "metadata": {},
   "source": [
    "Pro úplnost zmiňme, že pokud bychom potřebovali konverzi opačným směrem, tj. z PySparku do Pand, použijeme metodu *toPandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94d06ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Švejk</td>\n",
       "      <td>40</td>\n",
       "      <td>Praha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vyskočil</td>\n",
       "      <td>50</td>\n",
       "      <td>Horní Dolní</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  last_name  age      address\n",
       "0     Švejk   40        Praha\n",
       "1  Vyskočil   50  Horní Dolní"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pandas_frame = table_from_pandas.toPandas()\n",
    "new_pandas_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb08ca6",
   "metadata": {},
   "source": [
    "Načíst data jde pochopitelně i z csv souboru. Nicméně popravdě metoda, kterou si zde ukážeme, nebudeme asi fungovat, když ono csvčko nebude na normálním disku, ale někde na HDFS.  \n",
    "Načtení realizujeme s pomocí *read.csv*. Parametry jsou obdobné jaké v pandím *read_csv*. Zdůrazněme ale, že v parametru *path* specifikující cestu k souboru musí být string (resp. list stringů), nikoli pathlib.Path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19149ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv = spark.read.csv(path=\"pyspark_data/csv/data_for_csv_load.csv\", sep=\",\", header=True)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e9710",
   "metadata": {},
   "source": [
    "Takto to vypadá neproblematicky. Nicméně zkusme na náš dataframe vypustit metodu *describe*, který ukáže datové typy sloupců. Uvidíme, že jsou všechny brány jako stringy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa20a9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, id: string, first_name: string, last_name: string, age: string, gender: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b63cc2",
   "metadata": {},
   "source": [
    "Takové je chování, které nastává při nastavení parametru *inferSchema* na hodnotu None resp. False. Co dělat, kdybychom chtěli mít rozumné datové typy už od načtení? Když se *inferSchema* položí rovno True, PySpark načte prvních pár řádků ze souboru a z nich datový typ odvodí. Problém nastane v okamžiku, kdy se první řádky liší od řádků následujících a je tak automaticky zvolen nevhodný datový typ. Anebo se \"zbytečně\" kvůli typům načte celý soubor, což může trvat docela dlouho. Proto bývá vhodné definovat datový typ explicitně. Na to použijeme objekt typu *StructType* reprezentující schéma celého dataframu. Tento *StructType* obsahuje objekty *StructField* obsahující informace o jednotlivých sloupcích. První parametr konstruktoru *StructField* obsahuje jméno sloupce, druhý parametr datový typ. Třetí parametr by měl dle [dokumentace](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.StructField.html) obdsahovat informaci, zda pole může být nullable. Nicméně při mých pokusech na Sparku 3 se tento atribut po vytvoření dataframu nezávisle na tom, co jsem do konstuktoru vložil, automaticky nastavil na True (zjištěno prostřednictvím *table_from_csv.schema*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84e1b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_table_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", DoubleType()),\n",
    "    StructField(\"gender\", StringType())\n",
    "])\n",
    "\n",
    "table_from_csv = spark.read.csv(\n",
    "    path=\"pyspark_data/csv/data_for_csv_load.csv\", sep=\",\", \n",
    "    header=True, schema=file_table_schema\n",
    ")\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be9b8d1",
   "metadata": {},
   "source": [
    "Když nějaký sloupec dostane špatný datový typ (například \"age\" vs souboru je ve fromátu 25.0, tj. je to float, ale my mu dáme IntegerType), celý sloupec se naplnít nully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5bb592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|null|     M|\n",
      "|200|      Mary|    Shelly|null|     F|\n",
      "|300|    Johann|    Geothe|null|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|null|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "file_table_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"gender\", StringType())\n",
    "])\n",
    "\n",
    "table_from_csv = spark.read.csv(\n",
    "    path=\"pyspark_data/csv/data_for_csv_load.csv\", \n",
    "    sep=\",\", header=True, schema=file_table_schema\n",
    ")\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7ec09",
   "metadata": {},
   "source": [
    "Výše jsme zmínili, že při načítání souborů mužeme uvést nikoli pouze string, ale i list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b4c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "|160|     Isaac|    Asimov|48.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_files_list = [\n",
    "    \"pyspark_data/csv/data_for_csv_load.csv\", \n",
    "    \"pyspark_data/csv/data_for_csv_load_2.csv\"\n",
    "]\n",
    "table_from_csv = spark.read.csv(path=csv_files_list, sep=\",\", header=True)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1f13c",
   "metadata": {},
   "source": [
    "Nicméně musíme si dát pozor na to, aby soubory měly stejná schémata. V opačném případě se použije schéma jednoho ze souborů. U dat ze souborů ostatních se neexistující sloupce doplní nully a nadbytečné sloupce se useknou. Je otázkou, co určuje schéma dominující soubor - pořadí v listu to není."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b5dbe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "|160|     Isaac|    Asimov|48.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_files_list = [\n",
    "    \"pyspark_data/csv/data_for_csv_load.csv\", \n",
    "    \"pyspark_data/csv/data_for_csv_load_3.csv\"\n",
    "]\n",
    "table_from_csv = spark.read.csv(path=csv_files_list, sep=\",\", header=True)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b8276",
   "metadata": {},
   "source": [
    "Údajně by dokonce mělo stačit do pathy vložit adresář obsahující csv soubory a do dataframu by se měl obsah všech souborů načíst. Nicméně aspoň na lokálu se mi to nepodařilo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abe12e",
   "metadata": {},
   "source": [
    "Uložení dataframu do csv souboru by se podle dokumentace mělo provést pomocí\n",
    "```\n",
    "table_from_csv.write.csv(\"data_after_saving_to_csv.csv\", sep=\"|\")\n",
    "```\n",
    "Když to ale provedeme u sebe na lokále na Windows, obdržíme chybovou hlášku mluvící o Hadoopu. Proto pokud chceme dataframe uložit na (normální) disk, bude asi nejsnazší ho napřed zkonvertovat do pandí verze pomocí toPandas a poté uložit pandí metodou *to_csv*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab3b36",
   "metadata": {},
   "source": [
    "Mějme json o obsahu\n",
    "```json\n",
    "{\"first_name\": \"John\", \"last_name\": \"Doe\"},\n",
    "{\"first_name\": \"Jane\", \"last_name\": \"Doe\"}\n",
    "```\n",
    "Do dataframu ho načteme příkazem *spark.read.json*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d58f7337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "|      Jane|      Doe|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(path=\"pyspark_data/json/data_for_json_load.json\")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ffa3f",
   "metadata": {},
   "source": [
    "Nicméně pokud budou záznamy roztaženy na více než jeden řádek, např. takto:\n",
    "```json\n",
    "{\n",
    "  \"first_name\": \"John\", \n",
    "  \"last_name\": \"Doe\"\n",
    "},\n",
    "{\n",
    "  \"first_name\": \"Jane\", \n",
    "  \"last_name\": \"Doe\"\n",
    "}\n",
    "```\n",
    "stejný kód nám spadne s chybovou hláškou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "071014af",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e4d4c6a839c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtable_from_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pyspark_data/json/data_for_json_load_multiline.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtable_from_json\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\newnotebook\\miniconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\newnotebook\\miniconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\newnotebook\\miniconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(path=\"pyspark_data/json/data_for_json_load_multiline.json\")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375b2ad",
   "metadata": {},
   "source": [
    "Při pohledu do dokumentace by se zdálo, že řešením je použití parametru multiLine s hodnotou True. Jenže ouha, v takto vytvořené tabulce je jen jeden záznam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96c70226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(\n",
    "    path=\"pyspark_data/json/data_for_json_load_multiline.json\",\n",
    "    multiLine=True\n",
    ")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24bc75d",
   "metadata": {},
   "source": [
    "Aby Spark správně multiline jsony interpretoval, musí tyto začínat a končit hranatými závorkami, tj. musí vypadat takto:\n",
    "```json\n",
    "[{\n",
    "  \"first_name\": \"John\", \n",
    "  \"last_name\": \"Doe\"\n",
    "},\n",
    "{\n",
    "  \"first_name\": \"Jane\", \n",
    "  \"last_name\": \"Doe\"\n",
    "}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc5ce476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "|      Jane|      Doe|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(\n",
    "    path=\"pyspark_data/json/data_for_json_load_multiline_2.json\",\n",
    "    multiLine=True\n",
    ")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd70d2a",
   "metadata": {},
   "source": [
    "Hnízděné jsony lze načíst též, ale poté musí následovat zprocesování dataframu do použitelnější podoby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ba97e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|             address|first_name|last_name|\n",
      "+--------------------+----------+---------+\n",
      "|[{Prague, Some st...|      John|      Doe|\n",
      "|[{Brno, Some stre...|      Jane|      Doe|\n",
      "+--------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(\n",
    "    path=\"pyspark_data/json/data_for_json_load_complicated.json\",\n",
    ")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d32ae",
   "metadata": {},
   "source": [
    "### Dataframový přístup\n",
    "#### Informační metody\n",
    "V této podkapitole se budeme věnovat operacím s dataframy. Víceméně nám půjde o to najít ekvivalenty k pandasím konstrukcím.  \n",
    "V předcházející podkapitole jsme viděli, že se dataframe (přesněji prvních pár řádků) dá zobrazit pomocí metody *show*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00266951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv = spark.read.csv(path=\"pyspark_data/csv/data_for_csv_load.csv\", sep=\",\", header=True)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801bd12",
   "metadata": {},
   "source": [
    "V případě, že chceme vidět jen prvních N řádků, vložíme toto číslo N jako parametr do *show*. Ten bude tudíž ekvivalentní pandasímu *head*. Defaultní hodnota tohoto parametru nesoucího jméno *n* je 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cff86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+\n",
      "| id|first_name|last_name| age|gender|\n",
      "+---+----------+---------+----+------+\n",
      "|100|    Victor|     Hugo|25.0|     M|\n",
      "|200|      Mary|   Shelly|30.0|     F|\n",
      "+---+----------+---------+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b1e12",
   "metadata": {},
   "source": [
    "Pakliže je délka řetězce v některém sloupci delší než 20 znaků, je z hlediska toho, co zobrazuje *show*, tento řetězec oříznut (reálně se ale obsah buňky neztrácí)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a418e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+\n",
      "|numbers_1|numbers_2|           long_word|\n",
      "+---------+---------+--------------------+\n",
      "|        0|        0|                    |\n",
      "|        1|       10|                 abc|\n",
      "|        2|       20|              abcabc|\n",
      "|        3|       30|           abcabcabc|\n",
      "|        4|       40|        abcabcabcabc|\n",
      "|        5|       50|     abcabcabcabcabc|\n",
      "|        6|       60|  abcabcabcabcabcabc|\n",
      "|        7|       70|abcabcabcabcabcab...|\n",
      "|        8|       80|abcabcabcabcabcab...|\n",
      "|        9|       90|abcabcabcabcabcab...|\n",
      "|       10|      100|abcabcabcabcabcab...|\n",
      "|       11|      110|abcabcabcabcabcab...|\n",
      "|       12|      120|abcabcabcabcabcab...|\n",
      "|       13|      130|abcabcabcabcabcab...|\n",
      "|       14|      140|abcabcabcabcabcab...|\n",
      "|       15|      150|abcabcabcabcabcab...|\n",
      "|       16|      160|abcabcabcabcabcab...|\n",
      "|       17|      170|abcabcabcabcabcab...|\n",
      "|       18|      180|abcabcabcabcabcab...|\n",
      "|       19|      190|abcabcabcabcabcab...|\n",
      "+---------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_list = [\n",
    "    [number, number*10, \"abc\"*number] for number in range(100)\n",
    "]\n",
    "numbers_frame = spark.createDataFrame(\n",
    "    numbers_list,\n",
    "    schema=[\"numbers_1\", \"numbers_2\", \"long_word\"]\n",
    ")\n",
    "numbers_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f855cb4",
   "metadata": {},
   "source": [
    "Pokud ořezávání chceme vypnout, přidáme do *show* parametr *truncate* s hodnotou False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eced5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------------------------------------------------+\n",
      "|numbers_1|numbers_2|long_word                                                |\n",
      "+---------+---------+---------------------------------------------------------+\n",
      "|0        |0        |                                                         |\n",
      "|1        |10       |abc                                                      |\n",
      "|2        |20       |abcabc                                                   |\n",
      "|3        |30       |abcabcabc                                                |\n",
      "|4        |40       |abcabcabcabc                                             |\n",
      "|5        |50       |abcabcabcabcabc                                          |\n",
      "|6        |60       |abcabcabcabcabcabc                                       |\n",
      "|7        |70       |abcabcabcabcabcabcabc                                    |\n",
      "|8        |80       |abcabcabcabcabcabcabcabc                                 |\n",
      "|9        |90       |abcabcabcabcabcabcabcabcabc                              |\n",
      "|10       |100      |abcabcabcabcabcabcabcabcabcabc                           |\n",
      "|11       |110      |abcabcabcabcabcabcabcabcabcabcabc                        |\n",
      "|12       |120      |abcabcabcabcabcabcabcabcabcabcabcabc                     |\n",
      "|13       |130      |abcabcabcabcabcabcabcabcabcabcabcabcabc                  |\n",
      "|14       |140      |abcabcabcabcabcabcabcabcabcabcabcabcabcabc               |\n",
      "|15       |150      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabc            |\n",
      "|16       |160      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabc         |\n",
      "|17       |170      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabc      |\n",
      "|18       |180      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabc   |\n",
      "|19       |190      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabc|\n",
      "+---------+---------+---------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2dbdcd",
   "metadata": {},
   "source": [
    "Zdůrazněme, že *show* slouží opravdu jen na zobrazení prvních pár řádků. Pokud ale potřebujeme z dataframu prvních několik řádků vybrat, aby se s nimi posléze dály nějaké další operace, musí se použít metoda *limit*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72004194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+\n",
      "|numbers_1|numbers_2|   long_word|\n",
      "+---------+---------+------------+\n",
      "|        0|        0|            |\n",
      "|        1|       10|         abc|\n",
      "|        2|       20|      abcabc|\n",
      "|        3|       30|   abcabcabc|\n",
      "|        4|       40|abcabcabcabc|\n",
      "+---------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44057b",
   "metadata": {},
   "source": [
    "Nechat si zobrazit prvních pár řádků bývá užitečné, avšak dobré je dostat se i k nějakým informacím popisujícím celý dataframe. Například pokud chceme znát schéma dataframu (aka datové typy sloupců), použijeme metodu *printSchema*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b907be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numbers_1: long (nullable = true)\n",
      " |-- numbers_2: long (nullable = true)\n",
      " |-- long_word: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f389ccf",
   "metadata": {},
   "source": [
    "Shrnující statistiky přináší metoda *summary*. Jelikož vrací dataframe, musí se za její provolání nalepit *show*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "163909a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+--------------------+\n",
      "|summary|         numbers_1|        numbers_2|           long_word|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "|  count|               100|              100|                 100|\n",
      "|   mean|              49.5|            495.0|                null|\n",
      "| stddev|29.011491975882016|290.1149197588202|                null|\n",
      "|    min|                 0|                0|                    |\n",
      "|    25%|                24|              240|                null|\n",
      "|    50%|                49|              490|                null|\n",
      "|    75%|                74|              740|                null|\n",
      "|    max|                99|              990|abcabcabcabcabcab...|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41d132d",
   "metadata": {},
   "source": [
    "Parametry *summary* jsou chtěné statistiky. Defaultně se ukazuje téměř vše, co *summary* dokáže. Výjimkou je snad jen možnost specifikovat chtěný percentil: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e909f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+---------+\n",
      "|summary|numbers_1|numbers_2|long_word|\n",
      "+-------+---------+---------+---------+\n",
      "|    10%|        9|       90|     null|\n",
      "+-------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.summary(\"10%\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6606db",
   "metadata": {},
   "source": [
    "Extrémně podobnou funkcí je *describe*. Zde se ale jako parametry neudávají statistiky, ale sloupce, nad kterými se statistiky mají napočítat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b090a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+--------------------+\n",
      "|summary|         numbers_1|        numbers_2|           long_word|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "|  count|               100|              100|                 100|\n",
      "|   mean|              49.5|            495.0|                null|\n",
      "| stddev|29.011491975882016|290.1149197588202|                null|\n",
      "|    min|                 0|                0|                    |\n",
      "|    max|                99|              990|abcabcabcabcabcab...|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1b1f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+\n",
      "|summary|        numbers_2|           long_word|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|              100|                 100|\n",
      "|   mean|            495.0|                null|\n",
      "| stddev|290.1149197588202|                null|\n",
      "|    min|                0|                    |\n",
      "|    max|              990|abcabcabcabcabcab...|\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.describe(\"numbers_2\", \"long_word\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a19e8",
   "metadata": {},
   "source": [
    "Pro získání počtu řádků dataframu se v PySparku nepoužije *len*, nýbrž metoda *count*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ccf2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_frame.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98dbf00",
   "metadata": {},
   "source": [
    "#### Manipulace se sloupci\n",
    "PySparkové datasety jsou immutable. To znamená, že je jejich metody nemění, nýbrž po svém doběhnutí vrací nový dataframe. Ukažme si to na metodě přidávající/přepisující sloupec *withColumn*. Jejím prvním parametrem je název nového/přepisovaného sloupce, parametrem druhým pak hodnoty onoho sloupce. Pro jejich vytvoření budeme obvykle potřebovat funkce z *pyspark.sql.functions*. Kupříklad na šáhnutí na jiný sloupec se použije funkce *functions.col*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824062cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "table_from_csv.withColumn(\"age_2\", f.col(\"age\")+10)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9a25f",
   "metadata": {},
   "source": [
    "Vidíme že tabulka je pořád stejná. Když chceme její o sloupec bohatší verzi, musíme do proměnné vložit dataframe z výstupu metody. Připomeňme, že withColumn je transformace, tudíž aby se něco vůbec udělalo (nejen zobrazilo, ale aby opravdu PySpark udělal něco jiného než si jen transformaci zapsal do sady úkolů), musí se zavolat akce - v našem případě metoda *show*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c235d2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+-----+\n",
      "| id|first_name| last_name| age|gender|age_2|\n",
      "+---+----------+----------+----+------+-----+\n",
      "|100|    Victor|      Hugo|25.0|     M| 35.0|\n",
      "|200|      Mary|    Shelly|30.0|     F| 40.0|\n",
      "|300|    Johann|    Geothe|75.0|     M| 85.0|\n",
      "|400|    Albert|     Camus|null|     M| null|\n",
      "|500|   William|Shakespear|38.0|     M| 48.0|\n",
      "+---+----------+----------+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv = table_from_csv.withColumn(\"age_2\", f.col(\"age\")+10)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e5331",
   "metadata": {},
   "source": [
    "Pro vyhození sloupce slouží metoda *drop*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec9340b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv = table_from_csv.drop(\"age_2\")\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e96a5",
   "metadata": {},
   "source": [
    "Sloupce přejmenujeme pomocí *withColumnRenamed*, kde prvním parametrem je staré jméno sloupce a parametrem druhým jméno nové:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0683e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------+----+------+\n",
      "| id|first_name|author_last_name| age|gender|\n",
      "+---+----------+----------------+----+------+\n",
      "|100|    Victor|            Hugo|25.0|     M|\n",
      "|200|      Mary|          Shelly|30.0|     F|\n",
      "|300|    Johann|          Geothe|75.0|     M|\n",
      "|400|    Albert|           Camus|null|     M|\n",
      "|500|   William|      Shakespear|38.0|     M|\n",
      "+---+----------+----------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.withColumnRenamed(\"last_name\", \"author_last_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29bcc20",
   "metadata": {},
   "source": [
    "Občas je potřeba provést konverzi datového typu sloupce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0481005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: string (nullable = true)\n",
      " |-- numbers: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_types_list = [\n",
    "    [\"zero\", \"0\"],\n",
    "    [\"one\", \"1\"],\n",
    "    [\"two\", \"2\"]\n",
    "]\n",
    "data_types_frame = spark.createDataFrame(\n",
    "    data_types_list,\n",
    "    schema=[\"words\", \"numbers\"]\n",
    ")\n",
    "data_types_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f799b5b",
   "metadata": {},
   "source": [
    "To se provede s použitím metody *cast*, která obsahuje chtěný datový typ a která je vypuštěna na příslušný sloupec vybraný pomocí *functions.col*. Efektivně starý sloupec přepisujeme novým:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd345e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: string (nullable = true)\n",
      " |-- numbers: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "data_types_frame = data_types_frame.withColumn(\n",
    "    \"numbers\",\n",
    "    f.col(\"numbers\").cast(IntegerType())\n",
    ")\n",
    "data_types_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a731908",
   "metadata": {},
   "source": [
    "Pokud bychom chtěli z nějakého důvodu dostat najednou data z celého dataframu, použijeme metodu *collect*. Všimněte si na níže uvedeném příkladu, že tato metoda vrací list řádků.  \n",
    "Chceme ale opravdu najednou dostat data z celého dataframu? No, obvykle ne. Znamená to totiž, že se z jednotlivých executorů pošlou data na driver, což pro dostatečně velký dataframe vyústí v \"OutOfMemory\" error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa31d71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='100', first_name='Victor', last_name='Hugo', age='25.0', gender='M'),\n",
       " Row(id='200', first_name='Mary', last_name='Shelly', age='30.0', gender='F'),\n",
       " Row(id='300', first_name='Johann', last_name='Geothe', age='75.0', gender='M'),\n",
       " Row(id='400', first_name='Albert', last_name='Camus', age=None, gender='M'),\n",
       " Row(id='500', first_name='William', last_name='Shakespear', age='38.0', gender='M')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb051b3",
   "metadata": {},
   "source": [
    "Každopádně pokud už hodláme data získat pomocí collectu, můžeme z nich vyzobnout jednu buňku s pomocí indexů. Takový postup je pak vzdáleně podobný pandasímu *iloc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd2b1ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'William'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.collect()[4][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcfa91",
   "metadata": {},
   "source": [
    "Jak tedy provádět vybírání sloupců a řádků správně? Pro výběr sloupců se použije metoda select. Zdůrazněme, že narozdíl od Pandas se tu u více sloupců neočekává, že budou v listu, ale jdou jeden za druhým jako obyčejné argumenty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "276b97fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|first_name| last_name|\n",
      "+----------+----------+\n",
      "|    Victor|      Hugo|\n",
      "|      Mary|    Shelly|\n",
      "|    Johann|    Geothe|\n",
      "|    Albert|     Camus|\n",
      "|   William|Shakespear|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.select(\"first_name\", \"last_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd728c6",
   "metadata": {},
   "source": [
    "Pro vybrání řádků na základě hodnoty se použije metoda *filter* (resp. *where* - jedná se o alias pro *filter*). Jelikož pro potřeby takovéto operace je nutno na sloupce šáhnout, musíme opět použít metodu *col* z *pyspark.sql.functions*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "929da9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(f.col(\"age\")>=30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4127c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(f.col(\"last_name\")==\"Shakespear\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38087dbb",
   "metadata": {},
   "source": [
    "Pro znegování podmínky slouží vlnka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0344719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+\n",
      "| id|first_name|last_name| age|gender|\n",
      "+---+----------+---------+----+------+\n",
      "|100|    Victor|     Hugo|25.0|     M|\n",
      "|200|      Mary|   Shelly|30.0|     F|\n",
      "|300|    Johann|   Geothe|75.0|     M|\n",
      "|400|    Albert|    Camus|null|     M|\n",
      "+---+----------+---------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(~(f.col(\"last_name\")==\"Shakespear\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c7840",
   "metadata": {},
   "source": [
    "Co se týče použití více podmínek naráz, je situace stejná jako v pandách. Tj. jednotlivé podmínky se vloží do kulatých závorek a spojí se buďto & (v případě \"and\" logiky), anebo | (v případě \"or\" logiky)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "409913ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(\n",
    "    (f.col(\"last_name\")==\"Shakespear\")\n",
    "    | (f.col(\"first_name\")==\"Mary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76ab909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(\n",
    "    (f.col(\"gender\")==\"M\")\n",
    "    & (f.col(\"age\")>=30)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66f115",
   "metadata": {},
   "source": [
    "Pro vybrání řádků s null hodnotami použijeme metodu *isNull* aplikovanou na f.col(\"jmeno_sloupce\"). Podobný modus operandi má metoda *isNotNull*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3cf8f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+\n",
      "| id|first_name|last_name| age|gender|\n",
      "+---+----------+---------+----+------+\n",
      "|400|    Albert|    Camus|null|     M|\n",
      "+---+----------+---------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(f.col(\"age\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d4d4c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(f.col(\"age\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774fbd5",
   "metadata": {},
   "source": [
    "Co máme dělat, když chceme selectování a filtrování provést najednou? Zkrátka selecty a filtery napíšeme za sebe. Abychom neměli jeden gigantický řádek, celý příkaz obalíme do kulatých závorek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83a1e9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|first_name| last_name|\n",
      "+----------+----------+\n",
      "|   William|Shakespear|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "  table_from_csv\n",
    "  .filter(f.col(\"last_name\")==\"Shakespear\")\n",
    "  .select(\"first_name\", \"last_name\")\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdb6ef",
   "metadata": {},
   "source": [
    "Pro seřazení řádků podle hodnot v určitém sloupci použijeme *sort*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bc79128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.sort(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90820768",
   "metadata": {},
   "source": [
    "Pokud chceme sestupné řazení, přidáme do *sort* parametr *ascending* s hodnotou False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cf84e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.sort(\"age\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971bdca",
   "metadata": {},
   "source": [
    "Pokud chceme vidět unikátní hodnoty z určitého sloupce, napřed onen sloupec selectem vybereme a následně uplatníme metodu *distinct*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b750a617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.select(\"gender\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a24a4f",
   "metadata": {},
   "source": [
    "Pro vyhození řádků s nully se použije konstrukce *na.drop*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb996df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c0d11",
   "metadata": {},
   "source": [
    "Naopak pro nahrazení nullů nějakou hodotou použijeme *na.fill*. Té předáme parametr *value* říkající, čím se mají nully nahradit, a parametr *subset* specifikující, na kterých sloupcích má nahrazování probíhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6aefee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|   ?|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.na.fill(value=\"?\", subset=[\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84129703",
   "metadata": {},
   "source": [
    "Pro agregované statistiky napřed data zhlukneme do tříd s pomocí *groupBy*. Jako parametr se uvede jméno sloupce, podle kterého grupování proběhne. Následně do metody *agg* vložíme metody (obvykle z *pyspark.sql.functions*) statistiky napočítávající. Pokud se nám nelíbí defaultní jména takto vyprodukovaných sloupců, použijeme metodu *alias*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1184b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+--------+-----------------------+\n",
      "|gender|count(1)|min(age)|max(age)|nonsensical_aggregation|\n",
      "+------+--------+--------+--------+-----------------------+\n",
      "|     F|       1|    30.0|    30.0|                  200.0|\n",
      "|     M|       4|    25.0|    75.0|                  325.0|\n",
      "+------+--------+--------+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.groupBy(\"gender\").agg(\n",
    "  f.count(\"*\"),\n",
    "  f.min(\"age\"),\n",
    "  f.max(\"age\"),\n",
    "  f.avg(\"id\").alias(\"nonsensical_aggregation\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9101d",
   "metadata": {},
   "source": [
    "Pro najoinování dvou dataframů na sebe použijeme metodu *join* zavolanou na jednom z nich. Proměnná s druhým dataframem tvoří první parametr metody *join*. Druhý parametr,  *on*, specifikuje joinovací sloupec. Pokud se ten v obou tabulkách jmenuje stejně, má formu obyčejného stringu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e12ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+-----------+\n",
      "| id|first_name|last_name| age|gender|books_count|\n",
      "+---+----------+---------+----+------+-----------+\n",
      "|100|    Victor|     Hugo|25.0|     M|          5|\n",
      "|300|    Johann|   Geothe|75.0|     M|          7|\n",
      "|400|    Albert|    Camus|null|     M|          9|\n",
      "+---+----------+---------+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_list = [\n",
    "    [100, 5],\n",
    "    [300, 7],\n",
    "    [400, 9]\n",
    "]\n",
    "books_frame = spark.createDataFrame(\n",
    "    books_list,\n",
    "    schema=[\"id\", \"books_count\"]\n",
    ")\n",
    "\n",
    "joined_frame = table_from_csv.join(books_frame, on=\"id\")\n",
    "joined_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8387e4",
   "metadata": {},
   "source": [
    "Pokud se ale joinovací sloupec v dataframech jmenuje odlišně, obsahuje *on* porovnávání."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "301806ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+-------+-----------+\n",
      "| id|first_name|last_name| age|gender|some_id|books_count|\n",
      "+---+----------+---------+----+------+-------+-----------+\n",
      "|100|    Victor|     Hugo|25.0|     M|    100|          5|\n",
      "|300|    Johann|   Geothe|75.0|     M|    300|          7|\n",
      "|400|    Albert|    Camus|null|     M|    400|          9|\n",
      "+---+----------+---------+----+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_list = [\n",
    "    [100, 5],\n",
    "    [300, 7],\n",
    "    [400, 9]\n",
    "]\n",
    "books_frame = spark.createDataFrame(\n",
    "    books_list,\n",
    "    schema=[\"some_id\", \"books_count\"]\n",
    ")\n",
    "\n",
    "joined_frame = table_from_csv.join(\n",
    "    books_frame, \n",
    "    on=table_from_csv[\"id\"]==books_frame[\"some_id\"]\n",
    ")\n",
    "joined_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99004a7d",
   "metadata": {},
   "source": [
    "Pakliže chceme jiný typ joinu než inner join, musíme to specifikovat do parametru *how*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd867875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+-----------+\n",
      "| id|first_name| last_name| age|gender|books_count|\n",
      "+---+----------+----------+----+------+-----------+\n",
      "|500|   William|Shakespear|38.0|     M|       null|\n",
      "|100|    Victor|      Hugo|25.0|     M|          5|\n",
      "|200|      Mary|    Shelly|30.0|     F|       null|\n",
      "|400|    Albert|     Camus|null|     M|          9|\n",
      "|300|    Johann|    Geothe|75.0|     M|          7|\n",
      "+---+----------+----------+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_list = [\n",
    "    [100, 5],\n",
    "    [300, 7],\n",
    "    [400, 9]\n",
    "]\n",
    "books_frame = spark.createDataFrame(\n",
    "    books_list,\n",
    "    schema=[\"id\", \"books_count\"]\n",
    ")\n",
    "\n",
    "joined_frame = table_from_csv.join(books_frame, on=\"id\", how=\"left\")\n",
    "joined_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34add2",
   "metadata": {},
   "source": [
    "#### PySpark a datumy\n",
    "Dost často člověk potřebuje pracovat s datumovým datovým typem. Jelikož kvůli tomu se obvykle musí dělat konverze a i počítání rozdílů mezi dvěma dny nebývá úplně zjevné, věnujme této problematice trochu času.  \n",
    "Nejprve si vytvoříme dataframe. Všimněte si, že ačkoli druhý sloupec obsahuje de facto datumy, pro Spark jsou to stringy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69a404de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_data = [\n",
    "    [1,\"20.12.2021\"],\n",
    "    [2,\"05.01.2022\"],\n",
    "    [3,\"08.01.2022\"],\n",
    "    [4,\"12.02.2022\"]\n",
    "    \n",
    "]\n",
    "dates_frame = spark.createDataFrame(dates_data, [\"number\",\"date\"])\n",
    "dates_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079e537",
   "metadata": {},
   "source": [
    "Konverzi provedeme s pomocí *functions.to_date*. První parametrem je konvertovaný sloupec, druhý pak obsahuje předpis popisující původní data. Význam jednotlivých písmen, které se v něm mohou objevit, nalezneme [zde](https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb94907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- date_converted: date (nullable = true)\n",
      "\n",
      "+------+----------+--------------+\n",
      "|number|      date|date_converted|\n",
      "+------+----------+--------------+\n",
      "|     1|20.12.2021|    2021-12-20|\n",
      "|     2|05.01.2022|    2022-01-05|\n",
      "|     3|08.01.2022|    2022-01-08|\n",
      "|     4|12.02.2022|    2022-02-12|\n",
      "+------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "\n",
    "dates_frame = dates_frame.withColumn(\n",
    "    \"date_converted\",\n",
    "     f.to_date(f.col(\"date\"), \"dd.MM.yyyy\")\n",
    ")\n",
    "\n",
    "dates_frame.printSchema()\n",
    "dates_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534845ff",
   "metadata": {},
   "source": [
    "Aktuální datum získáme s pomocí *functions.current_date*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e44cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- date_converted: date (nullable = true)\n",
      " |-- current_date: date (nullable = false)\n",
      "\n",
      "+------+----------+--------------+------------+\n",
      "|number|      date|date_converted|current_date|\n",
      "+------+----------+--------------+------------+\n",
      "|     1|20.12.2021|    2021-12-20|  2022-06-05|\n",
      "|     2|05.01.2022|    2022-01-05|  2022-06-05|\n",
      "|     3|08.01.2022|    2022-01-08|  2022-06-05|\n",
      "|     4|12.02.2022|    2022-02-12|  2022-06-05|\n",
      "+------+----------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_frame = dates_frame.withColumn(\n",
    "    \"current_date\",\n",
    "     f.current_date()\n",
    ")\n",
    "\n",
    "dates_frame.printSchema()\n",
    "dates_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27348cfd",
   "metadata": {},
   "source": [
    "Rozdíl datumů ve dnech spočítáme prostřednictvím *functions.datediff*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33f70e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------+------------+\n",
      "|number|      date|date_converted|current_date|\n",
      "+------+----------+--------------+------------+\n",
      "|     1|20.12.2021|    2021-12-20|  2022-06-05|\n",
      "|     2|05.01.2022|    2022-01-05|  2022-06-05|\n",
      "|     3|08.01.2022|    2022-01-08|  2022-06-05|\n",
      "|     4|12.02.2022|    2022-02-12|  2022-06-05|\n",
      "+------+----------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_frame.withColumn(\n",
    "    \"current_minus_converted\",\n",
    "     f.datediff(f.col(\"date_converted\"),f.col(\"current_date\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a0c22",
   "metadata": {},
   "source": [
    "Pakliže potřebujeme k datu přičíst či odečíst den či měsíc, sáhneme po *functions.add_months* a *functions.date_add* (obě funkce dokáží přijmout kladné i záporné počty měsíců/dnů)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e955dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------+------------+------------+-------------+----------+-----------+\n",
      "|number|      date|date_converted|current_date|added_months|substr_months|added_days|substr_days|\n",
      "+------+----------+--------------+------------+------------+-------------+----------+-----------+\n",
      "|     1|20.12.2021|    2021-12-20|  2022-06-05|  2022-02-20|   2021-10-20|2022-01-04| 2021-12-05|\n",
      "|     2|05.01.2022|    2022-01-05|  2022-06-05|  2022-03-05|   2021-11-05|2022-01-20| 2021-12-21|\n",
      "|     3|08.01.2022|    2022-01-08|  2022-06-05|  2022-03-08|   2021-11-08|2022-01-23| 2021-12-24|\n",
      "|     4|12.02.2022|    2022-02-12|  2022-06-05|  2022-04-12|   2021-12-12|2022-02-27| 2022-01-28|\n",
      "+------+----------+--------------+------------+------------+-------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    dates_frame\n",
    "    .withColumn(\"added_months\", f.add_months(f.col(\"date_converted\"),2))\n",
    "    .withColumn(\"substr_months\", f.add_months(f.col(\"date_converted\"),-2)) \n",
    "    .withColumn(\"added_days\", f.date_add(f.col(\"date_converted\"),15)) \n",
    "    .withColumn(\"substr_days\", f.date_add(f.col(\"date_converted\"),-15))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a544ff9",
   "metadata": {},
   "source": [
    "Existují i funkce na extrakci roku, měsíce, dne atd. z datumu: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c6cb863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------+------------+--------------+---------------+-------------+-----------+-----------+------------+\n",
      "|number|      date|date_converted|current_date|extracted_year|extracted_month|extracted_day|day_of_year|day_of_week|week_of_year|\n",
      "+------+----------+--------------+------------+--------------+---------------+-------------+-----------+-----------+------------+\n",
      "|     1|20.12.2021|    2021-12-20|  2022-06-05|          2021|             12|           20|        354|          2|          51|\n",
      "|     2|05.01.2022|    2022-01-05|  2022-06-05|          2022|              1|            5|          5|          4|           1|\n",
      "|     3|08.01.2022|    2022-01-08|  2022-06-05|          2022|              1|            8|          8|          7|           1|\n",
      "|     4|12.02.2022|    2022-02-12|  2022-06-05|          2022|              2|           12|         43|          7|           6|\n",
      "+------+----------+--------------+------------+--------------+---------------+-------------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    dates_frame\n",
    "    .withColumn(\"extracted_year\", f.year(f.col(\"date_converted\")))\n",
    "    .withColumn(\"extracted_month\", f.month(f.col(\"date_converted\"))) \n",
    "    .withColumn(\"extracted_day\", f.dayofmonth(f.col(\"date_converted\"))) \n",
    "    .withColumn(\"day_of_year\", f.dayofyear(f.col(\"date_converted\")))\n",
    "    .withColumn(\"day_of_week\", f.dayofweek(f.col(\"date_converted\")))\n",
    "    .withColumn(\"week_of_year\", f.weekofyear(f.col(\"date_converted\")))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e56ac",
   "metadata": {},
   "source": [
    "#### Window funkce\n",
    "Někdy se hodí provádět určité operace nikoli na celém dataframu, ale pouze na jeho části určené buďto hodnotou nějakého sloupce, anebo počtem řádek.  \n",
    "Nejprve si vytvořme pokusný dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735843c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_window_list = [\n",
    "    [\"one\", 10],\n",
    "    [\"one\", 10],\n",
    "    [\"one\", 30],\n",
    "    [\"one\", 40],\n",
    "    [\"one\", 50],\n",
    "    [\"two\", 60],\n",
    "    [\"two\", 70],\n",
    "    [\"two\", 80],\n",
    "    [\"two\", 90],\n",
    "    [\"three\", 100]\n",
    "    \n",
    "]\n",
    "for_window_frame = spark.createDataFrame(\n",
    "    for_window_list,\n",
    "    schema=[\"word\", \"number\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d89fd",
   "metadata": {},
   "source": [
    "První ukázka se bude týkat číslování řádek. Nejprve si vytvoříme definici okna. Budeme chtít pro jednotlivá slova samostatné číslování, proto *partitionBy(\"word\")*. Záznamy v jedné skupině pak chceme setřídit podle hodnoty sloupce \"number\", proto následuje *orderBy(\"number\")*.   \n",
    "\n",
    "Čísla řádků budeme chtít umístit do nového sloupce, proto použijeme metodu *withColumn*. Na samotné napočítání vložíme do druhého parametru této sloupec vytvářející metody *functions.row_number().over(window_definition)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b6922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "| word|number|row_number|\n",
      "+-----+------+----------+\n",
      "|  two|    60|         1|\n",
      "|  two|    70|         2|\n",
      "|  two|    80|         3|\n",
      "|  two|    90|         4|\n",
      "|  one|    10|         1|\n",
      "|  one|    10|         2|\n",
      "|  one|    30|         3|\n",
      "|  one|    40|         4|\n",
      "|  one|    50|         5|\n",
      "|three|   100|         1|\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"row_number\",\n",
    "    f.row_number().over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d943f35",
   "metadata": {},
   "source": [
    "Pokud bychom chtěli nikoli číslo řádky, ale pořadí čísel ve sloupci \"number\", použijeme namísto funkce *row_number* funkci *rank*. Rozdíl je patrný pro řádky, kde hodnota sloupce \"word\" je rovna \"one\" a hodnota sloupce \"number\" odpovídá 10. Zatímco u *row_number* měl u sebe jeden řádek jedničku a druhý dvojku, v případě *rank* mají jedničku oba. Následující řádek má ale trojku, tj. pořadí 2 není. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d37343eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+\n",
      "| word|number|rank_number|\n",
      "+-----+------+-----------+\n",
      "|  two|    60|          1|\n",
      "|  two|    70|          2|\n",
      "|  two|    80|          3|\n",
      "|  two|    90|          4|\n",
      "|  one|    10|          1|\n",
      "|  one|    10|          1|\n",
      "|  one|    30|          3|\n",
      "|  one|    40|          4|\n",
      "|  one|    50|          5|\n",
      "|three|   100|          1|\n",
      "+-----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"rank\",\n",
    "    f.rank().over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba0324",
   "metadata": {},
   "source": [
    "Pokud by nám to vadilo, použijeme namísto *rank* funkci *dense_rank*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a584c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "| word|number|dense_rank|\n",
      "+-----+------+----------+\n",
      "|  two|    60|         1|\n",
      "|  two|    70|         2|\n",
      "|  two|    80|         3|\n",
      "|  two|    90|         4|\n",
      "|  one|    10|         1|\n",
      "|  one|    10|         1|\n",
      "|  one|    30|         2|\n",
      "|  one|    40|         3|\n",
      "|  one|    50|         4|\n",
      "|three|   100|         1|\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"dense_rank\",\n",
    "    f.dense_rank().over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865567c",
   "metadata": {},
   "source": [
    "Pokud potřebujeme mít sloupec, který bude o jednu či více řádek posunutý níže, aplikujeme funkci *lag*. Kde nebude co dát, tam se objeví null. Pokud bychom chtěli mít sloupec posunout opačným směrem (tj. o N řádků výše), použijeme funkci *lead*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9293613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+\n",
      "| word|number|lagged_column|\n",
      "+-----+------+-------------+\n",
      "|  two|    60|         null|\n",
      "|  two|    70|           60|\n",
      "|  two|    80|           70|\n",
      "|  two|    90|           80|\n",
      "|  one|    10|         null|\n",
      "|  one|    10|           10|\n",
      "|  one|    30|           10|\n",
      "|  one|    40|           30|\n",
      "|  one|    50|           40|\n",
      "|three|   100|         null|\n",
      "+-----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"lagged_column\",\n",
    "    f.lag(\"number\", 1).over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53105aab",
   "metadata": {},
   "source": [
    "Pakliže potřebujeme kumulativní sumu, aplikujeme *functions.sum*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b4e37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+\n",
      "| word|number|lagged_column|\n",
      "+-----+------+-------------+\n",
      "|  two|    60|           60|\n",
      "|  two|    70|          130|\n",
      "|  two|    80|          210|\n",
      "|  two|    90|          300|\n",
      "|  one|    10|           20|\n",
      "|  one|    10|           20|\n",
      "|  one|    30|           50|\n",
      "|  one|    40|           90|\n",
      "|  one|    50|          140|\n",
      "|three|   100|          100|\n",
      "+-----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"cumulative_sum\",\n",
    "    f.sum(\"number\").over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b2350",
   "metadata": {},
   "source": [
    "### SQL přístup\n",
    "Doposud jsme si ukazovali práci s PySparkem, která se svým modem operandi dosti podobala práci s pandami. Nicméně existuje ještě jeden přístup, který se podobá zacházení s SQL databázemi.  \n",
    "Nejprve musíme na dataframu provolat metodu *createOrReplaceTempView*. Té předáme jen jeden parametr - jméno onoho dočasného pohledu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231ebabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_csv.createOrReplaceTempView(\"some_sql_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12aa51c",
   "metadata": {},
   "source": [
    "Tento pohled můžeme posléze používat v sql dotazech, které vložíme jako parametr do metody *sql*, která je napojená na samotnou pysparkovou sešnu. Výsledkem je další pysparkový dataframe, na který, pokud chceme vidět jeho obsah, musíme napojit metodu *show*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1962fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from some_sql_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f667fdb1",
   "metadata": {},
   "source": [
    "### Cachování\n",
    "Při psaní této sekce jsem extenzivně čerpal  [odsud](https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34?gi=407c957f2bcb) - doporučuji přečíst.  \n",
    "Představme si, že bychom s pomocí selectování a filtrování vyrobili z nějakého dataframu dataframe menší a ten umístili do nové proměnné. Když bychom na této nové tabulce začali provádět nějaké akce, dojde bohužel k tomu, že se vytváření onoho redukovaného dataframu provede pro každou z těchto akcí zvlášť. Aby se zbytečně neplýtvalo výpočetními prostředky, existuje možnost si dataframe nacachovat, tj. uložit do paměti/na harddisk pro budoucí využití.\n",
    "Cachování se realizuje provoláním dataframové metody *cache* resp. *persist*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12350588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, first_name: string, last_name: string, age: string, gender: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6e95c",
   "metadata": {},
   "source": [
    "Rozdíl mezi nimi spočívá jen v tom, že u persistu lze specifikovat, kam přesně budou data uložena. Nicméně ve většině případů si člověk vystačí s defaultním MEMORY_AND_DISK, tj. data se prioritně ukládají do RAMky a když ta dojde, míří zbytek na disk.  \n",
    "V případě panda-like přístupu je cachování lazy transformace. U SQL přístupu, který má podobu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19a479e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"cache table some_sql_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46e1ca",
   "metadata": {},
   "source": [
    "to ale neplatí - zde máme co do činění s eager operací, tj. potřebná posloupnost transformací ihned proběhne a umístí data do úložiště. Nicméně toto defaultní chování máme možnost změnit na lazy s použitím odpovídajícího klíčového slova "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96057f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"cache lazy table some_sql_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981df0f6",
   "metadata": {},
   "source": [
    "Pro zjištění, zda je dataframe nacachovaný, se použije atribut *is_cached*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d322c3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d07c20",
   "metadata": {},
   "source": [
    "Pokud bychom ručně chtěli data z cache odstanit, použijeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a278faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, first_name: string, last_name: string, age: string, gender: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce9f7e",
   "metadata": {},
   "source": [
    "resp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67780211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"uncache table some_sql_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e6d70",
   "metadata": {},
   "source": [
    "Předejdeme tak situaci, kdy dojde paměť a Spark samotný začne vyhazovat tabulky na základě míry jejich nepoužívanosti. Navíc čím víc je volné paměti, tím operace probíhají rychleji (není nutné swapovat na disk). Ze stejného důvodu může být nastaven administrátorem PySparkového clusteru timeout (k nalezení v případě Cloudery na XXX) - pokud se na dataframe nesáhlo po dobu delší než je tento časový úsek, je onen dataframe z cache automaticky odstraněn.  \n",
    "Stejnou motivaci má i rada cachovat nikoli celé dataframy, ale pouze ty jejich části, které potřebujeme.  \n",
    "Zmiňme nakonec, že ne vždy cachování vede k rychlejšímu běhu programu. Pokud bychom měli relativně velký parquet soubor a prováděli na něm filtrování, asi tato operace odběhne rychleji, než kdyby se musel napřed celý načíst do paměti (nebo dokonce swapovat na disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490afa22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74be9b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a83a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f2861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
