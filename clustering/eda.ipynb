{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import power_transform\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, silhouette_samples\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def description(df):\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary['Name'] = summary['index']\n",
    "    summary['Missing'] = df.isnull().sum().values\n",
    "    summary['PercMissing'] = df.isnull().sum().values / df.isnull().count().values\n",
    "    summary['Uniques'] = df.nunique().values\n",
    "    summary['Data type'] = df.dtypes.values\n",
    "    summary = summary.merge(df.describe().transpose().reset_index(), on = \"index\",how=\"left\")\n",
    "\n",
    "    return summary\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def jaccard_similarity_continuous_rows_df(df, idx1, idx2):\n",
    "    \"\"\"\n",
    "    Compute the Jaccard similarity coefficient for continuous data between two rows in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    idx1 (int): Index of the first row.\n",
    "    idx2 (int): Index of the second row.\n",
    "\n",
    "    Returns:\n",
    "    float: Jaccard similarity coefficient.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Extract the values of the two rows\n",
    "    x = df.iloc[idx1].values\n",
    "    y = df.iloc[idx2].values\n",
    "\n",
    "    # Calculate intersection and union\n",
    "    intersection = np.minimum(x, y).sum()\n",
    "    union = np.maximum(x, y).sum()\n",
    "\n",
    "    # Return Jaccard similarity coefficient\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def analyze_web_similarity(df, n_similar=5, standardize=False):\n",
    "    \"\"\"\n",
    "    Analyzuje podobnost webových stránek na základě jejich features.\n",
    "    \n",
    "    # Provedení analýzy\n",
    "    results = analyze_web_similarity(df)\n",
    "\n",
    "    # Vizualizace výsledků\n",
    "    visualize_similarities(results)\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame s názvy webů v prvním sloupci a features v dalších sloupcích\n",
    "    n_similar: Počet podobných stránek k zobrazení pro každou stránku\n",
    "    \n",
    "    Returns:\n",
    "    dict: Slovník s různými metrikami podobnosti a analýzami\n",
    "    \"\"\"\n",
    "    # Oddělení názvů a features\n",
    "    websites = df.iloc[:, 0]\n",
    "    features = df.iloc[:, 1:]\n",
    "    \n",
    "    # # Standardizace features\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "    else:\n",
    "        features_scaled = features\n",
    "    \n",
    "    # 1. Kosinová podobnost\n",
    "    cosine_sim = cosine_similarity(features_scaled)\n",
    "    \n",
    "    # 2. Euklidovská vzdálenost\n",
    "    euclidean_dist = squareform(pdist(features_scaled, 'euclidean'))\n",
    "    \n",
    "    # Vytvoření dictionary pro každou webovou stránku s jejími nejpodobnějšími protějšky\n",
    "    similarity_dict = {}\n",
    "    for i, website in enumerate(websites):\n",
    "        # Najdeme nejpodobnější stránky (kromě sebe sama)\n",
    "        similar_indices = cosine_sim[i].argsort()[::-1][1:n_similar+1]\n",
    "        similar_websites = [\n",
    "            {\n",
    "                'web': websites.iloc[idx],\n",
    "                'cosine_similarity': cosine_sim[i][idx],\n",
    "                'euclidean_distance': euclidean_dist[i][idx],\n",
    "                \"jaccard_sim\" : jaccard_similarity_continuous_rows_df(features, i, idx)\n",
    "            }\n",
    "            for idx in similar_indices\n",
    "        ]\n",
    "        similarity_dict[website] = similar_websites\n",
    "    \n",
    "    # Základní statistiky features\n",
    "    feature_stats = {\n",
    "        'correlation_matrix': features.corr(),\n",
    "        'feature_importance': np.std(features_scaled, axis=0),\n",
    "        'feature_names': features.columns\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'similarity_dict': similarity_dict,\n",
    "        'cosine_similarity_matrix': cosine_sim,\n",
    "        'euclidean_distance_matrix': euclidean_dist,\n",
    "        'feature_stats': feature_stats\n",
    "    }\n",
    "\n",
    "def visualize_similarities(analysis_results, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Vizualizuje výsledky analýzy podobnosti.\n",
    "    \"\"\"\n",
    "    # Vytvoření heat mapy korelací features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        analysis_results['feature_stats']['correlation_matrix'],\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        annot=False\n",
    "    )\n",
    "    plt.title('Korelační matice features')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Vytvoření grafu důležitosti features\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    feature_importance = pd.Series(\n",
    "        analysis_results['feature_stats']['feature_importance'],\n",
    "        index=analysis_results['feature_stats']['feature_names']\n",
    "    )\n",
    "    feature_importance.sort_values(ascending=True).plot(kind='barh')\n",
    "    plt.title('Důležitost jednotlivých features')\n",
    "    plt.xlabel('Standardizovaná směrodatná odchylka')\n",
    "    plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = pd.read_csv('sites_embedding.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "summary_data = description(data)\n",
    "summary_data\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_tranformed = data.copy()\n",
    "for col in data.columns[1:]:\n",
    "    data_tranformed[col + \"_transformed\"], fitted_lambda = stats.boxcox(data_tranformed[col] + 1)\n",
    "    print(f\"Column {col} transformed with lambda {fitted_lambda}\")\n",
    "\n",
    "data_tranformed = data_tranformed.drop(columns=data.columns[1:25])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "summary_data_transf = description(data_tranformed)\n",
    "summary_data_transf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "summary_data.plot(x='Name', y='mean', kind='bar', figsize=(12, 6))\n",
    "summary_data.plot(x='Name', y='std', kind='bar', figsize=(12, 6))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(data_tranformed.iloc[:, 1:].sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_tranformed.iloc[:, 1:25].sum().plot(kind=\"bar\", figsize=(10, 6))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_scaled = data.copy()\n",
    "scaler = StandardScaler()\n",
    "data_scaled.iloc[:, 1:] = scaler.fit_transform(data_scaled.iloc[:, 1:])\n",
    "description(data_scaled)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pairplot to visualize relationships\n",
    "sns.pairplot(data)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pairplot to visualize relationships\n",
    "sns.pairplot(data_tranformed)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# for col in data.columns[1:]:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(y=col, data=data)\n",
    "#     # Add labels and title\n",
    "#     plt.xlabel(col)\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for col in data_tranformed.columns[1:]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(y=col, data=data_tranformed)\n",
    "    # Add labels and title\n",
    "    plt.xlabel(col)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for col in data_tranformed.columns[1:]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data_tranformed[col], bins=50, kde=True)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Histogram and KDE of Box-Cox Transformed ' + col)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cols_features = pd.DataFrame()\n",
    "\n",
    "for col in data_tranformed.columns[1:]:\n",
    "    col_head = (\n",
    "        data_tranformed[\n",
    "            data_tranformed[col] >= data_tranformed[col].quantile(0.99)]\n",
    "            # .sort_values(by=col, ascending=False)\n",
    "            .head(20)\n",
    "            .reset_index(drop=True)\n",
    "    )[\"site\"]\n",
    "    cols_features = pd.concat([cols_features, pd.DataFrame({col : col_head})], axis=1)\n",
    "\n",
    "cols_features"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "features_names = {\n",
    "    'F_1_transformed' : \"social_media\", \n",
    "    'F_2_transformed' : \"news_social\", \n",
    "    'F_3_transformed' : \"?\",\n",
    "    'F_4_transformed' : \"health_lifestyle\", \n",
    "    'F_5_transformed' : \"tech/games\", \n",
    "    'F_6_transformed' : \"lifestyle\",\n",
    "    'F_7_transformed' : \"finance\", \n",
    "    'F_8_transformed' : \"food\", \n",
    "    'F_9_transformed' : \"sport\",\n",
    "    'F_10_transformed' : \"games\", \n",
    "    'F_11_transformed' : \"medical/health\", \n",
    "    'F_12_transformed' : \"guns\",\n",
    "    'F_13_transformed' : \"natures\", \n",
    "    'F_14_transformed' : \"lifestyle_2\", \n",
    "    'F_15_transformed' : \"apps\",\n",
    "    'F_16_transformed' : \"news_2\",\n",
    "    'F_17_transformed' : \"sport/lifestyle\",\n",
    "    'F_18_transformed' : \"news\",\n",
    "    'F_19_transformed' : \"pets\", \n",
    "    'F_20_transformed' : \"accademy\", \n",
    "    'F_21_transformed' : \"courses\",\n",
    "    'F_22_transformed' : \"sport\", \n",
    "    'F_23_transformed' : \"travel\", \n",
    "    'F_24_transformed' : \"cars\"\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for col in data_tranformed.columns[1:]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data_tranformed[col].loc[data_tranformed[col] > 0], bins=50, kde=True)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Histogram and KDE of Box-Cox Transformed ' + col)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# # Select numerical columns\n",
    "# numerical_cols = data_tranformed.columns.difference([\"site\"])\n",
    "# print(\"Num Cols: \" + str(numerical_cols))\n",
    "# numerical_df = data_tranformed[numerical_cols]\n",
    "# numerical_df[numerical_df == 0] = np.nan\n",
    "# corr_matrix = numerical_df.corr()\n",
    "\n",
    "# plt.figure(figsize=(16, 12))\n",
    "# sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "# plt.title('Correlation Matrix of Numerical Variables')\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select numerical columns\n",
    "numerical_cols = data.columns.difference([\"site\"])\n",
    "print(\"Num Cols: \" + str(numerical_cols))\n",
    "numerical_df = data[numerical_cols]\n",
    "corr_matrix = numerical_df.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Numerical Variables')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select numerical columns\n",
    "numerical_cols = data_tranformed.columns[data_tranformed.columns.str.contains(\"_transformed\")]\n",
    "print(\"Num Cols: \" + str(numerical_cols))\n",
    "numerical_df = data_tranformed[numerical_cols]\n",
    "corr_matrix = numerical_df.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Numerical Variables')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "corr_cutoff = 0.7\n",
    "\n",
    "numerical_cols = data_tranformed.columns[data_tranformed.columns.str.contains(\"_transformed\")]\n",
    "print(\"Num Cols: \" + str(numerical_cols))\n",
    "numerical_df = data_tranformed[numerical_cols]\n",
    "corr_matrix = numerical_df.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix[abs(corr_matrix) > corr_cutoff], annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Numerical Variables')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#%% PCA\n",
    "numerical_cols = data.columns.difference([\"site\"])\n",
    "numerical_df = data[numerical_cols]\n",
    "\n",
    "pca = PCA(n_components=4) \n",
    "principal_components = pca.fit_transform(numerical_df)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Numerical Data')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#%% PCA\n",
    "numerical_cols = data_tranformed.columns.difference([\"site\"])\n",
    "numerical_df = data_tranformed[numerical_cols]\n",
    "\n",
    "pca = PCA(n_components=4) \n",
    "principal_components = pca.fit_transform(numerical_df)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Numerical Transformed Data')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find number of optimal clusters using Elbow method and silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3D Plot\n",
    "numerical_cols = data.columns.difference([\"site\"])\n",
    "numerical_df = data[numerical_cols]\n",
    "pca = PCA(n_components=4)  \n",
    "principal_components = pca.fit_transform(numerical_df)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "\n",
    "x_col = 'PC1'\n",
    "y_col = 'PC2'\n",
    "z_col = 'PC3'\n",
    "\n",
    "fig = px.scatter_3d(pca_df, x=x_col, y=y_col, z=z_col, color=z_col,\n",
    "                    title=f'3D Scatter Plot of {x_col} vs {y_col} vs {z_col}')\n",
    "fig.update_layout(width=1000, height=800)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3D plot on Transformed Data\n",
    "\n",
    "numerical_cols = data_tranformed.columns.difference([\"site\"])\n",
    "numerical_df = data_tranformed[numerical_cols]\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "principal_components = pca.fit_transform(numerical_df)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "\n",
    "x_col = 'PC1'\n",
    "y_col = 'PC2'\n",
    "z_col = 'PC3'\n",
    "\n",
    "fig = px.scatter_3d(pca_df, x=x_col, y=y_col, z=z_col, color=z_col,\n",
    "                    title=f'3D Scatter Plot of {x_col} vs {y_col} vs {z_col}')\n",
    "fig.update_layout(width=1000, height=800)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "df = data_tranformed.copy()\n",
    "tsne = TSNE(n_components=2, perplexity=100, max_iter=300)\n",
    "tsne_result = tsne.fit_transform(df.iloc[:, 1:])\n",
    "df['TSNE1'] = tsne_result[:, 0]\n",
    "df['TSNE2'] = tsne_result[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', data=df)\n",
    "plt.title('t-SNE of Website Embeddings')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "df = data_tranformed.copy()\n",
    "tsne = TSNE(n_components=3, perplexity=100, max_iter=300)\n",
    "tsne_result = tsne.fit_transform(df.iloc[:, 1:])\n",
    "\n",
    "df['TSNE1'] = tsne_result[:, 0]\n",
    "df['TSNE2'] = tsne_result[:, 1]\n",
    "df['TSNE3'] = tsne_result[:, 2]\n",
    "\n",
    "x_col = 'TSNE1'\n",
    "y_col = 'TSNE2'\n",
    "z_col = 'TSNE3'\n",
    "\n",
    "# Create a 3D scatter plot using Plotly\n",
    "fig = px.scatter_3d(df, x=x_col, y=y_col, z=z_col, color=z_col,\n",
    "                    title=f'3D Scatter Plot of {x_col} vs {y_col} vs {z_col}')\n",
    "\n",
    "fig.update_layout(width=1000, height=800)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Meams"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, silhouette_samples\n",
    "\n",
    "class WordEmbeddingClustering:\n",
    "    \"\"\"\n",
    "    A class for clustering word embeddings from a pandas DataFrame where\n",
    "    the first column contains the words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize with the embedding DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        embedding_df : pd.DataFrame\n",
    "            DataFrame where the first column is the word and the remaining columns are embedding dimensions.\n",
    "        \"\"\"\n",
    "        self.embedding_df = embedding_df\n",
    "        self.words = embedding_df.iloc[:, 0].tolist()\n",
    "        self.embeddings = embedding_df.iloc[:, 1:].values\n",
    "        self.normalized_embeddings = self.embeddings\n",
    "        self.labels_ = None\n",
    "        self.model = None\n",
    "\n",
    "    def cluster_embeddings(self, method: str = 'kmeans', n_clusters: int = 10, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform clustering on the normalized embeddings.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            Clustering algorithm ('kmeans', 'dbscan', or 'gmm').\n",
    "        n_clusters : int\n",
    "            Number of clusters (used for 'kmeans' and 'gmm').\n",
    "        \"\"\"\n",
    "        if method == 'kmeans':\n",
    "            self.model = KMeans(n_clusters=n_clusters, **kwargs)\n",
    "        elif method == 'dbscan':\n",
    "            self.model = DBSCAN(**kwargs)\n",
    "        elif method == 'gmm':\n",
    "            self.model = GaussianMixture(n_components=n_clusters, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        self.labels_ = self.model.fit_predict(self.normalized_embeddings)\n",
    "        self.labels_series = pd.Series(self.labels_, index=self.words, name='cluster')\n",
    "\n",
    "    def get_cluster_words(self, n_words_per_cluster: int = 5):\n",
    "        \"\"\"\n",
    "        Get representative words for each cluster.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_words_per_cluster : int\n",
    "            Number of words per cluster to return.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Dict[int, pd.DataFrame]\n",
    "            Mapping of cluster labels to DataFrames of representative words.\n",
    "        \"\"\"\n",
    "        if self.labels_ is None:\n",
    "            raise ValueError(\"Clustering has not been performed yet.\")\n",
    "        \n",
    "        cluster_words = {}\n",
    "        unique_labels = np.unique(self.labels_)\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            if label == -1:\n",
    "                continue  # Skip noise points\n",
    "            cluster_indices = np.where(self.labels_ == label)[0]\n",
    "            cluster_embeddings = self.embeddings[cluster_indices]\n",
    "            cluster_words_list = [self.words[idx] for idx in cluster_indices]\n",
    "            cluster_df = pd.DataFrame(cluster_embeddings, index=cluster_words_list)\n",
    "\n",
    "            if hasattr(self.model, 'cluster_centers_'):\n",
    "                center = self.model.cluster_centers_[label]\n",
    "            elif hasattr(self.model, 'means_'):\n",
    "                center = self.model.means_[label]\n",
    "            else:\n",
    "                center = cluster_embeddings.mean(axis=0)\n",
    "            \n",
    "            distances = np.linalg.norm(cluster_embeddings - center, axis=1)\n",
    "            closest_indices = np.argsort(distances)[:n_words_per_cluster]\n",
    "            representative_words = cluster_df.iloc[closest_indices]\n",
    "            cluster_words[label] = representative_words\n",
    "            \n",
    "        return cluster_words\n",
    "\n",
    "    def get_cluster_statistics(self):\n",
    "        \"\"\"\n",
    "        Calculate basic statistics for each cluster.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            DataFrame with cluster statistics.\n",
    "        \"\"\"\n",
    "        if self.labels_ is None:\n",
    "            raise ValueError(\"Clustering has not been performed yet.\")\n",
    "        \n",
    "        stats = []\n",
    "        unique_labels = np.unique(self.labels_)\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            if label == -1:\n",
    "                continue\n",
    "            cluster_size = np.sum(self.labels_ == label)\n",
    "            stats.append({\n",
    "                'cluster': label,\n",
    "                'size': cluster_size\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats).set_index('cluster')\n",
    "\n",
    "    def evaluate_clustering(self):\n",
    "        \"\"\"\n",
    "        Evaluate clustering using Silhouette Score and Davies-Bouldin Index,\n",
    "        and provide per-cluster silhouette scores.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        Dict[str, Any]\n",
    "            Dictionary with evaluation metrics, including per-cluster silhouette scores.\n",
    "        \"\"\"\n",
    "        if self.labels_ is None:\n",
    "            raise ValueError(\"Clustering has not been performed yet.\")\n",
    "        \n",
    "        unique_labels = np.unique(self.labels_)\n",
    "        if len(unique_labels) > 1:\n",
    "            # Compute silhouette scores for all samples\n",
    "            sample_silhouette_values = silhouette_samples(self.normalized_embeddings, self.labels_)\n",
    "            # Overall silhouette score\n",
    "            sil_score = sample_silhouette_values.mean()\n",
    "            # Per-cluster silhouette scores\n",
    "            cluster_silhouette_scores = {}\n",
    "            for label in unique_labels:\n",
    "                if label == -1:  # Skip noise points if using DBSCAN\n",
    "                    continue\n",
    "                cluster_mask = self.labels_ == label\n",
    "                cluster_silhouette = sample_silhouette_values[cluster_mask].mean()\n",
    "                cluster_silhouette_scores[label] = cluster_silhouette\n",
    "            # Davies-Bouldin Index\n",
    "            db_score = davies_bouldin_score(self.normalized_embeddings, self.labels_)\n",
    "        else:\n",
    "            sil_score = 0.0\n",
    "            db_score = 0.0\n",
    "            cluster_silhouette_scores = {label: 0.0 for label in unique_labels}\n",
    "        \n",
    "        return {\n",
    "            'silhouette_score': sil_score,\n",
    "            'davies_bouldin_index': db_score,\n",
    "            'cluster_silhouette_scores': cluster_silhouette_scores\n",
    "        }\n",
    "    \n",
    "    def find_optimal_clusters(self, k_range):\n",
    "        from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "        wcss = []\n",
    "        silhouette_scores = []\n",
    "        db_scores = []\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            labels = kmeans.fit_predict(self.normalized_embeddings)\n",
    "            wcss.append(kmeans.inertia_)\n",
    "            silhouette_scores.append(silhouette_score(self.normalized_embeddings, labels))\n",
    "            db_scores.append(davies_bouldin_score(self.normalized_embeddings, labels))\n",
    "        \n",
    "        metrics = {\n",
    "            'k_values': list(k_range),\n",
    "            'wcss': wcss,\n",
    "            'silhouette_scores': silhouette_scores,\n",
    "            'davies_bouldin_scores': db_scores\n",
    "        }\n",
    "        return metrics\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "clusterer = WordEmbeddingClustering(data_tranformed)\n",
    "clusterer.cluster_embeddings(\n",
    "    method='dbscan',\n",
    "    n_clusters=7\n",
    ")\n",
    "\n",
    "evaluation = clusterer.evaluate_clustering()\n",
    "print(\"\\nClustering Evaluation:\")\n",
    "print(pd.DataFrame(evaluation, index=['value']).T)\n",
    "print(\"----\"*20)\n",
    "print(pd.DataFrame.from_dict(evaluation[\"cluster_silhouette_scores\"], orient='index', columns=['silhouette_score']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "clusterer.labels_series[clusterer.labels_series == 4]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "k_range = range(2, 24)\n",
    "\n",
    "# Find optimal clusters\n",
    "metrics = clusterer.find_optimal_clusters(k_range)\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(metrics['k_values'], metrics['wcss'], 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow Method')\n",
    "plt.xticks(metrics['k_values'])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Silhouette Analysis\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(metrics['k_values'], metrics['silhouette_scores'], 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "plt.xticks(metrics['k_values'])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Davies-Bouldin Index\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(metrics['k_values'], metrics['davies_bouldin_scores'], 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.title('Davies-Bouldin Index Analysis')\n",
    "plt.xticks(metrics['k_values'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Provedení analýzy\n",
    "results = analyze_web_similarity(data_tranformed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results[\"similarity_dict\"][\"wine.com\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyza Nul"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nuly = data_tranformed.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "maska_nul = nuly == 0\n",
    "nuly[maska_nul] = 1\n",
    "nuly[~maska_nul] = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slopce, ktery korelujou svyma nulama a zbytekem jsou pravdepodobne pocet navstevnosti a delka stravena na strance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nuly_corr = nuly.drop(\"site\", axis=1).corr()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(nuly_corr[abs(nuly_corr)>0.5], annot=True, fmt='.2f', cmap='coolwarm')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeste se podivame na Jaccardovu podobnost nul"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Compute Jaccard distance for all rows\n",
    "jaccard_distances = pairwise_distances(nuly.drop(\"site\", axis=1).values, metric=\"jaccard\")\n",
    "\n",
    "# Convert distance to similarity (1 - distance)\n",
    "jaccard_similarities = 1 - jaccard_distances\n",
    "\n",
    "# Create a similarity DataFrame\n",
    "similarity_jaccard_df = pd.DataFrame(jaccard_similarities, index=nuly.drop(\"site\", axis=1).index, columns=nuly.drop(\"site\", axis=1).index)\n",
    "\n",
    "print(similarity_jaccard_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "similarity_dict = {}\n",
    "n_similar = 5\n",
    "for i, website in enumerate(data_tranformed[\"site\"]):\n",
    "    # Najdeme nejpodobnější stránky (kromě sebe sama)\n",
    "    similar_indices = similarity_jaccard_df[i].argsort()[::-1][1:n_similar+1]\n",
    "    similar_websites = [\n",
    "        {\n",
    "            'web': data_tranformed[\"site\"].iloc[idx],\n",
    "            'jaccard_similarity': similarity_jaccard_df[i][idx]\n",
    "        }\n",
    "        for idx in similar_indices\n",
    "    ]\n",
    "    similarity_dict[website] = similar_websites"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "similarity_dict[\"wine.com\"] "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dalsi kroky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* H2O automl pro clustering\n",
    "* pomoci elbow metody zjistit pocet clusteru, metrika silhouette\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
