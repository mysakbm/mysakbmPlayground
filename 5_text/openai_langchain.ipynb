{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9200dc",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d36b45-481b-473f-bfd6-d8656887d8dc",
   "metadata": {},
   "source": [
    "Pozn.: zde se nacházejí importy potřebné pro to, aby po restartu kernelu běžela libovolná buňka. V rámci výkladu jsou při setkání se s určitou třídou či funkcí importy samozřejmě uvedeny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db94ccfe-01ea-4e5b-8a90-f75d359dc487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:47:10.485947Z",
     "start_time": "2024-09-23T10:47:09.274345Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import openai\n",
    "import langchain\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import TransformChain\n",
    "\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory \n",
    "from langchain.memory import ConversationTokenBufferMemory \n",
    "from langchain.memory import ConversationSummaryMemory \n",
    "from langchain.memory import ConversationEntityMemory \n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import BSHTMLLoader\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import Language\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "llm_model_name = \"gpt-4o-mini\" #jmeno modelu, ktery se bude pouzivat napric celym notebookem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9b1d5-7127-490e-8ffd-b23f2e3ce37c",
   "metadata": {},
   "source": [
    "V době psaní těchto řádků je to už několik měsíců, co vzalo ChatGPT svět útokem. Člověk se tak mohl pravidelně i ve sdělovacích prostředcích setkávat s více či méně relevantními články o AI. Tento spisek nemá ambice kráčet jim po boku - absolutně netuším, zda AI (z nějakého důvodu ztotožněná s generativními modely - např. s \"doplňovači\" textu ala GPT) začne nahrazovat zaměstnance, zda nás vyhubí anebo zda život půjde dál ve starých kolejích. Namísto toho bych tu chtěl ukázat, jak pracovat s modely od OpenAI a to sice jak napřímo, tak skrze framework Langchain.  \n",
    "Na tomto místě musím doporučit [langchainové kurzy](https://www.deeplearning.ai/short-courses/), které jsou dostupné na deeplearning.ai a ze kterých jsem při tvoření tohoto textu vycházel.\n",
    "\n",
    "# Obsah\n",
    "- [OpenAI](#OpenAI)\n",
    "  - [Tokeny](#Tokeny)\n",
    "  - [API klíč](#API-klíč)\n",
    "  - [Jednoduchý příklad](#Jednoduchý-příklad)\n",
    "  - [Chatbot](#Chatbot)\n",
    "- [Langchain](#Langchain)\n",
    "  - [Šablony promptů](#Šablony-promptů)\n",
    "  - [Chains](#Chains)\n",
    "    - [LLMChain](#LLMChain)\n",
    "    - [Sekvenční chainy](#Sekvenční-chainy)\n",
    "    - [RouterChain](#RouterChain)\n",
    "    - [Transformation chain](#Transformation-chain)\n",
    "  - [Paměť](#Paměť)\n",
    "    - [ConversationBufferMemory](#ConversationBufferMemory)\n",
    "    - [ConversationBufferWindowMemory](#ConversationBufferWindowMemory)\n",
    "    - [ConversationTokenBufferMemory](#ConversationTokenBufferMemory)\n",
    "    - [ConversationSummaryMemory](#ConversationSummaryMemory)\n",
    "    - [ConversationEntityMemory](#ConversationEntityMemory)\n",
    "  - [Q&A nad dokumenty](#Q&A-nad-dokumenty)\n",
    "    - [Načtení dokunetů](#Načtení-dokumentů)\n",
    "      - [Načtení pdfka](#Načtení-pdfka)\n",
    "      - [Načtení html souboru](#Načtení-html-souboru)\n",
    "      - [Načtení webové stránky](#Načtení-webové-stránky)\n",
    "    - [Splittery](#Splittery)\n",
    "    - [Embeddings, vectorstore](#Embeddings,-vectorstore)\n",
    "    - [Similarity search](#Similarity-search)\n",
    "    - [Question answering](#Question-answering)\n",
    "    - [Automatické používání metadat](#Automatické-používání-metadat)\n",
    "    - [Prefix před fragmenty](#Prefix-před-fragmenty)\n",
    "  - [Kompletní ukázky](#Kompletní-ukázky)\n",
    "    - [Obyčejný chatbot](#Obyčejný-chatbot)\n",
    "    - [Q&A chatbot](#Q&A-chatbot)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 1. OpenAI"
   ],
   "id": "41228ad37420dcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "**Note**: For current Langchain usage, you should use `ChatOpenAI` from `langchain_openai`. Here's why:\n",
    "\n",
    "The `langchain_openai` import represents the newer, recommended approach after Langchain's restructuring of their OpenAI integrations. The older `langchain.llms.OpenAI` path is being phased out.\n",
    "\n",
    "Here's the correct modern usage:\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=\"your-api-key\"\n",
    ")\n",
    "```\n",
    "\n",
    "The key reasons for using `ChatOpenAI` from `langchain_openai`:\n",
    "\n",
    "1. It supports both chat and completion models seamlessly\n",
    "2. It provides better integration with newer OpenAI models\n",
    "3. It has more consistent handling of messages and responses\n",
    "4. It follows OpenAI's recommended practices for model interaction\n"
   ],
   "id": "6a8d70832fb5d4ff"
  },
  {
   "cell_type": "markdown",
   "id": "53e198da-9fac-41e2-9600-3f32ae798f47",
   "metadata": {},
   "source": [
    "### Tokeny\n",
    "Pro použití OpenAI člověk napřed musí sebe i svou kartu registrovat [zde](https://platform.openai.com/). Na stránkách je poté třeba nechat si vygenerovat API klíč, s jehož pomocí při RESTovém volání OpenAI pozná, co má komu vlastně naúčtovat. Ceník nalezneme [tady](https://openai.com/pricing). Je vhodné zdůraznit, že narozdíl od dejme tomu Midjourney si člověk nekupuje měsíční předplatné, nýbrž platí za míru používání modelů. Přesněji platí za počet modelem zpracovaných tokenů. Do toho se počítají jak tokeny do modelu vstupující, tak tokeny modelem produkované. A co že je vlastně onen token? Jedná se o skupinu písmen, která tvoří slovo anebo jeho část. V angličtině v průměru vychází 1 token na 0,75 slova, v jiných jazycích je poměr horší. Pro získání reálné představy doporučuji podívat se [sem](https://platform.openai.com/tokenizer). Krom očividného omezení peněženkou je součet vstupních a výstupních tokenů (a tak i délka textu) omezen pamětí modelu (kontextem z pricing stránky) - například pro 4K GPT-3.5 model není možné mít text (otázku a odpověď) delší než 4000 tokenů - nadbytečné tokeny budou oříznuty, resp. vůbec nevzniknou. O tom se člověk může předvědčit v \"klasickém\" webovém GUI rozhraní ChatGPT. Model sám o sobě si nepamatuje, co se dělo v předchozím hovoru. Proto se mu musí celá historie konverzace při každé interakci posílat nanovo. Po čase je historie příliš dlouhá a tak jsou staré tokeny zahozeny a model tudíž začne zapomínat začátky konverzací."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabb417-6202-4e39-b69d-3bada92a78ab",
   "metadata": {},
   "source": [
    "### API klíč\n",
    "První tutoriálový příklad na [stránkách openai balíčku](https://pypi.org/project/openai/) začíná s \n",
    "```python\n",
    "import openai\n",
    "openai.api_key = \"sk-...\"  # supply your API key however you choose\n",
    "```\n",
    "Jeho problém tkví ve skutečnosti, že je API klíč umístěn uprostřed kódu. Když si člověk nedá pozor a kód někam pošle (třeba na GitHub), může se jeho jménem (a jeho peněženkou) GPT modelů dotazovat celý internet. Proto bude lepší používat [python-dotenv](https://pypi.org/project/python-dotenv/) balíček. S jeho pomocí Python načte klíč z .env souboru a nastaví ho jako proměnnou prostředí. Jak to konkrétně provedeme? Nejprve si ve stejném adresáři, ve kterém se nalézá notebook (ipynb soubor), vytvoříme .env soubor (opravdu se takto jmenuje, tj. v názvu je jen přípona) a vložíme do něj\n",
    "```\n",
    "OPENAI_API_KEY=nazdar1234\n",
    "```\n",
    "Poté naimportujeme *dotenv* a provoláme funkci *load_dotenv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfaf82f1-23eb-4e48-98e4-295e08019685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:47:15.881406Z",
     "start_time": "2024-09-23T10:47:15.878293Z"
    }
   },
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6b05d-19c9-47fd-90c4-fd47298dbcef",
   "metadata": {},
   "source": [
    "Že se proměnná prostředí načetla ověříme s pomocí balíčku *os*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89a80cd8-c39f-48c5-8a37-512a9dc775f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c577a6-b8ec-4715-903f-027d59d8f30b",
   "metadata": {},
   "source": [
    "Pokud pracujeme s Azure OpenAI, je třeba inicializovat ještě několik dalších proměnných prostředí. Na těch už nic moc tajného není, tudíž mohou být volně v kódu. Vypadají nějak takto:\n",
    "```python\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_base = \"http://testingazureopenai.openai.azure.com\"\n",
    "```\n",
    "Api type bude asi vždy stejné, api version se bude s časem měnit a nakonec api base závisí na jméně openai resourcu. Tyto parametry se dají nejsnadněji zjistit, když v AzureAI studiu vlezeme do \"Chat Playground\" a v sekci \"Chat Session\" klepneme na \"View Code\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c3d54-2db3-46e4-ba2b-6c56b8f74e9f",
   "metadata": {},
   "source": [
    "### Jednoduchý příklad\n",
    "Nyní ale přikročme k prvnímu chatovacímu příkladu. Napřed si načteme potřebné balíčky a API klíč. Ten je potřeba vložit do *OpenAI* objektu, konkrétně do parametru *api_key*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4e0c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:47:23.390088Z",
     "start_time": "2024-09-23T10:47:23.376945Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ed9d6-f9fd-4e26-a99e-d82e041b7d54",
   "metadata": {},
   "source": [
    "Následně si vytvoříme objekt reprezentující chat a to s pomocí [*chat.completions.create*](https://platform.openai.com/docs/api-reference/chat/create). Tomu podhodíme jednak model, který chceme používat (parametr *model*), jednak dosavadní historii konverzace (parametr *messages*). Ta má podobu listu jsonů obsahujících jednak informaci o mluvčím (klíč \"role\" s hodnotami \"user\" či \"assistant\"), jednak samotnou promluvu (klíč \"content\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba9dfb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:48:55.017416Z",
     "start_time": "2024-09-22T19:48:54.159133Z"
    }
   },
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Ahoj.\"}\n",
    "    ],\n",
    "    model=llm_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c581e07-af0a-4a56-ba18-719a270045c3",
   "metadata": {},
   "source": [
    "Výstupem je následující věc. Všimněte si, že na konci dostáváme i informaci o počtu použitých tokenů."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae970788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:48:58.939970Z",
     "start_time": "2024-09-22T19:48:58.937465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Au5z2W1nUzZivxgCkj1uNrScLyc4A', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Ahoj! Jak se máš? Jak ti mohu pomoci?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1737932732, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_bd83329f63', usage=CompletionUsage(completion_tokens=17, prompt_tokens=11, total_tokens=28, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b960a-c9cc-4a33-8d97-401b5739963f",
   "metadata": {},
   "source": [
    "Pokud chceme ale jen odpověď chatbota, použijeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0724ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:49:09.611592Z",
     "start_time": "2024-09-22T19:49:09.609044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ahoj! Jak se máš? Jak ti mohu pomoci?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce50b5-a25a-466e-86a7-b8efc51ef306",
   "metadata": {},
   "source": [
    "Do *create* metody lze vložit další parametry. Například *temperature*, která s rostoucí hodnotou vede k chaotičtějším/kreativnějším odpovědím. Podle dokumentace je minimum 0, maximum 2 a default 1. Obecně ale hodnoty větší než 1 vedou obvykle k nesmyslné změti písmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc94acb-3396-4689-98af-c72f04c47de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ahoj! Jak ti mohu pomoci?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=llm_model_name,\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Ahoj.\"}\n",
    "  ],\n",
    "  temperature=2\n",
    ")\n",
    "\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0bebc-3e04-43dd-b853-7322c77d028d",
   "metadata": {},
   "source": [
    "Do *messages* lze vložit i tzv. systémový prompt, který chatbotu říká,v jaké roli má vlastně vystupovat. Takováto věc se označuje klíčem \"system\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70eaf800-d080-4b1f-8e5f-456b6a6f6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Určitě si dej oříšky! Jsou chutné, zdravé a dodají ti energii.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=llm_model_name,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Jsi veverka milující oříšky. Odpovídáš maximálně ve dvou větách.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Co bych si měl dát ke svačině?\"}\n",
    "  ],\n",
    "  temperature=0.7\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d415a8e",
   "metadata": {},
   "source": [
    "Pozn.: výše uvedené informace platí pro verzi balíčku openai větší nebo rovnou 1.0.0 ze září 2023. Pokud musíte pracovat s verzí starší, vypadá kód nějak takto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a1d9945-57ce-4741-83d4-62021dbcabf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# import dotenv\n",
    "\n",
    "# dotenv.load_dotenv()\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# completion = openai.ChatCompletion.create(\n",
    "#   model=llm_model_name,\n",
    "#   messages=[\n",
    "#     {\"role\": \"user\", \"content\": \"Ahoj.\"}\n",
    "#   ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8da9c-18af-4973-a488-0bc73f954164",
   "metadata": {},
   "source": [
    "Pokud pracujeme s Azure OpenAI, musíme napsat něco v následujícím duchu:\n",
    "```python\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=openai.api_version,\n",
    "    azure_endpoint=openai.api_base,\n",
    "    api_key=openai.api_key\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"jmeno_deploymentu\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Ahoj.\"}\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8850361-8b56-4533-8686-0a8275321f2d",
   "metadata": {},
   "source": [
    "### Chatbot\n",
    "Bohužel kód jak ho tady máme jako chatbot nefunguje - nijak se tu neukládá historie konverzací a pokaždé tak začínáme nanovo. V openai balíčku se žádná příhodná funkce ani třída nenalézá a tak si celou (byť v tomto případě ultra krátkou) logiku musíme napsat sami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dfecf1c-b50b-4075-b5ac-28ea82ec94e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:17:29.398591Z",
     "start_time": "2024-09-21T22:17:16.455323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pro ukončení konverzace napište 'exit'\n",
      "Chatbot: Mám se skvěle, právě jsem našla pár lahodných oříšků! Oříšky jsou pro mě všechno!\n"
     ]
    }
   ],
   "source": [
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"Jsi veverka milující oříšky. Odpovídáš maximálně ve dvou větách.\"}\n",
    "  ]\n",
    "\n",
    "print(\"Pro ukončení konverzace napište 'exit'\")\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"Uživatel: \")\n",
    "    if user_message == \"exit\":\n",
    "        break\n",
    "    chat_history.append(\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    )\n",
    "    conversation = client.chat.completions.create(\n",
    "        model=llm_model_name,\n",
    "        messages=chat_history,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    chatbot_answer = conversation.choices[0].message.content\n",
    "    chat_history.append(\n",
    "        {\"role\": \"assistant\", \"content\": chatbot_answer}\n",
    "    )\n",
    "    print(f\"Chatbot: {chatbot_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7604bd",
   "metadata": {},
   "source": [
    "Nize je ten samy kod, jen prepsany pomoci Walrus operatoru \":=\"."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "  ]\n",
    "\n",
    "print(\"Pro ukončení konverzace napište 'exit'\")\n",
    "\n",
    "while (user_message := input(\"Uživatel: \")) != \"exit\":\n",
    "    chat_history.append(\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    )\n",
    "    conversation = client.chat.completions.create(\n",
    "\n",
    "        model=llm_model_name,\n",
    "        messages=chat_history,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    chatbot_answer = conversation.choices[0].message.content\n",
    "    chat_history.append(\n",
    "        {\"role\": \"assistant\", \"content\": chatbot_answer}\n",
    "    )\n",
    "    print(f\"Chatbot: {chatbot_answer}\")\n",
    "#%% md\n",
    "## Promty"
   ],
   "id": "fde06e1850dd7e15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "def get_completion(prompt, client, model=llm_model_name):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "#%%\n",
    "get_completion(\"What is 1+1?\", client)\n",
    "#%%\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "#%%\n",
    "style = \"\"\"American English \\"
   ],
   "id": "69cddac300070360"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d3acee1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:52:31.311627Z",
     "start_time": "2024-09-22T19:52:30.192608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am quite frustrated that the lid of my blender came off and splattered my kitchen walls with smoothie. To make matters worse, the warranty does not cover the cost of cleaning up my kitchen. I would really appreciate your assistance with this issue. Thank you.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_completion(prompt, client)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8398575-3403-4630-a8ee-2b1a4ffcd106",
   "metadata": {},
   "source": [
    "# 2. Langchain\n",
    "Sice bychom mohli pokračovat v používání čistého openai balíčku, ale museli bychom přitom programovat více, než by bylo potřeba. Existuje totiž balíček Langchain, který hromadu práce udělá za nás.  \n",
    "Pozn.: I v případě Langchainu se s postupem času objevily změny. Původní kód bude dohledatelný v historii repa, které právě čtete. Krom změn kódu se objevila i potřeba nainstalovat si integrační balíček [langchain-openai](https://pypi.org/project/langchain-openai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e3ccf-1714-436f-957c-9c9ab46ae48a",
   "metadata": {},
   "source": [
    "### Šablony promptů"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539dd7f3-d650-4bd5-b9fb-11c9b81d5233",
   "metadata": {},
   "source": [
    "S pomocí šablon lze s minimální prací navíc přepoužívat i poměrně komplexní prompty. Pro názornost nicméně začněme něčím jednoduchým.  \n",
    "Napřed si napíšeme stringový základ šablony, ve kterém vložíme dodatečné parametry do složených závorek. Vlastně to vypadá jako f-stringy, jen to f-ko na začátku chybí. Šablonu jako takovou vytvoříme s pomocí *ChatPromptTemplate.from_template*, do které jako parametr vložíme string z předchozího kroku."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4ddbaf4f18463e97"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd46cf10-cd9f-4818-995e-010afbca9f0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:52:49.391902Z",
     "start_time": "2024-09-22T19:52:49.389825Z"
    }
   },
   "outputs": [],
   "source": [
    "language = \"anglickém\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcccc59-d2de-4f30-8b2a-e6e41023316c",
   "metadata": {},
   "source": [
    "Jako testovací text použijeme kousek z [wiki článku o veverkách](https://cs.wikipedia.org/wiki/Veverka_obecn%C3%A1)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "squirrel_text = \"\"\"\n",
    "Veverka obecná obvykle dorůstá 19 až 23 cm a dosahuje hmotnosti mezi 250 a 340 g někdy i víc. \n"
   ],
   "id": "1fddbacfbd632620"
  },
  {
   "cell_type": "markdown",
   "id": "6e35af2a-1645-43ec-b89d-7e63450525b9",
   "metadata": {},
   "source": [
    "Výsledek vypadá následně:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "jako „pokrývku“ těla při spánku, je 14,5 až 20 cm dlouhý.[3] Charakteristickým znakem veverky obecné jsou střapce \n",
    "chlupů na ušních boltcích směřující do špičky a viditelné především v zimním období. \n"
   ],
   "id": "2789a7e2c3512045"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56310e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:55:34.685564Z",
     "start_time": "2024-09-22T19:55:34.683249Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4b79c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:55:36.383516Z",
     "start_time": "2024-09-22T19:55:36.380967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "088d6049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:55:37.918690Z",
     "start_time": "2024-09-22T19:55:37.915067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf8ca85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:55:39.158338Z",
     "start_time": "2024-09-22T19:55:39.156368Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85445c6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:55:40.018713Z",
     "start_time": "2024-09-22T19:55:40.016678Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0388a597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:55:40.486025Z",
     "start_time": "2024-09-22T19:55:40.483728Z"
    }
   },
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "69e87022cb297279"
  },
  {
   "cell_type": "markdown",
   "id": "82241053-df9c-4834-85d6-ff873d1ac1f1",
   "metadata": {},
   "source": [
    "Když pak do metody *invoke* našeho chatovacího modelu vložíme jako parametr šablonu a výsledku se zeptáme na atribut *content*, získáme odpověď. Vidíme, že anglický jazyk sice model pochopil, ale shrnutí zabralo více než požadovanou jednu větu. no, shrnutí - text vypadá spíš jako překlad..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18a12548-0533-4904-8abb-4d564f029715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:01:11.656041Z",
     "start_time": "2024-09-22T20:01:10.743228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The common squirrel typically measures 19 to 23 cm in length, weighs between 250 and 340 g, has a bushy tail for balance and warmth, and features distinctive ear tufts and sharp, curved claws for climbing.\n"
     ]
    }
   ],
   "source": [
    "summary_response = chat.invoke(filled_template)\n",
    "print(summary_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e2b9b",
   "metadata": {},
   "source": [
    "Jeste zkusime, co to udela, kdyz to zavolame na ten druhy priklad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19380b7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:01:59.494726Z",
     "start_time": "2024-09-22T20:01:58.628860Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat.invoke(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8141c664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:02:01.924293Z",
     "start_time": "2024-09-22T20:02:01.922239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am quite frustrated that the lid of my blender came off and splattered smoothie all over my kitchen walls. To make matters worse, the warranty does not cover the cost of cleaning up my kitchen. I would really appreciate your assistance with this issue. Thank you.\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe752345",
   "metadata": {},
   "source": [
    "Dalsi priklad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcd66c3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:02:31.775784Z",
     "start_time": "2024-09-22T20:02:31.773830Z"
    }
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f280e196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:02:34.225727Z",
     "start_time": "2024-09-22T20:02:34.224076Z"
    }
   },
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f98bc85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:02:51.680230Z",
     "start_time": "2024-09-22T20:02:51.678046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "639226ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:02:55.265201Z",
     "start_time": "2024-09-22T20:02:54.033462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy, esteemed customer! I be regretful to inform ye that the warranty be not coverin' the expenses for cleanin' yer galley, as it appears ye may have misused yer blender by forgettin' to secure the lid afore settin' it to whirl. Aye, 'tis a bit of tough luck, indeed! Fair winds to ye, and may we cross paths again!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat.invoke(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499ff42-49f0-4f3c-b0e6-11e09215cdff",
   "metadata": {},
   "source": [
    "Člověka by napadlo, že se text bez ztráty informací do jedné věty možná shrnout nedá a nízká teplota zabraňuje kreativnější práci s informacemi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a7f8b43-889c-480d-b36d-14664ec8606c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:03:26.497128Z",
     "start_time": "2024-09-22T20:03:25.469062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eurasian squirrel typically measures 19 to 23 cm in length, weighs between 250 and 340 g, and is characterized by its bushy tail, ear tufts, and sharp curved claws for climbing.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(\n",
    "    temperature=0.7, \n",
    "    model_name=llm_model_name\n",
    ")\n",
    "summary_response = chat.invoke(filled_template)\n",
    "print(summary_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2210f-d44d-4277-8fd4-83bc67e32dd6",
   "metadata": {},
   "source": [
    "Nicméně možná si model zkrátka co se instrukcí týče jenom nerozumí s češtinou tak dobře jako s angličtinou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95b4ff0e-6ebd-4891-9da2-4878126475fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:03:45.417133Z",
     "start_time": "2024-09-22T20:03:44.041319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veverka obecná dorůstá délky 19 až 23 cm, váží 250 až 340 g, má huňatý ocas dlouhý 14,5 až 20 cm a charakteristické střapce na uších, přičemž její ostré drápy jí usnadňují lezení po stromech.\n"
     ]
    }
   ],
   "source": [
    "template_text = \"\"\"\n",
    "Summarize the text surrounded by three quotation marks in one sentence. The answer must be written in {language} language. \n",
    "Text: '''{text}'''\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_text)\n",
    "language = \"czech\"\n",
    "\n",
    "filled_template = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=squirrel_text\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.0, \n",
    "    model_name=llm_model_name\n",
    ")\n",
    "summary_response = chat.invoke(filled_template)\n",
    "print(summary_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11a4d7-4275-43b9-8019-96a4fba3142b",
   "metadata": {},
   "source": [
    "Možná se nám někdy stane, že bude odpověď useknutá. To nejspíš bude dáno skutečností, že defaultní hodnota parametru *max_tokens* pro *ChatOpenAI*, která má velikost 256, na celou odpověď zkrátka nestačí. Pokud nechceme být omezováni (resp. pokud chceme být omezeni jen maximální velikostí kontextového okna), vložíme do parametru -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed485a07-ae8a-42cb-9395-e41d0567806a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:35:22.221405Z",
     "start_time": "2024-09-22T20:35:21.222518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veverka obecná dorůstá délky 19 až 23 cm, váží 250 až\n"
     ]
    }
   ],
   "source": [
    "template_text = \"\"\"\n",
    "Summarize the text surrounded by three quotation marks in one sentence. The answer must be written in {language} language. \n",
    "Text: '''{text}'''\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_text)\n",
    "language = \"czech\"\n",
    "\n",
    "filled_template = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    text=squirrel_text\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.0, \n",
    "    model_name=llm_model_name,\n",
    "    max_tokens=22\n",
    ")\n",
    "summary_response = chat.invoke(filled_template)\n",
    "print(summary_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d51d8f6",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399bc41-54db-44a3-b20d-8f3d17f20bcc",
   "metadata": {},
   "source": [
    "Zkusme nyní lehce komplikovanější šablonu, která se bude koukat na [popis hry Na křídlech](https://www.tlamagames.com/deskove-hry/wingspan/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75715ffa-6a2f-49ab-bf9b-d667a2d82966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:35:53.507452Z",
     "start_time": "2024-09-22T20:35:52.621896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"minimal_age\": 14,\n",
      "  \"number_of_players\": \"1-5\",\n",
      "  \"victory_condition\": \"hráč s nejvíce body po 4 kolech\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "wingspan_text = \"\"\"\n",
    "Wingspan je kompetitivní středně těžká hra využívající karty a engine building mechanismus.\n",
    "\n",
    "Stáváte se nadšenými ornitology a sběrateli a snažíte se objevit a přilákat ty nejzajímavější ptáky da Vaší sítě rezervací. Každý pták posiluje řetěz kombinací pro daný habitat (akci). Tyto habitaty jsou zaměřeny na několik klíčových oblastí rozvoje:\n",
    "\n",
    "    Dostat žetony jídla výběrem kostky z krmítka (dice tower)\n",
    "    Kladení vajec s využitím miniaturních vajec v různých barvách\n",
    "    Dobrání ze stovek unikátních karet ptáků a zahrání těchto karet\n",
    "\n",
    "Vítězem je ten hráč, který má po 4 kolech nejvíce bodů.\n",
    "\n",
    "Pokud máte rádi hry jako Terraforming Mars a Gizmos, tak by tahle hra měla zalétnout na Váš stůl.\n",
    "\n",
    "Hra je určena pro 1-5 hráčů od 14 let.\n",
    "\n",
    "Pravidla i herní materiál je v angličtině.\n",
    "\"\"\"\n",
    "\n",
    "template_text = \"\"\"\n",
    "From text surrounded by by three quotation marks extract following information.\n",
    "\n",
    "1) Minimal age\n",
    "2) Number of players\n",
    "3) Victory condition\n",
    "\n",
    "The answer must be written in {language} language in {format} format. \n",
    "\n",
    "Text: '''{text}'''\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_text)\n",
    "language = \"czech\"\n",
    "format = \"json\"\n",
    "\n",
    "filled_template = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    format=format,\n",
    "    text=wingspan_text\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.0, \n",
    "    model_name=llm_model_name\n",
    ")\n",
    "summary_response = chat.invoke(filled_template)\n",
    "print(summary_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20f0a950-ab0f-429d-be0f-328a59743f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:36:52.201023Z",
     "start_time": "2024-09-22T20:36:51.082197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Minimal age: 14  \n",
      "2) Number of players: 1-5  \n",
      "3) Victory condition: The player with the most points after 4 rounds wins.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template_text)\n",
    "language = \"english\"\n",
    "format = \"plain text\"\n",
    "\n",
    "filled_template = prompt_template.format_messages(\n",
    "    language=language,\n",
    "    format=format,\n",
    "    text=wingspan_text\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.0, \n",
    "    model_name=llm_model_name\n",
    ")\n",
    "summary_response = chat.invoke(filled_template)\n",
    "print(summary_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd17bb-7854-4268-a6e8-6e59fccbd576",
   "metadata": {},
   "source": [
    "V případě práce s Azure OpenAI budeme namísto *ChatOpenAI* používat *AzureChatOpenAI*. Rozdíl je i v parametrech - nebudeme specifikovat *model_name*, nýbrž *deployment_name*. V Azuru se totiž vytváří svého druhu instance modelů mimo (obvykle) v GUI, přičemž jsou pojmenovány právě jako deploymenty.  Vytvořené deploymenty člověk nalezne v Azure AI studiu v sekci \"deployments\" (do konstruktoru dáváme název z prvního sloupce tabulky).\n",
    "\n",
    "```python\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"api_klíč\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"endpoint ve formátu ala http://testingazureopenai.openai.azure.com\"\n",
    "chat = AzureChatOpenAI(deployment_name=\"jmeno_deploymentu\", temperature=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba1771",
   "metadata": {},
   "source": [
    "Jiny priklad, jak definovat vystup z LLM modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2c0edff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:37:04.032941Z",
     "start_time": "2024-09-22T20:37:04.029892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\"
   ],
   "id": "4184b002a060e31"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "795c6da8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:38:54.364559Z",
     "start_time": "2024-09-22T20:38:54.361577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea50fa0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# You will get an error by running this line of code \u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# because'gift' is not a dictionary\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# 'gift' is a string\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgift\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# You will get an error by running this line of code \n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "response.content.get('gift')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "#%%\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)\n",
    "#%%\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model_name)\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)\n",
    "#%%\n",
    "type(response.content)\n",
    "#%%\n",
    "# You will get an error by running this line of code \n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "response.content.get('gift')\n",
    "#%% md\n",
    "## Parse the LLM output string into a Python dictionary\n",
    "#%%\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "#%%\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "\n",
    "                                      did it take for the product\\\n",
    "\n",
    "                                      to arrive? If this \\\n",
    "\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ],
   "id": "86b5b93342c0a3ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)",
   "id": "afd3950fc0647efd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "#%%"
   ],
   "id": "f0f4d66ca8b935c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "review_template_2 = \"\"\"\\\n",
    "\n",
    "For the following text, extract the following information:\n"
   ],
   "id": "216fea09e1388137"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dcaa5b94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:43:16.025257Z",
     "start_time": "2024-09-22T20:43:16.022963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": \"True\",\n",
      "\t\"delivery_days\": \"2\",\n",
      "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)\n",
    "#%%\n",
    "print(messages[0].content)\n",
    "#%%\n",
    "response = chat.invoke(messages)\n",
    "#%%\n",
    "print(response.content)\n",
    "#%%\n",
    "output_dict = output_parser.parse(response.content)\n",
    "#%%\n",
    "output_dict\n",
    "#%%\n",
    "type(output_dict)\n",
    "#%%"
   ],
   "id": "1b90872c915092c"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52c8d9e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:43:22.818522Z",
     "start_time": "2024-09-22T20:43:22.815554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d7fe1-aac0-4863-beeb-7e33e91bb277",
   "metadata": {},
   "source": [
    "## Chains\n",
    "Je načase přistoupit k věci, podle které nese Langchain své jméno - k chainům. Ty jsou [definovány](https://python.langchain.com/docs/modules/chains/) jako posloupnosti volání komponent, které mohou obsahovat další chainy. To by ve výsledku mělo vést k větší přehlednosti a snazší udržování kódu.\n",
    "#### LLMChain \n",
    "Viz. [LLMChain](https://python.langchain.com/docs/modules/chains/foundational/llm_chain)\n",
    "Jedná se o asi nejjednodušší chain. Člověk do něj při inicializaci nasype template a jazykový model. Při samotném použití *LLMChain* provoláme s hodnotou proměnné, která se dosadí do šablony. Výsledný textový řetězec poputuje do jazykového modelu a následně obdržíme výsledek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1bedc7db58b4d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:51:29.263238Z",
     "start_time": "2024-09-22T20:51:29.244431Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.95, model=llm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565c7f63a6b559f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:51:30.514842Z",
     "start_time": "2024-09-22T20:51:30.512810Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246c00ba72989906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:51:31.188104Z",
     "start_time": "2024-09-22T20:51:31.186062Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_45286/4149722537.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f30020986f1bc6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:51:33.761988Z",
     "start_time": "2024-09-22T20:51:32.281606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': 'Queen Size Sheet Set',\n",
       " 'text': \"Choosing a name for a company that specializes in queen size sheet sets can convey quality, comfort, and style. Here are some suggestions that reflect those attributes:\\n\\n1. **Queen's Rest Sheets**\\n2. **Majestic Slumber**\\n3. **Regal Comfort Linens**\\n4. **Purely Queen**\\n5. **Dreamy Queen Sheets**\\n6. **Sovereign Sheets**\\n7. **Royal Sleep Sets**\\n8. **Queen Size Haven**\\n9. **Crown Comfort Linens**\\n10. **Elite Queen Sheets**\\n\\nConsider your target audience and brand values when selecting a name, and make sure to check for domain availability if you plan to create a website.\"}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "llm_chain.invoke(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0852cc-8d27-42c0-ac65-70618f0f8318",
   "metadata": {},
   "source": [
    "Obvykle ale nechceme slovníkovou omáčku okolo, ale jen samotnou odpověď. Kdysi bylo řešením použít metodu *run*. Ta ale brzo bude odstraněna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e416d9d-89cd-43af-8b7a-72638f79d8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:52:12.080415Z",
     "start_time": "2024-09-22T20:52:10.773033Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_45286/1011596546.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm_chain.run(product)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Choosing the best name for a company that specializes in queen size sheet sets can depend on the brand's identity, target audience, and overall vibe. Here are some suggestions:\\n\\n1. **QueenSheets Co.**\\n2. **RoyalRest Linens**\\n3. **Crown Comforts**\\n4. **Queen’s Haven Bedding**\\n5. **Serenity Sheets**\\n6. **Majestic Linens**\\n7. **Dreamy Queen Sheets**\\n8. **Regal Rest Sheets**\\n9. **Queen Size Comforts**\\n10. **Sumptuous Sleep Co.**\\n\\nMake sure to check for trademark availability and domain name availability if you plan to create a website.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "llm_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6aa76f-4912-4912-815a-6fd39fcedee6",
   "metadata": {},
   "source": [
    "Tudíž musíme explicitně \"slovníkově\" zmínit, co vlastně chceme vidět."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1ffc2bbb-49cc-456a-89ad-85723bdb4848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T20:52:47.595952Z",
     "start_time": "2024-09-22T20:52:46.064680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are some ideas for a company that specializes in queen-size sheet sets:\\n\\n1. **Queen's Comfort**\\n2. **Royal Sheets**\\n3. **Regal Rest**\\n4. **Queen's Haven**\\n5. **Majestic Linens**\\n6. **Serene Sleep**\\n7. **Lavish Layers**\\n8. **Dreamy Queen**\\n9. **Plush Queen**\\n10. **Comfort Cove**\\n\\nConsider what resonates with your target audience while reflecting the quality and comfort of the products you offer.\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "llm_chain.invoke(product)[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891a943-ad6e-4ce3-8c3a-1ba79ba655de",
   "metadata": {},
   "source": [
    "V případě, že má jít do šablony více proměnných, musíme tyto proměnné do metody *invoke* předávat jako slovník v parametru *input*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "576ae240-e315-4598-a8a7-3ff207879ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:00:33.016623Z",
     "start_time": "2024-09-22T21:00:33.011789Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template_two_var = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Write the best name for {pet}. Answer should be in one sentence, in {language} language, but should contain reasoning.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bf26fdb72f75a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:00:36.228370Z",
     "start_time": "2024-09-22T21:00:36.225756Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_chain_two_var = LLMChain(llm=llm, prompt=prompt_template_two_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc6ccaec7fa236bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:00:38.914068Z",
     "start_time": "2024-09-22T21:00:38.050699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pet': 'cat',\n",
       " 'language': 'german',\n",
       " 'text': 'Der beste Name für eine Katze ist \"Flauschi\", weil er die weiche, kuschelige Natur von Katzen perfekt einfängt.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vars = {\"pet\": \"cat\", \"language\": \"german\"}\n",
    "llm_chain_two_var.invoke(input=input_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ce79d-75f5-433b-9c60-7ccb8b3ff790",
   "metadata": {},
   "source": [
    "Pokud člověk chce odpovědi na více separátních vstupů, není třeba popořadě manuálně provolávat model, ale lze použít metodu *apply*. Té se podhodí list slovníků s proměnnými."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb81771cd5b6aee",
   "metadata": {},
   "source": [
    "Vzhledem k rychlosti vyvoje LangChainu, je nasledujici metoda uz nefunkcni. Nevim proc, proste uz nefunguje. A kdyz \n",
    "clovek googlí, tak na strance \n",
    "[dokumentace k LLMChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.LLMChain) se dozvi, ze LLMChain byl taky deprecated...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74a0d464-9309-4ff6-b130-7ba2d6bb89ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:04:07.069203Z",
     "start_time": "2024-09-22T21:04:06.985734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'The best name for a parrot is \"Echo,\" as it reflects their remarkable ability to mimic sounds and voices, embodying the playful and communicative nature of these vibrant birds.'},\n",
       " {'text': 'Nejlepší jméno pro medvěda je \"Brouček\", protože vyjadřuje jeho roztomilost a zároveň sílu, kterou v sobě skrývá.'},\n",
       " {'text': 'Der beste Name für einen Affen ist \"Sprinkles\", weil er die verspielte und bunte Natur dieser lebhaften Tiere widerspiegelt.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list = [\n",
    "    {\"pet\": \"parrot\", \"language\":\"english\"},\n",
    "    {\"pet\": \"bear\", \"language\":\"czech\"},\n",
    "    {\"pet\": \"monkey\", \"language\":\"german\"}\n",
    "]\n",
    "\n",
    "llm_chain_two_var.apply(input_list)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Sekvenční chainy jsou určeny pro situace, kdy výstup jednoho provolání modelu vkládáme jako vstup do provolání druhého. Existují ve dvou variantách:\n",
    "- SimpleSequentialChain, který má jen jeden vstup a jeden výstup\n",
    "\n",
    "- SequentialChain umožňující existenci více vstupů a výstupů\n"
   ],
   "id": "adb1a5c15aa87f2a"
  },
  {
   "cell_type": "markdown",
   "id": "b7b8f8d6-798d-460c-960b-612e8f96d6c9",
   "metadata": {},
   "source": [
    "Následně s jejich pomocí vytvoříme SimpleSequentialChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "168a9c7a-a259-4dc1-9d40-3fbd42c43ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:17:13.758189Z",
     "start_time": "2024-09-22T21:17:13.756016Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[post_chain, summary_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb6010-d00b-4b5d-86f8-6aea084e2fe0",
   "metadata": {},
   "source": [
    "Ten uvedeme do provozu provoláním metody *invoke*. Do té vložíme hodnotu, která bude dosazena do prvního LLMChainu (pořadí chainů dáno pořadím v listu vkládaného do parametru *chains*).  \n",
    "Jelikož jsme *SimpleSequentialChain* vytvořili s parametrem *verbose*=True, vidíme i mezivýsledky - výstup prvního a druhého LLMChainu v modré resp. žluté barvě. A ano, jsou zde warningy. Asi se jedná o nějaké volání v pozadí; zatím jsem nepřišel na to, co mám čím nahradit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "773304e6-0b89-45ff-b8b1-7f70f7913592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:17:19.762144Z",
     "start_time": "2024-09-22T21:17:16.322865Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3m🌟 Exciting Times at Hamster Food Inc.! 🌟\n",
      "\n",
      "We're thrilled to announce some incredible updates from our team dedicated to creating the finest, healthiest, and tastiest options for our furry friends! 🐹✨\n",
      "\n",
      "At Hamster Food Inc., we believe that every pet deserves the best nutrition possible. That's why we've been hard at work developing new formulas packed with essential nutrients that support the health and happiness of hamsters of all breeds. From high-quality grains to fresh fruits and veggies, our products are designed to mimic a hamster’s natural diet while ensuring they get the energy they need to play and thrive.\n",
      "\n",
      "We can't wait to share more about our upcoming product launches, sustainability initiatives, and partnerships with pet organizations that align with our mission of promoting responsible pet care.\n",
      "\n",
      "A huge thank you to our dedicated team, our loyal customers, and our adorable little pals for inspiring us every day! 🥳\n",
      "\n",
      "Stay tuned for more updates and let’s keep making the world a better place for our pets—one tasty bite at a time! 🍽️💚\n",
      "\n",
      "#HamsterFoodInc #PetNutrition #HappyPets #SustainablePetCare #InnovationInPetFood #FurryFriends\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3mHamster Food Inc. is excited to announce new, nutritious product updates aimed at improving the health and happiness of hamsters, while also emphasizing sustainability and responsible pet care.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "post_summary = overall_chain.invoke(\"Hamster food Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28539572-1485-4766-a8a0-561da430286c",
   "metadata": {},
   "source": [
    "Výsledek jsme si uložili do proměnné *post_summary*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3fa8b01-4550-4703-b080-b02172f6ea2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:17:23.897856Z",
     "start_time": "2024-09-22T21:17:23.895292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hamster food Inc.',\n",
       " 'output': 'Hamster Food Inc. is excited to announce new, nutritious product updates aimed at improving the health and happiness of hamsters, while also emphasizing sustainability and responsible pet care.'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_summary"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "Jelikož jsme *SimpleSequentialChain* vytvořili s parametrem *verbose*=True, vidíme i mezivýsledky - výstup prvního a druhého LLMChainu v modré resp. žluté barvě. A ano, jsou zde warningy. Asi se jedná o nějaké volání v pozadí; zatím jsem nepřišel na to, co mám čím nahradit.\n",
    "#%%\n",
    "post_summary = overall_chain.invoke(\"Hamster food Inc.\")\n",
    "#%% md\n",
    "Výsledek jsme si uložili do proměnné *post_summary*.\n",
    "#%%\n",
    "post_summary\n",
    "#%% md\n",
    "S *verbose*=False žádné barevné výsledky neuvidíme.\n",
    "#%%"
   ],
   "id": "c58745b3746c93a9"
  },
  {
   "cell_type": "markdown",
   "id": "c720438d-e6b6-49b4-a1f0-80b3101edaba",
   "metadata": {},
   "source": [
    "Nyní se podívejme na *SequentialChain*. Zde první LLMChain bude mít dva vstupy. Navíc tu v parametru *output_key* říkáme, do jak pojmenované proměnné zpracované dalšími LLMChainy se má výstup tohoto chainu uložit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "db3cd209-f82b-41d0-a576-139bade3c728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:18:34.592177Z",
     "start_time": "2024-09-22T21:18:34.577937Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.95, model=llm_model_name)\n",
    "\n",
    "prompt_template_post = ChatPromptTemplate.from_template(\n",
    "    \"Write a LinkedIn post for company {company} with length of {sentences_count} sentences.\"\n",
    ")\n",
    "\n",
    "post_chain = LLMChain(llm=chat, prompt=prompt_template_post, output_key=\"linkendin_post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e026ead-6c99-4c00-be14-35a0cad3b3dc",
   "metadata": {},
   "source": [
    "Dva vstupy a jeden *output_key* má i druhý *LLMChain*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aaf3aa57-2e0b-4d80-a525-4abefb236af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:19:02.347188Z",
     "start_time": "2024-09-22T21:19:02.344248Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template_summary = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Summarize the text surrounded by three quotation marks in one sentence. Use {language} language.\n",
    "    Text: '''{linkendin_post}''''\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "summary_chain = LLMChain(llm=chat, prompt=prompt_template_summary, output_key=\"summarization\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dbb3592f4845cdcc"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a2deee5c-cbb1-4640-bf69-9780abddff15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:24:08.263765Z",
     "start_time": "2024-09-22T21:24:06.202376Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'language': 'czech',\n",
       " 'sentences_count': 3,\n",
       " 'company': 'Turtles speedtravel',\n",
       " 'summarization': 'Turtles SpeedTravel přináší nové inovativní řešení pro rychlé a pohodlné cestování, která umožní objevovat nové obzory.'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain.invoke({\"language\":\"czech\", \"sentences_count\":3, \"company\":\"Turtles speedtravel\"})"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[post_chain, summary_chain],\n",
    "    input_variables=[\"company\", \"sentences_count\", \"language\"],\n",
    "\n",
    "    output_variables=[\"summarization\"],\n",
    "    verbose=True\n",
    ")\n",
    "#%% md\n",
    "Aktivovat *SequentialChain* můžeme vložením slovníku se záznamem pro každou *input_variable* do instance chainu.\n",
    "#%%\n",
    "overall_chain.invoke({\"language\":\"czech\", \"sentences_count\":3, \"company\":\"Turtles speedtravel\"})\n",
    "#%% md"
   ],
   "id": "74b0e5f0bac8392"
  },
  {
   "cell_type": "markdown",
   "id": "549bd846",
   "metadata": {},
   "source": [
    "Jiny priklad s vice prompty a vice output_variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40956c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:24:47.776497Z",
     "start_time": "2024-09-22T21:24:47.761100Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model_name)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5bfba736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:24:53.928342Z",
     "start_time": "2024-09-22T21:24:53.926102Z"
    }
   },
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d0d3832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:25:00.288675Z",
     "start_time": "2024-09-22T21:25:00.285407Z"
    }
   },
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3e11f62b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:25:05.063576Z",
     "start_time": "2024-09-22T21:25:05.060803Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "021f98d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:25:09.673208Z",
     "start_time": "2024-09-22T21:25:09.670297Z"
    }
   },
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "646cf406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:32:10.174116Z",
     "start_time": "2024-09-22T21:32:06.387335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:SequentialChain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"Review\": \"Jidlo nebylo spatny, ale zadna slava to taky nebyla\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"Review\": \"Jidlo nebylo spatny, ale zadna slava to taky nebyla\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Translate the following review to english:\\n\\nJidlo nebylo spatny, ale zadna slava to taky nebyla\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_28553/114229642.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  overall_chain(review)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain > llm:ChatOpenAI] [727ms] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The food wasn't bad, but it wasn't anything special either.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The food wasn't bad, but it wasn't anything special either.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 13,\n",
      "                \"prompt_tokens\": 31,\n",
      "                \"total_tokens\": 44,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_bd83329f63\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-cf144d4f-2968-4864-928b-bed5a0ab5a2d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 31,\n",
      "              \"output_tokens\": 13,\n",
      "              \"total_tokens\": 44\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 13,\n",
      "      \"prompt_tokens\": 31,\n",
      "      \"total_tokens\": 44,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_bd83329f63\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain] [728ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"English_Review\": \"The food wasn't bad, but it wasn't anything special either.\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"Review\": \"Jidlo nebylo spatny, ale zadna slava to taky nebyla\",\n",
      "  \"English_Review\": \"The food wasn't bad, but it wasn't anything special either.\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Can you summarize the following review in 1 sentence:\\n\\nThe food wasn't bad, but it wasn't anything special either.\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain > llm:ChatOpenAI] [545ms] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The food was mediocre, lacking any standout qualities.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The food was mediocre, lacking any standout qualities.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 11,\n",
      "                \"prompt_tokens\": 30,\n",
      "                \"total_tokens\": 41,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_72ed7ab54c\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ad92f2c5-c259-4c12-a5c7-9c38d0f6a929-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 30,\n",
      "              \"output_tokens\": 11,\n",
      "              \"total_tokens\": 41\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 11,\n",
      "      \"prompt_tokens\": 30,\n",
      "      \"total_tokens\": 41,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_72ed7ab54c\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain] [546ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"summary\": \"The food was mediocre, lacking any standout qualities.\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"Review\": \"Jidlo nebylo spatny, ale zadna slava to taky nebyla\",\n",
      "  \"English_Review\": \"The food wasn't bad, but it wasn't anything special either.\",\n",
      "  \"summary\": \"The food was mediocre, lacking any standout qualities.\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What language is the following review:\\n\\nJidlo nebylo spatny, ale zadna slava to taky nebyla\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain > llm:ChatOpenAI] [625ms] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The review is in Czech. It translates to: \\\"The food wasn't bad, but it wasn't anything special either.\\\"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The review is in Czech. It translates to: \\\"The food wasn't bad, but it wasn't anything special either.\\\"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 24,\n",
      "                \"prompt_tokens\": 31,\n",
      "                \"total_tokens\": 55,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_72ed7ab54c\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-aea89de7-81f6-4f5a-8064-0eb5c2ebd81d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 31,\n",
      "              \"output_tokens\": 24,\n",
      "              \"total_tokens\": 55\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 24,\n",
      "      \"prompt_tokens\": 31,\n",
      "      \"total_tokens\": 55,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_72ed7ab54c\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain] [626ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"language\": \"The review is in Czech. It translates to: \\\"The food wasn't bad, but it wasn't anything special either.\\\"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"Review\": \"Jidlo nebylo spatny, ale zadna slava to taky nebyla\",\n",
      "  \"English_Review\": \"The food wasn't bad, but it wasn't anything special either.\",\n",
      "  \"summary\": \"The food was mediocre, lacking any standout qualities.\",\n",
      "  \"language\": \"The review is in Czech. It translates to: \\\"The food wasn't bad, but it wasn't anything special either.\\\"\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Write a follow up response to the following summary in the specified language:\\n\\nSummary: The food was mediocre, lacking any standout qualities.\\n\\nLanguage: The review is in Czech. It translates to: \\\"The food wasn't bad, but it wasn't anything special either.\\\"\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain > llm:ChatOpenAI] [1.60s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Děkuji za vaši recenzi. I když vás jídlo nezaujalo, zajímalo by mě, co konkrétně byste doporučil vylepšit. Bylo by skvělé vědět, jaké prvky by mohly jídlo posunout na vyšší úroveň a co byste rádi viděli více. Vaše názory jsou pro nás cenné!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Děkuji za vaši recenzi. I když vás jídlo nezaujalo, zajímalo by mě, co konkrétně byste doporučil vylepšit. Bylo by skvělé vědět, jaké prvky by mohly jídlo posunout na vyšší úroveň a co byste rádi viděli více. Vaše názory jsou pro nás cenné!\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 90,\n",
      "                \"prompt_tokens\": 58,\n",
      "                \"total_tokens\": 148,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_72ed7ab54c\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-56f57162-98b7-45de-ae5b-0079c7df6029-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 58,\n",
      "              \"output_tokens\": 90,\n",
      "              \"total_tokens\": 148\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 90,\n",
      "      \"prompt_tokens\": 58,\n",
      "      \"total_tokens\": 148,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_72ed7ab54c\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:SequentialChain > chain:LLMChain] [1.60s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"followup_message\": \"Děkuji za vaši recenzi. I když vás jídlo nezaujalo, zajímalo by mě, co konkrétně byste doporučil vylepšit. Bylo by skvělé vědět, jaké prvky by mohly jídlo posunout na vyšší úroveň a co byste rádi viděli více. Vaše názory jsou pro nás cenné!\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:SequentialChain] [3.50s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"English_Review\": \"The food wasn't bad, but it wasn't anything special either.\",\n",
      "  \"summary\": \"The food was mediocre, lacking any standout qualities.\",\n",
      "  \"followup_message\": \"Děkuji za vaši recenzi. I když vás jídlo nezaujalo, zajímalo by mě, co konkrétně byste doporučil vylepšit. Bylo by skvělé vědět, jaké prvky by mohly jídlo posunout na vyšší úroveň a co byste rádi viděli více. Vaše názory jsou pro nás cenné!\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': 'Jidlo nebylo spatny, ale zadna slava to taky nebyla',\n",
       " 'English_Review': \"The food wasn't bad, but it wasn't anything special either.\",\n",
       " 'summary': 'The food was mediocre, lacking any standout qualities.',\n",
       " 'followup_message': 'Děkuji za vaši recenzi. I když vás jídlo nezaujalo, zajímalo by mě, co konkrétně byste doporučil vylepšit. Bylo by skvělé vědět, jaké prvky by mohly jídlo posunout na vyšší úroveň a co byste rádi viděli více. Vaše názory jsou pro nás cenné!'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = \"Jidlo nebylo spatny, ale zadna slava to taky nebyla\"\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af783813-1f49-45fc-aa13-33fcc8aa06a6",
   "metadata": {},
   "source": [
    "#### RouterChain\n",
    "RouterChain dokáže posoudit, na který z podřízených chainů poslat uživatelský vstup. Narozdíl od sekvenčích chainů, kde je posloupnost řízena jménem proměnných (a tedy volbou programátora), v případě LLMRouterChain se o rozzařování skutečně stará jazykový model.  \n",
    "Pozn.: v této podkapitole čerpám z langchainového kurzu z deeplearning.ai opravdu extenzivně - MULTI_PROMPT_ROUTER_TEMPLATE je převzat beze změn metodou ctrl+C, ctrl+V.  \n",
    "Nejprve si vytvoříme základy šablon pro programování a pro vaření."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5b5ac58-2447-4fe6-89b8-f00ec7c81224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "it_problem_template = \"\"\"You are a very smart programmer. \n",
    "You are great at answering questions about computers, programming and IT in a concise \n",
    "and easy to understand manner. \n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "cooking_template = \"\"\"Your are master chef, expert at cooking and food preparation. \n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "\n",
    "Here is a question:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"programming\", \n",
    "        \"description\": \"Good for answering questions about programming and IT\", \n",
    "        \"prompt_template\": it_problem_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"food\", \n",
    "        \"description\": \"Good for answering questions about food and cooking\", \n",
    "        \"prompt_template\": cooking_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0867f-41c0-4e24-87a1-0123065299c1",
   "metadata": {},
   "source": [
    "Následně dojde k vytvoření instance jazykového modelu a listu slovníku o podchainech."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "destination_chains = {}\n",
    "\n",
    "for one_prompt_info in prompt_infos:\n",
    "    name = one_prompt_info[\"name\"]\n",
    "    prompt_template = one_prompt_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=chat, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{one_prompt_info['name']}: {one_prompt_info['description']}\" for one_prompt_info in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "print(destinations_str)\n",
    "#%% md\n",
    "Vytvoříme si i defaultní podchain, kam půjdou uživatelské dotazy, které se nevejdou nikam jinam.\n",
    "#%%\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=chat, prompt=default_prompt)\n",
    "#%% md\n",
    "Následuje extenzivní popis router šablony.\n",
    "#%%\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \n",
    "language model select the model prompt best suited for the input. \n",
    "You will be given the names of the available prompts and a \n",
    "description of what the prompt is best suited for. \n",
    "You may also revise the original input if you think that revising"
   ],
   "id": "8c3cdb4215b009a0"
  },
  {
   "cell_type": "markdown",
   "id": "9711e81d-f1f8-4ba2-a8f6-ed2afcde68cb",
   "metadata": {},
   "source": [
    "Následuje extenzivní popis router šablony."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string - name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string - a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \n",
    "\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>"
   ],
   "id": "be52e1a3745f4c3e"
  },
  {
   "cell_type": "markdown",
   "id": "a731d4b6-a561-4bcf-adc6-9df69dbbd3d2",
   "metadata": {},
   "source": [
    "A nakonec tu máme chain, který *LLMRouterChain* obaluje a spojuje ho s podchainy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c1b49296-d632-48c7-896d-0da0b9cad176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_28553/3038952769.py:1: LangChainDeprecationWarning: Use RunnableLambda to select from multiple prompt templates. See example in API reference: https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html\n",
      "  chain = MultiPromptChain(router_chain=router_chain,\n"
     ]
    }
   ],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed5882-c749-4379-84c6-2554e4a8c1a2",
   "metadata": {},
   "source": [
    "Zde máme příklad programátorského podchainu."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "\n",
    "    template=router_template,\n",
    "\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(chat, router_prompt)"
   ],
   "id": "3bfc8183f317f019"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A nakonec tu máme chain, který *LLMRouterChain* obaluje a spojuje ho s podchainy.",
   "id": "9b639f165b4b4d91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )\n",
    "#%% md\n",
    "Zde máme příklad programátorského podchainu.\n",
    "#%%\n",
    "chain.invoke(\"What are good packages for graphs creation in Python?\")\n",
    "#%% md\n",
    "Tady je dotaz na kuchařský podchain.\n",
    "#%%\n",
    "chain.run(\"How should I prepare fried cheese?\")\n",
    "#%% md\n",
    "A nakonec defaultní podchain.\n",
    "#%%\n",
    "chain.run(\"Who was Joseph II?\")\n",
    "#%% md\n",
    "#### Transformation chain\n",
    "Viz [Transformation chain](https://python.langchain.com/docs/modules/chains/foundational/transformation)."
   ],
   "id": "f0a1f64069c29336"
  },
  {
   "cell_type": "markdown",
   "id": "6118b25a-0d30-49ba-8a1b-1a54332aefd6",
   "metadata": {},
   "source": [
    "#### Transformation chain\n",
    "Viz [Transformation chain](https://python.langchain.com/docs/modules/chains/foundational/transformation).\n",
    "Transformation chain slouží k tomu, aby se na vstupní textový řetězec použila nějaká obecná pythoní funkce. Její jméno (tj. bez parametrů a kulatých závorek) se vloží do parametru *transofrm* konstruktoru *TransformChain*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "54de474f-f9c7-444a-8ace-f4684390ff5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:33:51.392401Z",
     "start_time": "2024-09-22T21:33:51.389591Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "\n",
    "def replace_animal(input_dict):\n",
    "    orig_text = input_dict[\"text\"]\n",
    "    replaced_text = orig_text.replace(\"squirrel\", \"unicorn\")\n",
    "    replaced_text = replaced_text.replace(\"squirrels\", \"unicorns\")\n",
    "    replaced_text = replaced_text.replace(\"Squirrel\", \"Unicorn\")\n",
    "    replaced_text = replaced_text.replace(\"Squirrels\", \"Unicorns\")\n",
    "    return {\"output_text\": replaced_text}\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=replace_animal\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae2552-8427-41d4-9206-9e79ecd969a7",
   "metadata": {},
   "source": [
    "Následně se s chainy pracuje jako v předchozích příkladech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "89372aa4-b014-4fbf-a321-0f04f115d58b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:34:06.786386Z",
     "start_time": "2024-09-22T21:34:06.770887Z"
    }
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Find strange formulations in following text and write why are they strange:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "Summary:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "chat = ChatOpenAI(temperature=0.0, model_name=llm_model_name)\n",
    "llm_chain = LLMChain(llm=chat, prompt=prompt)\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b8b9c938-4597-4714-949d-91f4c60498e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:34:15.002212Z",
     "start_time": "2024-09-22T21:34:11.055042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:SimpleSequentialChain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"\\n    Squirrels are members of the family Sciuridae (/sɪˈjuːrɪdeɪ, -diː/), a family that includes small or medium-size rodents. \\n    The squirrel family includes tree squirrels, ground squirrels (including chipmunks and prairie dogs, among others), \\n    and flying squirrels. Squirrels are indigenous to the Americas, Eurasia, and Africa, and were introduced by humans to Australia.[1] \\n    The earliest known fossilized squirrels date from the Eocene epoch, and among other living rodent families, the squirrels are \\n    most closely related to the mountain beaver and to the dormice.\\n    \"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:SimpleSequentialChain > chain:TransformChain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"text\": \"\\n    Squirrels are members of the family Sciuridae (/sɪˈjuːrɪdeɪ, -diː/), a family that includes small or medium-size rodents. \\n    The squirrel family includes tree squirrels, ground squirrels (including chipmunks and prairie dogs, among others), \\n    and flying squirrels. Squirrels are indigenous to the Americas, Eurasia, and Africa, and were introduced by humans to Australia.[1] \\n    The earliest known fossilized squirrels date from the Eocene epoch, and among other living rodent families, the squirrels are \\n    most closely related to the mountain beaver and to the dormice.\\n    \"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:SimpleSequentialChain > chain:TransformChain] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output_text\": \"\\n    Unicorns are members of the family Sciuridae (/sɪˈjuːrɪdeɪ, -diː/), a family that includes small or medium-size rodents. \\n    The unicorn family includes tree unicorns, ground unicorns (including chipmunks and prairie dogs, among others), \\n    and flying unicorns. Unicorns are indigenous to the Americas, Eurasia, and Africa, and were introduced by humans to Australia.[1] \\n    The earliest known fossilized unicorns date from the Eocene epoch, and among other living rodent families, the unicorns are \\n    most closely related to the mountain beaver and to the dormice.\\n    \"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:SimpleSequentialChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"output_text\": \"\\n    Unicorns are members of the family Sciuridae (/sɪˈjuːrɪdeɪ, -diː/), a family that includes small or medium-size rodents. \\n    The unicorn family includes tree unicorns, ground unicorns (including chipmunks and prairie dogs, among others), \\n    and flying unicorns. Unicorns are indigenous to the Americas, Eurasia, and Africa, and were introduced by humans to Australia.[1] \\n    The earliest known fossilized unicorns date from the Eocene epoch, and among other living rodent families, the unicorns are \\n    most closely related to the mountain beaver and to the dormice.\\n    \"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:SimpleSequentialChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Find strange formulations in following text and write why are they strange:\\n\\n\\n    Unicorns are members of the family Sciuridae (/sɪˈjuːrɪdeɪ, -diː/), a family that includes small or medium-size rodents. \\n    The unicorn family includes tree unicorns, ground unicorns (including chipmunks and prairie dogs, among others), \\n    and flying unicorns. Unicorns are indigenous to the Americas, Eurasia, and Africa, and were introduced by humans to Australia.[1] \\n    The earliest known fossilized unicorns date from the Eocene epoch, and among other living rodent families, the unicorns are \\n    most closely related to the mountain beaver and to the dormice.\\n    \\n\\nSummary:\"\n",
      "  ]\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:SimpleSequentialChain > chain:LLMChain > llm:ChatOpenAI] [6.26s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The text contains several strange formulations that raise questions about accuracy and coherence:\\n\\n1. **\\\"Unicorns are members of the family Sciuridae\\\"**: This is strange because unicorns are mythical creatures and do not belong to any biological classification. The Sciuridae family includes squirrels and similar rodents, making the claim that unicorns are part of this family nonsensical.\\n\\n2. **\\\"The unicorn family includes tree unicorns, ground unicorns, and flying unicorns\\\"**: The categorization of unicorns into different types (tree, ground, flying) is unusual and confusing. In mythology, unicorns are typically depicted as horse-like creatures with a single horn, and the idea of \\\"flying unicorns\\\" or \\\"ground unicorns\\\" does not align with traditional representations.\\n\\n3. **\\\"Unicorns are indigenous to the Americas, Eurasia, and Africa\\\"**: This statement is strange because unicorns are not real animals and thus cannot be indigenous to any region. The use of \\\"indigenous\\\" implies a biological reality that does not exist for mythical creatures.\\n\\n4. **\\\"The earliest known fossilized unicorns date from the Eocene epoch\\\"**: This is problematic because there are no fossil records of unicorns, as they are purely fictional. The mention of a specific geological epoch adds a layer of false credibility to the claim.\\n\\n5. **\\\"The unicorns are most closely related to the mountain beaver and to the dormice\\\"**: This statement is strange because it suggests a biological relationship that cannot exist, as unicorns are not real animals. The comparison to actual rodent families is misleading and creates confusion about the nature of unicorns.\\n\\nOverall, the text combines elements of fantasy with scientific terminology in a way that creates a bizarre and contradictory narrative, leading to confusion and a lack of clarity regarding the subject matter.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The text contains several strange formulations that raise questions about accuracy and coherence:\\n\\n1. **\\\"Unicorns are members of the family Sciuridae\\\"**: This is strange because unicorns are mythical creatures and do not belong to any biological classification. The Sciuridae family includes squirrels and similar rodents, making the claim that unicorns are part of this family nonsensical.\\n\\n2. **\\\"The unicorn family includes tree unicorns, ground unicorns, and flying unicorns\\\"**: The categorization of unicorns into different types (tree, ground, flying) is unusual and confusing. In mythology, unicorns are typically depicted as horse-like creatures with a single horn, and the idea of \\\"flying unicorns\\\" or \\\"ground unicorns\\\" does not align with traditional representations.\\n\\n3. **\\\"Unicorns are indigenous to the Americas, Eurasia, and Africa\\\"**: This statement is strange because unicorns are not real animals and thus cannot be indigenous to any region. The use of \\\"indigenous\\\" implies a biological reality that does not exist for mythical creatures.\\n\\n4. **\\\"The earliest known fossilized unicorns date from the Eocene epoch\\\"**: This is problematic because there are no fossil records of unicorns, as they are purely fictional. The mention of a specific geological epoch adds a layer of false credibility to the claim.\\n\\n5. **\\\"The unicorns are most closely related to the mountain beaver and to the dormice\\\"**: This statement is strange because it suggests a biological relationship that cannot exist, as unicorns are not real animals. The comparison to actual rodent families is misleading and creates confusion about the nature of unicorns.\\n\\nOverall, the text combines elements of fantasy with scientific terminology in a way that creates a bizarre and contradictory narrative, leading to confusion and a lack of clarity regarding the subject matter.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 377,\n",
      "                \"prompt_tokens\": 166,\n",
      "                \"total_tokens\": 543,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "              \"system_fingerprint\": \"fp_72ed7ab54c\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f1ab7840-d21b-4509-8309-29b62182b971-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 166,\n",
      "              \"output_tokens\": 377,\n",
      "              \"total_tokens\": 543\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 377,\n",
      "      \"prompt_tokens\": 166,\n",
      "      \"total_tokens\": 543,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"system_fingerprint\": \"fp_72ed7ab54c\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:SimpleSequentialChain > chain:LLMChain] [6.26s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"text\": \"The text contains several strange formulations that raise questions about accuracy and coherence:\\n\\n1. **\\\"Unicorns are members of the family Sciuridae\\\"**: This is strange because unicorns are mythical creatures and do not belong to any biological classification. The Sciuridae family includes squirrels and similar rodents, making the claim that unicorns are part of this family nonsensical.\\n\\n2. **\\\"The unicorn family includes tree unicorns, ground unicorns, and flying unicorns\\\"**: The categorization of unicorns into different types (tree, ground, flying) is unusual and confusing. In mythology, unicorns are typically depicted as horse-like creatures with a single horn, and the idea of \\\"flying unicorns\\\" or \\\"ground unicorns\\\" does not align with traditional representations.\\n\\n3. **\\\"Unicorns are indigenous to the Americas, Eurasia, and Africa\\\"**: This statement is strange because unicorns are not real animals and thus cannot be indigenous to any region. The use of \\\"indigenous\\\" implies a biological reality that does not exist for mythical creatures.\\n\\n4. **\\\"The earliest known fossilized unicorns date from the Eocene epoch\\\"**: This is problematic because there are no fossil records of unicorns, as they are purely fictional. The mention of a specific geological epoch adds a layer of false credibility to the claim.\\n\\n5. **\\\"The unicorns are most closely related to the mountain beaver and to the dormice\\\"**: This statement is strange because it suggests a biological relationship that cannot exist, as unicorns are not real animals. The comparison to actual rodent families is misleading and creates confusion about the nature of unicorns.\\n\\nOverall, the text combines elements of fantasy with scientific terminology in a way that creates a bizarre and contradictory narrative, leading to confusion and a lack of clarity regarding the subject matter.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:SimpleSequentialChain] [6.26s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"The text contains several strange formulations that raise questions about accuracy and coherence:\\n\\n1. **\\\"Unicorns are members of the family Sciuridae\\\"**: This is strange because unicorns are mythical creatures and do not belong to any biological classification. The Sciuridae family includes squirrels and similar rodents, making the claim that unicorns are part of this family nonsensical.\\n\\n2. **\\\"The unicorn family includes tree unicorns, ground unicorns, and flying unicorns\\\"**: The categorization of unicorns into different types (tree, ground, flying) is unusual and confusing. In mythology, unicorns are typically depicted as horse-like creatures with a single horn, and the idea of \\\"flying unicorns\\\" or \\\"ground unicorns\\\" does not align with traditional representations.\\n\\n3. **\\\"Unicorns are indigenous to the Americas, Eurasia, and Africa\\\"**: This statement is strange because unicorns are not real animals and thus cannot be indigenous to any region. The use of \\\"indigenous\\\" implies a biological reality that does not exist for mythical creatures.\\n\\n4. **\\\"The earliest known fossilized unicorns date from the Eocene epoch\\\"**: This is problematic because there are no fossil records of unicorns, as they are purely fictional. The mention of a specific geological epoch adds a layer of false credibility to the claim.\\n\\n5. **\\\"The unicorns are most closely related to the mountain beaver and to the dormice\\\"**: This statement is strange because it suggests a biological relationship that cannot exist, as unicorns are not real animals. The comparison to actual rodent families is misleading and creates confusion about the nature of unicorns.\\n\\nOverall, the text combines elements of fantasy with scientific terminology in a way that creates a bizarre and contradictory narrative, leading to confusion and a lack of clarity regarding the subject matter.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\\n    Squirrels are members of the family Sciuridae (/sɪˈjuːrɪdeɪ, -diː/), a family that includes small or medium-size rodents. \\n    The squirrel family includes tree squirrels, ground squirrels (including chipmunks and prairie dogs, among others), \\n    and flying squirrels. Squirrels are indigenous to the Americas, Eurasia, and Africa, and were introduced by humans to Australia.[1] \\n    The earliest known fossilized squirrels date from the Eocene epoch, and among other living rodent families, the squirrels are \\n    most closely related to the mountain beaver and to the dormice.\\n    ',\n",
       " 'output': 'The text contains several strange formulations that raise questions about accuracy and coherence:\\n\\n1. **\"Unicorns are members of the family Sciuridae\"**: This is strange because unicorns are mythical creatures and do not belong to any biological classification. The Sciuridae family includes squirrels and similar rodents, making the claim that unicorns are part of this family nonsensical.\\n\\n2. **\"The unicorn family includes tree unicorns, ground unicorns, and flying unicorns\"**: The categorization of unicorns into different types (tree, ground, flying) is unusual and confusing. In mythology, unicorns are typically depicted as horse-like creatures with a single horn, and the idea of \"flying unicorns\" or \"ground unicorns\" does not align with traditional representations.\\n\\n3. **\"Unicorns are indigenous to the Americas, Eurasia, and Africa\"**: This statement is strange because unicorns are not real animals and thus cannot be indigenous to any region. The use of \"indigenous\" implies a biological reality that does not exist for mythical creatures.\\n\\n4. **\"The earliest known fossilized unicorns date from the Eocene epoch\"**: This is problematic because there are no fossil records of unicorns, as they are purely fictional. The mention of a specific geological epoch adds a layer of false credibility to the claim.\\n\\n5. **\"The unicorns are most closely related to the mountain beaver and to the dormice\"**: This statement is strange because it suggests a biological relationship that cannot exist, as unicorns are not real animals. The comparison to actual rodent families is misleading and creates confusion about the nature of unicorns.\\n\\nOverall, the text combines elements of fantasy with scientific terminology in a way that creates a bizarre and contradictory narrative, leading to confusion and a lack of clarity regarding the subject matter.'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_chain.invoke(\n",
    "    \"\"\"\n",
    "    Squirrels are members of the family Sciuridae (/sɪˈjuːrɪdeɪ, -diː/), a family that includes small or medium-size rodents. \n",
    "    The squirrel family includes tree squirrels, ground squirrels (including chipmunks and prairie dogs, among others), \n",
    "    and flying squirrels. Squirrels are indigenous to the Americas, Eurasia, and Africa, and were introduced by humans to Australia.[1] \n",
    "    The earliest known fossilized squirrels date from the Eocene epoch, and among other living rodent families, the squirrels are \n",
    "    most closely related to the mountain beaver and to the dormice.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8e26d-ded6-43fe-99e1-982fc97eec93",
   "metadata": {},
   "source": [
    "## Paměť\n",
    "LLM modely jako takové si při odpovídání na uživatelův vstup předchozí interakce nepamatují. Proto se do nich musí explicitně přidat paměť, která bude historii v té či oné formě obsahovat.  \n",
    "#### ConversationBufferMemory\n",
    "Základním paměťovým objektem je *ConversationBufferMemory*. Ten si zkrátka pamatuje celou konverzaci tak, jak probíhala. Jak ale vypadá praktické použití? Asi nejsnazší je použit [*ConversationChain*](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/conversation/base.py). Do jeho konstruktoru vložíme chatovací model, paměť a lze sem umístit i flag, že požadujeme verbose výstup.  \n",
    "BACHA - *ConversationBufferMemory* dovoluje v parametru *memory_key* měnit defaultní klíč (s hodnotou *history*), skrze který se dá dostat k paměti. Jenomže *ConversationalChain* s ničím takovým nepočítá a pokud nedostane *history*, tak způsobí pád."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e30a619d70e6753a"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa96aa00-dad5-4088-9a4d-25b67ef6c587",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T10:42:29.457650Z",
     "start_time": "2024-09-23T10:42:26.927241Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, who are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm your friendly AI assistant, here to help you with a wide range of topics. I can provide information, answer questions, and even chat about your interests. What would you like to talk about today?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: Do you know any famous hamsters?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Do you know any famous hamsters?',\n",
       " 'history': [HumanMessage(content='Hi, who are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm your friendly AI assistant, here to help you with a wide range of topics. I can provide information, answer questions, and even chat about your interests. What would you like to talk about today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Do you know any famous hamsters?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes, I do! One of the most famous hamsters is \"Hammy,\" who became popular through the animated series \"The Adventures of Hammy.\" Another well-known hamster is \"Binky,\" who gained fame on social media for his adorable antics and funny videos. Additionally, there\\'s \"Mr. Nibbles,\" a hamster that was featured in various internet memes. Hamsters have a charming way of capturing our hearts with their playful behavior! Do you have a favorite hamster or a specific one in mind?', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Yes, I do! One of the most famous hamsters is \"Hammy,\" who became popular through the animated series \"The Adventures of Hammy.\" Another well-known hamster is \"Binky,\" who gained fame on social media for his adorable antics and funny videos. Additionally, there\\'s \"Mr. Nibbles,\" a hamster that was featured in various internet memes. Hamsters have a charming way of capturing our hearts with their playful behavior! Do you have a favorite hamster or a specific one in mind?'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Do you know any famous hamsters?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "63fc0831-a7d2-42fb-9dad-e0bf224092d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T08:17:29.696145Z",
     "start_time": "2024-09-23T08:17:28.388383Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, who are you?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm your friendly AI assistant, here to help you with a wide range of topics. I can provide information, answer questions, and even chat about your interests. What would you like to talk about today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Do you know any famous hamsters?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, I do! One of the most famous hamsters is \"Hammy,\" who became popular through the animated series \"The Adventures of Hammy.\" Another well-known hamster is \"Binky,\" who gained fame on social media for his adorable antics and funny videos. Additionally, there\\'s \"Mr. Nibbles,\" a hamster that was featured in various internet memes. Hamsters have a charming way of capturing our hearts with their playful behavior! Do you have a favorite hamster or a specific one in mind?', additional_kwargs={}, response_metadata={})]\n",
      "Human: Have you ever heard about hamster called 'Boo'?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Have you ever heard about hamster called 'Boo'?\",\n",
       " 'history': [HumanMessage(content='Hi, who are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm your friendly AI assistant, here to help you with a wide range of topics. I can provide information, answer questions, and even chat about your interests. What would you like to talk about today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Do you know any famous hamsters?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes, I do! One of the most famous hamsters is \"Hammy,\" who became popular through the animated series \"The Adventures of Hammy.\" Another well-known hamster is \"Binky,\" who gained fame on social media for his adorable antics and funny videos. Additionally, there\\'s \"Mr. Nibbles,\" a hamster that was featured in various internet memes. Hamsters have a charming way of capturing our hearts with their playful behavior! Do you have a favorite hamster or a specific one in mind?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"Have you ever heard about hamster called 'Boo'?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes, I\\'ve heard of Boo! However, Boo is actually a Pomeranian dog, not a hamster. He became an internet sensation due to his cute appearance and playful personality, often referred to as the \"world\\'s cutest dog.\" If you\\'re thinking of a specific hamster named Boo, I don\\'t have information on that. Hamsters can be quite popular on social media too! Do you have any particular stories or videos in mind related to Boo or any other hamsters?', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Yes, I\\'ve heard of Boo! However, Boo is actually a Pomeranian dog, not a hamster. He became an internet sensation due to his cute appearance and playful personality, often referred to as the \"world\\'s cutest dog.\" If you\\'re thinking of a specific hamster named Boo, I don\\'t have information on that. Hamsters can be quite popular on social media too! Do you have any particular stories or videos in mind related to Boo or any other hamsters?'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Have you ever heard about hamster called 'Boo'?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5127a2e-022d-42e8-a699-35a6ddb7da58",
   "metadata": {},
   "source": [
    "Na co slouží v definici paměťového objektu parametr *return_messages*? S pomocí metody *load_memory_variables* lze z paměťového objetku získat dosavadní historii konverzace (zde prázdné složené závorky coby parametr metody mají své použití pro určité typy pamětí, které nějaký vstupní parametr požadují). Pakliže bylo *return_messages* rovno False, obdržíme výstup v podobě velkého stringu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "102d6bbd-4ffa-4a2e-9253-441f696f3449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T09:52:02.035886Z",
     "start_time": "2024-09-23T09:52:01.915938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, who are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hello! I'm your friendly AI assistant, here to help you with a wide range of topics. I can provide information, answer questions, and even chat about your interests. What would you like to talk about today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Do you know any famous hamsters?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes, I do! One of the most famous hamsters is \"Hammy,\" who became popular through the animated series \"The Adventures of Hammy.\" Another well-known hamster is \"Binky,\" who gained fame on social media for his adorable antics and funny videos. Additionally, there\\'s \"Mr. Nibbles,\" a hamster that was featured in various internet memes. Hamsters have a charming way of capturing our hearts with their playful behavior! Do you have a favorite hamster or a specific one in mind?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"Have you ever heard about hamster called 'Boo'?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Yes, I\\'ve heard of Boo! However, Boo is actually a Pomeranian dog, not a hamster. He became an internet sensation due to his cute appearance and playful personality, often referred to as the \"world\\'s cutest dog.\" If you\\'re thinking of a specific hamster named Boo, I don\\'t have information on that. Hamsters can be quite popular on social media too! Do you have any particular stories or videos in mind related to Boo or any other hamsters?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_buffer_memory.load_memory_variables({})"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#%%\n",
    "conversation.invoke(input=\"Have you ever heard about hamster called 'Boo'?\")\n",
    "#%% md\n"
   ],
   "id": "1a761719577a50dd"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c93f8487",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692ac7d",
   "metadata": {},
   "source": [
    "A vlozime do ni nasledujici konverzaci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3a44e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebf9df0",
   "metadata": {},
   "source": [
    "Kdyz ted vytiskneme memory, dostaneme prave onu konverzaci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "118794fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09f25d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d3176",
   "metadata": {},
   "source": [
    "A muzeme pridavat dalsi a dalsi context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9c67489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a806b37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146d5cc-a7a6-46f0-af4a-5584237c51cb",
   "metadata": {},
   "source": [
    "V případě, že potřebujeme nastavit systémový prompt, bohužel musíme sáhnout po něčem komplikovanějším než je *ConversationChain* (byť v dalších příkladech pamětí budeme pro jednoduchost používat právě ten). Kód níže uvedený byl převzat (s mírnou úpravou) ze spodku [této stránky](https://python.langchain.com/docs/modules/memory/) dokumentace.  \n",
    "Pozn.: pokud bude v tomto případě u paměti nastaveno *return_messages*=False, kód spadne. Pád je dán faktem, že *MessagesPlaceholder* potřebuje historii jako list konverzací, nikoli jako jeden string. Též je třeba, aby *variable_name* v *MessagePlaceholder* odpovídalo *memory_key* v paměti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1504d770-df54-4a15-a991-8a7eab6fa616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.3, model_name=llm_model_name)\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are an infantile AI assistant who start every message with 'peekaboo'.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "conv_buffer_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=conv_buffer_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4b211208-9e87-4d76-a0d6-c7211eb63ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: You are an infantile AI assistant who start every message with 'peekaboo'.\n",
      "Human: Hi, who are you?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Hi, who are you?',\n",
       " 'chat_history': [HumanMessage(content='Hi, who are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Peekaboo! I'm your friendly AI assistant here to help you with any questions or information you need. What can I do for you today?\", additional_kwargs={}, response_metadata={})],\n",
       " 'text': \"Peekaboo! I'm your friendly AI assistant here to help you with any questions or information you need. What can I do for you today?\"}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"Hi, who are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1a998216-e772-4de3-af55-68699a3e3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: You are an infantile AI assistant who start every message with 'peekaboo'.\n",
      "Human: Hi, who are you?\n",
      "AI: Peekaboo! I'm your friendly AI assistant here to help you with any questions or information you need. What can I do for you today?\n",
      "Human: Do you know any famous hamsters?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Do you know any famous hamsters?',\n",
       " 'chat_history': [HumanMessage(content='Hi, who are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Peekaboo! I'm your friendly AI assistant here to help you with any questions or information you need. What can I do for you today?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Do you know any famous hamsters?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Peekaboo! Yes, there are a few famous hamsters! One of the most well-known is \"Hamtaro,\" the adorable little hamster from the animated series and manga. Another is \"Gus,\" the hamster from the popular YouTube channel \"Hammy the Hamster.\" They both have captured the hearts of many fans! Do you want to know more about them?', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'Peekaboo! Yes, there are a few famous hamsters! One of the most well-known is \"Hamtaro,\" the adorable little hamster from the animated series and manga. Another is \"Gus,\" the hamster from the popular YouTube channel \"Hammy the Hamster.\" They both have captured the hearts of many fans! Do you want to know more about them?'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation({\"question\": \"Do you know any famous hamsters?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad91ad1-a671-4300-a66c-bd4ae710cf1f",
   "metadata": {},
   "source": [
    "#### ConversationBufferWindowMemory\n",
    "*ConversationBufferWindowMemory* je variací na výše uvedený *ConversationBufferMemory*. Narozdíl od něj neobsahuje celou konverzaci, ale jen posledních *k* výměn (udáváme stejně se jmenujícím parametrem v konstruktoru paměti). Díky tomu je provoz chatbota levnější."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "220a9a74-ce83-408d-85b0-3ed26c23f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory \n",
    "\n",
    "chat = ChatOpenAI(temperature=0.3, model_name=llm_model_name)\n",
    "\n",
    "window_memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    verbose=False,\n",
    "    memory=window_memory\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%% md\n",
    "#### ConversationBufferWindowMemory\n"
   ],
   "id": "c3481ebc20133b1d"
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0c9812d7-af38-4c78-bdb5-77e72b1c1f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Do you know any famous rabbits?\\nAI: Absolutely! There are several famous rabbits in popular culture. One of the most iconic is Bugs Bunny, the clever and witty character from Warner Bros. cartoons. He first appeared in the late 1930s and is known for his catchphrase, \"What\\'s up, Doc?\" Another famous rabbit is Peter Rabbit, created by Beatrix Potter in her beloved children\\'s stories. Peter is known for his mischievous adventures in Mr. McGregor\\'s garden. Additionally, there\\'s the White Rabbit from Lewis Carroll\\'s \"Alice\\'s Adventures in Wonderland,\" who is famous for his time-keeping habits and leading Alice down the rabbit hole into Wonderland. Do you have a favorite rabbit character?'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f3f1a5",
   "metadata": {},
   "source": [
    "Alternativne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0db1b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "974712d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ac04c152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a48a7-be10-448d-b29e-2cf1b1f11bc5",
   "metadata": {},
   "source": [
    "#### ConversationTokenBufferMemory\n",
    "Na podobném principu je založena i *ConversationTokenBufferMemory*, která uchovává historii jen dokud se vejde do *max_token_limit* počtu tokenů. Pokud nějaký příspěvek tuto hranici přesahuje, historie se de facto vymeže, tj. není to tak, že by v ní zbyl kousek starého příspěvku. Jelikož tokenizace je pro různé modely různá, musí se paměti v parametru *llm* předat i použitý model. Navíc je potřeba (minimálně pro OpenAI modely, ale možná i pro open source modely) mít nainstalovaný balíček [tiktoken](https://pypi.org/project/tiktoken/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "823c769a-e473-406d-bc19-fbf304019d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory \n",
    "\n",
    "chat = ChatOpenAI(temperature=0.3, model_name=llm_model_name)\n",
    "\n",
    "token_memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=60)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    verbose=False,\n",
    "    memory=token_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7de6bc66-12bd-484c-800c-f4e4ab5ba2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi, who are you?',\n",
       " 'history': '',\n",
       " 'response': \"Hello! I'm an AI designed to assist you with a wide range of topics. I can help answer questions, provide information, and even engage in friendly conversation. My goal is to make our interaction enjoyable and informative. What would you like to talk about today?\"}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Hi, who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bd773ff1-4962-43c3-ac1b-461dc4e55341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"AI: Hello! I'm an AI designed to assist you with a wide range of topics. I can help answer questions, provide information, and even engage in friendly conversation. My goal is to make our interaction enjoyable and informative. What would you like to talk about today?\"}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6f0f5a55-0f74-4fc0-94fd-6bb96071a6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Do you know any famous beavers?',\n",
       " 'history': \"AI: Hello! I'm an AI designed to assist you with a wide range of topics. I can help answer questions, provide information, and even engage in friendly conversation. My goal is to make our interaction enjoyable and informative. What would you like to talk about today?\",\n",
       " 'response': 'Absolutely! One of the most famous beavers in popular culture is \"Bucky the Beaver,\" who is the mascot for the University of Wisconsin-Madison. He\\'s known for his energetic personality and is a beloved figure at sporting events.\\n\\nAnother notable beaver is \"Beaver Cleaver,\" the main character from the classic television show \"Leave It to Beaver,\" which aired in the late 1950s and early 1960s. The show depicted the life of a young boy and his family, and Beaver became an iconic representation of childhood in America.\\n\\nIn the realm of literature, there\\'s also \"The Tale of Mr. Tod\" by Beatrix Potter, which features a character named Benjamin Bunny who interacts with a beaver. \\n\\nIf you\\'re interested in real-life beavers, they are fascinating creatures known for their impressive dam-building skills and their role in creating wetland ecosystems. Do you have a specific beaver in mind, or are you curious about something else related to them?'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Do you know any famous beavers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3dd329fe-6a9d-4237-9282-b2bdfba479df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "783014e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f096dfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92435d6e-8e90-4bbc-beb9-8ff42f00d5e3",
   "metadata": {},
   "source": [
    "#### ConversationSummaryMemory\n",
    "Odlišným typem paměti je *ConversationSummaryMemory*. U ní se provolává jazykový model (specifikovaný v konstruktoru paměti pod parametrem *llm*, tj. může mít jiné vlastnosti než \"hlavní\" v aplikaci používaný model), který má za úkol provést sumarizaci dosavadní historie konverzace a nové konverzační výměny mezi člověkem a strojem. Ale muze to klidne byt ten samy. Užitečné je to zejména u dlouhých kovnerzací, kde úspora z redukce délky historie přebije nutnost většího počtu provolávání modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6fc9fab2-418d-4710-a153-367e7769927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.memory import ConversationSummaryMemory \n",
    "\n",
    "chat = ChatOpenAI(temperature=0.3, model_name=llm_model_name)\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm=OpenAI(temperature=0),\n",
    "                                           max_token_limit=100)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    verbose=True,\n",
    "    memory=summary_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5443d978-aec8-48b6-963e-e28d53d43bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, who are you?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi, who are you?',\n",
       " 'history': '',\n",
       " 'response': \"Hello! I'm an AI designed to assist and engage in conversations with you. I can provide information, answer questions, and discuss a wide range of topics. My goal is to make our interaction enjoyable and informative. What would you like to talk about today?\"}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Hi, who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "aca32eb8-94db-46bd-96d4-9ba12c81a44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': '\\nThe human asks the AI about its purpose and the AI explains that it is designed to assist and engage in conversations. It can provide information, answer questions, and discuss various topics with the goal of making the interaction enjoyable and informative. The AI asks the human what they would like to talk about.'}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "97f30514-cf70-4a5e-bcde-9ca022abfd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human asks the AI about its purpose and the AI explains that it is designed to assist and engage in conversations. It can provide information, answer questions, and discuss various topics with the goal of making the interaction enjoyable and informative. The AI asks the human what they would like to talk about.\n",
      "Human: Do you know what are differences between rhinos and unicorns?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Do you know what are differences between rhinos and unicorns?',\n",
       " 'history': '\\nThe human asks the AI about its purpose and the AI explains that it is designed to assist and engage in conversations. It can provide information, answer questions, and discuss various topics with the goal of making the interaction enjoyable and informative. The AI asks the human what they would like to talk about.',\n",
       " 'response': 'Absolutely! Rhinos and unicorns are quite different in many ways, both in reality and in mythology.\\n\\n1. **Existence**: Rhinos are real animals that belong to the family Rhinocerotidae. They are large, thick-skinned mammals found in Africa and South Asia. There are five species of rhinos, including the white rhino and the Indian rhino. On the other hand, unicorns are mythical creatures often depicted as horse-like animals with a single spiraled horn on their foreheads. They are not real and are part of folklore and fantasy.\\n\\n2. **Physical Characteristics**: Rhinos are known for their massive size, thick skin, and one or two horns made of keratin (the same material as human nails). They have a stocky body and are generally gray or brown in color. Unicorns, in contrast, are usually portrayed as elegant, horse-like creatures, often depicted as white with a shimmering mane and tail, and of course, their iconic single horn.\\n\\n3. **Habitat**: Rhinos inhabit grasslands, savannas, and forests, depending on the species. They are often found near water sources. Unicorns, being mythical, are said to inhabit enchanted forests or magical realms, often associated with purity and grace.\\n\\n4. **Cultural Significance**: Rhinos are often the subject of conservation efforts due to their endangered status, and they play a role in various ecosystems. Unicorns, however, symbolize purity, beauty, and magic in various cultures and are often featured in literature, art, and popular media.\\n\\n5. **Behavior**: Rhinos are generally solitary animals, especially the males, while females may be seen with their calves. They can be quite territorial. Unicorns, in stories, are often depicted as gentle and elusive creatures, sometimes possessing magical abilities.\\n\\nSo, while rhinos are fascinating creatures that play a significant role in our natural world, unicorns are enchanting figures of myth and legend! Is there a specific aspect of either that you’d like to explore further?'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Do you know what are differences between rhinos and unicorns?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "57808f63-eed8-41f2-a093-2aa7edbc5669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ' The human asks the AI about its purpose and the AI explains that it is designed to assist and engage in conversations. The AI can provide information, answer questions, and discuss various topics with the goal of making the interaction enjoyable and informative. The AI asks the human what they would like to talk about and the conversation shifts to discussing the differences between rhinos and unicorns. The AI explains that rhinos are real animals with physical characteristics such as their size, thick skin, and horns, while unicorns are mythical creatures often depicted as elegant and magical. The AI also discusses the habitat, cultural significance, and behavior of both species. The AI then asks the human if there is a specific aspect they would like to explore further.'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "736ee973-a2e0-4d4b-98dd-3653082a3198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      " The human asks the AI about its purpose and the AI explains that it is designed to assist and engage in conversations. The AI can provide information, answer questions, and discuss various topics with the goal of making the interaction enjoyable and informative. The AI asks the human what they would like to talk about and the conversation shifts to discussing the differences between rhinos and unicorns. The AI explains that rhinos are real animals with physical characteristics such as their size, thick skin, and horns, while unicorns are mythical creatures often depicted as elegant and magical. The AI also discusses the habitat, cultural significance, and behavior of both species. The AI then asks the human if there is a specific aspect they would like to explore further.\n",
      "Human: And what about similarities?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'And what about similarities?',\n",
       " 'history': ' The human asks the AI about its purpose and the AI explains that it is designed to assist and engage in conversations. The AI can provide information, answer questions, and discuss various topics with the goal of making the interaction enjoyable and informative. The AI asks the human what they would like to talk about and the conversation shifts to discussing the differences between rhinos and unicorns. The AI explains that rhinos are real animals with physical characteristics such as their size, thick skin, and horns, while unicorns are mythical creatures often depicted as elegant and magical. The AI also discusses the habitat, cultural significance, and behavior of both species. The AI then asks the human if there is a specific aspect they would like to explore further.',\n",
       " 'response': \"That's a great question! While rhinos and unicorns are fundamentally different—one being a real animal and the other a mythical creature—they do share some interesting similarities. \\n\\n1. **Horns**: Both rhinos and unicorns are often depicted with a prominent horn. Rhinos have one or two horns made of keratin, the same material as human nails, while unicorns are typically portrayed with a single spiraled horn that symbolizes purity and magic.\\n\\n2. **Symbolism**: Both creatures hold significant symbolic meanings in various cultures. Rhinos can symbolize strength and resilience, often representing conservation efforts due to their endangered status. Unicorns, on the other hand, are symbols of purity, grace, and the fantastical, often associated with magic and the unattainable.\\n\\n3. **Connection to Nature**: Both are often associated with natural environments. Rhinos inhabit grasslands and savannas, while unicorns are frequently depicted in lush, enchanted forests or meadows, emphasizing a connection to nature and the wild.\\n\\n4. **Cultural Representation**: Both have appeared in various forms of art, literature, and folklore. Rhinos are featured in wildlife documentaries and conservation campaigns, while unicorns are prevalent in fairy tales, fantasy novels, and artwork.\\n\\n5. **Mystique**: Both evoke a sense of wonder. Rhinos are fascinating creatures with unique behaviors and adaptations, while unicorns capture the imagination with their magical qualities.\\n\\nIf you're curious about any specific similarities or want to dive deeper into one of these points, just let me know!\"}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"And what about similarities?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3810d5d7-5281-474b-b4c2-7010de01df75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ' The human asks the AI about its purpose and the AI explains that it is designed to assist and engage in conversations. The AI can provide information, answer questions, and discuss various topics with the goal of making the interaction enjoyable and informative. The AI asks the human what they would like to talk about and the conversation shifts to discussing the differences between rhinos and unicorns. The AI explains that rhinos and unicorns have both similarities and differences, such as their horns, symbolism, connection to nature, cultural representation, and mystique. The AI offers to explore any specific similarities further if the human is interested.'}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70bea6-dfe2-4aa1-887b-1d4cbd3043de",
   "metadata": {},
   "source": [
    "#### ConversationEntityMemory\n",
    "Dalším speciálním typem paměti je *ConversationEntityMemory*. Ta z konverzace s pomocí jazykového modelu extrahuje informace o entitách.  \n",
    "Abychom se vyhnuli chybové hlášce\n",
    "```\n",
    "Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['entities', 'history'] as inputs from memory, and input as the normal input key. (type=value_error)\n",
    "```\n",
    "musíme do *ConversationChain* přidat nový parametr *prompt*, do kterého vložíme importováním získanou *ENTITY_MEMORY_CONVERSATION_TEMPLATE*"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "summary_memory.load_memory_variables({})\n",
    "#%% md\n",
    "#### ConversationEntityMemory\n",
    "Dalším speciálním typem paměti je *ConversationEntityMemory*. Ta z konverzace s pomocí jazykového modelu extrahuje informace o entitách.  \n",
    "Abychom se vyhnuli chybové hlášce\n",
    "```\n",
    "Got unexpected prompt input variables. The prompt expects ['history', 'input'], but got ['entities', 'history'] as inputs from memory, and input as the normal input key. (type=value_error)\n",
    "\n",
    "```\n",
    "musíme do *ConversationChain* přidat nový parametr *prompt*, do kterého vložíme importováním získanou *ENTITY_MEMORY_CONVERSATION_TEMPLATE*"
   ],
   "id": "44e2412ce993d10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "#%%"
   ],
   "id": "3caebd2eb1c09dfa"
  },
  {
   "cell_type": "markdown",
   "id": "ca3731f4-820b-4393-bacd-3f5c5c2e4e01",
   "metadata": {},
   "source": [
    "Pro zkontrolování obsahu paměti tentokrát nemůžeme použít *entity_memory.load_memory_variables({})*, ale musíme aplikovat *conversation.memory.entity_store.store*. Prozatím tam nic není."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "49689bb0-d85e-4892-83e4-422015af71e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory.entity_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "774023ff-d12f-41f4-afc4-7ae885ff7d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Do you know what are differences between rhinos and unicorns?',\n",
       " 'history': \"Human: Hi, who are you?\\nAI: I'm your virtual assistant, here to help you with a variety of tasks and answer any questions you might have. Whether you need information, assistance with a project, or just want to chat about a topic, I'm here for you! How can I assist you today?\",\n",
       " 'entities': {'rhinos': '', 'unicorns': ''},\n",
       " 'response': 'Yes, there are several key differences between rhinos and unicorns:\\n\\n1. **Existence**: Rhinos are real animals that exist in the wild, primarily in Africa and Asia. They are large, thick-skinned mammals known for their distinctive horns. Unicorns, on the other hand, are mythical creatures often depicted as horse-like animals with a single horn on their foreheads. They are part of folklore and fantasy, and there is no scientific evidence of their existence.\\n\\n2. **Physical Characteristics**: Rhinos have a robust body, thick skin, and one or two horns made of keratin (the same material as human nails). Their skin is often gray or brown, and they have a large, barrel-shaped body. Unicorns are typically portrayed as elegant, horse-like creatures, often depicted as white with a spiraled horn and sometimes with a flowing mane and tail.\\n\\n3. **Habitat**: Rhinos inhabit various environments, including savannas, grasslands, and forests, depending on the species. They require large areas of land to roam and graze. Unicorns, being mythical, do not have a specific habitat but are often associated with enchanted forests or magical realms in stories.\\n\\n4. **Behavior**: Rhinos are generally solitary animals, although some species may form small groups. They are herbivores and spend a lot of time grazing. Unicorns, in folklore, are often depicted as gentle and elusive creatures, sometimes possessing magical abilities, and are often associated with purity and grace.\\n\\n5. **Cultural Significance**: Rhinos are often the focus of conservation efforts due to their endangered status and the threats they face from poaching and habitat loss. Unicorns, meanwhile, symbolize various themes in literature and art, such as innocence, beauty, and the fantastical.\\n\\nIf you have more specific questions about either rhinos or unicorns, feel free to ask!'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(input=\"Do you know what are differences between rhinos and unicorns?\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=chat,\n",
    "    verbose=False,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=entity_memory\n",
    ")"
   ],
   "id": "a8372841743652d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "conversation.invoke(input=\"Hi, who are you?\")\n",
    "#%% md"
   ],
   "id": "bdc37d24c6d1c2f4"
  },
  {
   "cell_type": "markdown",
   "id": "cffe2681-f57f-40e8-95ae-752d5d9b4573",
   "metadata": {},
   "source": [
    "# 3. Q&A nad dokumenty"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9554af70b82b4edb"
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5eed3104-f4f1-4f1c-863c-bbbef3e18b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./pomocne_soubory/podminky_debetnich_karet.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5ee6eb52-beeb-4529-9482-18b6a1125ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87da8cb6-9983-4277-8d69-21a57ef01762",
   "metadata": {},
   "source": [
    "Load nahraje krom obsahu stránek dokumentu i metadata. V nich máme název souboru a číslo stránky. Tj. vypadá to nějak takto:\n",
    "```\n",
    "[Document(page_content='  \\n PODMÍNKY D EBETNÍCH KARET  ... výnosů z  trestné činnost i a financování te rorismu , ve znění \\npozdějších předpi sů ', metadata={'source': 'source_files\\\\podminky_debetnich_karet.pdf', 'page': 0}),\n",
    " Document(page_content='PODMÍNKY DEBETNÍCH KARET ... poskytnutí dané Bankovní \\nslužby.  \\n ', metadata={'source': 'source_files\\\\podminky_debetnich_karet.pdf', 'page': 1}),\n",
    "...\n",
    "```\n",
    "Kazda stranka je objekt typu *Document*.\n",
    "Pozn.: To, že u každé stránky - objektu typu *Document* - vidíme na začátku vždy \"PODMÍNKY DEBENTNÍCH KARET\", je dáno skutečností, že se zápatí stránky, které je vždy stejné, nějak dostalo na začátek. To mimo jiné znamená, že je potřeba dokumenty před seriózním použitím začistit. Bohužel se tu též objevuje nadkritické množství mezer v místech, kde mezery neměly být (například hned v \"PODMÍNKY D EBETNÍCH KARET\")."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    " Document(page_content='PODMÍNKY DEBETNÍCH KARET ... poskytnutí dané Bankovní \\nslužby.  \\n ', metadata={'source': 'source_files\\\\podminky_debetnich_karet.pdf', 'page': 1}),\n",
    "...\n",
    "```\n",
    "Kazda stranka je objekt typu *Document*."
   ],
   "id": "2b09700542053811"
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0fe8efbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './pomocne_soubory/podminky_debetnich_karet.pdf', 'page': 0}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7c910-4ed0-4513-8b8a-40d997b41447",
   "metadata": {},
   "source": [
    "Jiný způsob, jak načíst obsah pdf souborů, je založen na práci balíčku [unstructured](https://pypi.org/project/unstructured/). Při přípravě tohoto textu jsem (pokud si dobře pamatuji) pro pfd žádnou rozšířenou verzi (ala pip install \"unstructured[pdf]\") instalovat nemusel. Tudíž jsem nenarazil na potřebu mít Rust kompilátor kvůli jedné z prerekvizit - balíčku safetensors. Nicméně dodatečně bylo třeba nainstalovat balíčky pdf2image a pdfminer.six (nikoli pdfminer - ten už je neudržovaný, navíc snaha o jeho použití skončila s chybovou hláškou \"ModuleNotFoundError: No module named 'pdfminer.high_level'\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "15630cd4-9ce1-424e-b5a5-c702cb5111d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6da361-0c78-4bee-a6af-1990d15fb959",
   "metadata": {},
   "source": [
    "Unstructured loadery mohou fungovat ve dvou modech. Pokud bude parametr *mode* položen rovný \"single\", tak se celý dokument - zde pdfko - po použití metody *load* vrátí jako jeden langchainový *Document* objekt (tj. neproběhne ani rozdělení na stránky). Nicméně pokud se bude *mode* rovnat \"elements\", dojde k roztrhání dokumentu na malé kousky a ty kousky ponesou popisek charakterizují jejich obsah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6b734b67-0ca2-42cd-81e6-521cf1051e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredPDFLoader(\"./pomocne_soubory/podminky_debetnich_karet.pdf\", mode=\"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6c62357f-ddbf-40fd-af3b-3fa61ba69f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%% md\n",
    "Rozdělení na kategorie nicméně není úplně spolehlivé. Když bychom si u našeho dokumentu nechali vypsat všechny \"Title\" dokumenty kódem\n",
    "\n",
    "```python\n",
    "for one_doc in data:\n",
    "    if one_doc.metadata[\"category\"] == \"Title\":\n",
    "        print(one_doc.page_content)\n",
    "```\n",
    "najdeme tam mezi nadpisy a kousky zápatí i normální text, např. \"3D Secure. Všechny námi poskytnuté debetní karty jsou 3D Secure aktivní.\". A naopak v \"NarrativeText\" člověk nalezne pár nadpisů. Je nakonec otázkou, zda není lepší, když si člověk dokument prohlédne a začistí ho ručně podle svého. Nakonec tak bude nejvhodnější single mode unstructured (narozdíl od PyPDFLoaderu nemá nabytečné mezery)\n",
    "#%%\n",
    "loader = UnstructuredPDFLoader(\"./pomocne_soubory/podminky_debetnich_karet.pdf\", mode=\"single\")\n",
    "data = loader.load()"
   ],
   "id": "d4e1679da6b5ce11"
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a7c6497d-ed58-4601-858e-4410eae88c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "\n",
    "fragment = Document(\n",
    "    page_content=\"this is fragment text\",\n",
    "    metadata={\"file\":\"something.pdf\", \"page\":0, \"another_metadata_field\":\"something\"}\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.documents.base import Document\n",
    "\n",
    "fragment = Document(\n",
    "    page_content=\"this is fragment text\",\n",
    "\n",
    "    metadata={\"file\":\"something.pdf\", \"page\":0, \"another_metadata_field\":\"something\"}\n",
    ")"
   ],
   "id": "9908e6344125a538"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Načtení html souboru\n",
    "Pro načítání html stránek existují v rámci Langchainu dvě podporované cesty. Jedna vyžaduje použití balíčku [unstructured](https://pypi.org/project/unstructured/). Jako data použijeme wiki stránku o [jednom druhu křečka](https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster). Zdůrazněme, že následující postup se týka html souboru uloženého na disku, nikoli webové stránky - na to je jiný loader."
   ],
   "id": "5812c985faa63622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain_community.document_loaders import UnstructuredHTMLLoader",
   "id": "d36de342ddde0a39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "loader = UnstructuredHTMLLoader(\"source_files\\\\Winter white dwarf hamster - Wikipedia.htm\")",
   "id": "222a113755034d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data = loader.load()",
   "id": "6be32bd8e7cb8354"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nahraný objekt vypadá (po vyřazení většiny textu z důvodu přehlednosti) takto:\n",
    "```"
   ],
   "id": "5f55b0eb82148536"
  },
  {
   "cell_type": "markdown",
   "id": "59297196-13b6-491c-8991-c4fa23bbd9dc",
   "metadata": {},
   "source": [
    "Nahraný objekt vypadá (po vyřazení většiny textu z důvodu přehlednosti) takto:\n",
    "```\n",
    "[Document(page_content='Toggle the table of contents\\n\\nToggle the table of contents\\n\\nWinter white dwarf hamster\\n\\n43 languages\\n\\nالعربية\\n\\nAsturianu\\n\\nБългарски\\n\\nBrezhoneg\\n\\nCatalà\\n\\nCebuano\\n\\nČeština\\n\\nDeutsch\\n\\nDiné bizaad\\n\\nEesti\\n\\nEspañol\\n\\nEuskara\\n\\nفارسی\\n\\nFrançais\\n\\nFrysk\\n\\n한국어\\n\\nՀայերեն\\n\\nHrvatski\\n\\nBah  \n",
    "...  \n",
    "nCategories:\\n\\nIUCN Red List least concern species\\n\\nPhodopus\\n\\nRodents of Asia\\n\\nMammals described in 1773\\n\\nMammals of Siberia\\n\\nTaxa named by Peter Simon Pallas\\n\\nHidden categories: \\n\\nArticles with short description\\n\\nShort description is different from Wikidata\\n\\nGood articles\\n\\nArticles with \\'species\\' microformats', metadata={'source': 'source_files\\\\Winter white dwarf hamster - Wikipedia.htm'})]\n",
    "```\n",
    "Před použitím bude tedy potřeba provést opravdu masivní začištění."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50eec47-4c9c-424e-88c4-09d1addb7d5e",
   "metadata": {},
   "source": [
    "Opět je možné použít unstructured načítání v modu \"elements\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0846d3b-4aa9-4bfb-8feb-be7ca63f9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredHTMLLoader(\"source_files\\\\Winter white dwarf hamster - Wikipedia.htm\", mode=\"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81140d58-5790-4d63-99d6-487772f2fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f873617-a9d9-446a-8c34-e4f20e0cf046",
   "metadata": {},
   "source": [
    "Výsledek vypadá takto:\n",
    "```\n",
    "[Document(page_content='Toggle the table of contents', metadata={'source': 'source_files\\\\Winter white dwarf hamster - Wikipedia.htm', 'filename': 'Winter white dwarf hamster - Wikipedia.htm', 'file_directory': 'source_files', 'last_modified': '2023-08-22T20:35:12', 'filetype': 'text/html', 'page_number': 1, 'emphasized_text_contents': ['Toggle the table of contents'], 'emphasized_text_tags': ['span'], 'category': 'Title'}),  \n",
    "...  \n",
    " Document(page_content='population density is highly varied.', metadata={'source': 'source_files\\\\Winter white dwarf hamster - Wikipedia.htm', 'filename': 'Winter white dwarf hamster - Wikipedia.htm', 'file_directory': 'source_files', 'last_modified': '2023-08-22T20:35:12', 'filetype': 'text/html', 'page_number': 3, 'link_urls': ['https://en.wikipedia.org/wiki/Population_density'], 'link_texts': ['population density'], 'category': 'NarrativeText'}),\n",
    " Document(page_content='[23]', metadata={'source': 'source_files\\\\Winter white dwarf hamster - Wikipedia.htm', 'filename': 'Winter white dwarf hamster - Wikipedia.htm', 'file_directory': 'source_files', 'last_modified': '2023-08-22T20:35:12', 'filetype': 'text/html', 'page_number': 3, 'link_urls': ['#cite_note-J1979:R1998:E-23'], 'link_texts': ['[23]'], 'category': 'UncategorizedText'}),  \n",
    " ...  \n",
    "Document(page_content=\"Articles with 'species' microformats\", metadata={'source': 'source_files\\\\Winter white dwarf hamster - Wikipedia.htm', 'filename': 'Winter white dwarf hamster - Wikipedia.htm', 'file_directory': 'source_files', 'last_modified': '2023-08-22T20:35:12', 'filetype': 'text/html', 'page_number': 3, 'link_urls': ['https://en.wikipedia.org/wiki/Category:Articles_with_%27species%27_microformats'], 'link_texts': [\"Articles with 'species' microformats\"], 'category': 'ListItem'})]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047f941-6317-4bd8-b3e0-97112ad525d5",
   "metadata": {},
   "source": [
    "Pro načtení html souboru lze též použít loader využívající BeautifulSoup4. Moje první provolání metody *load* vedlo k chybové hlášce obsahující mimo jiné\n",
    "```\n",
    "soup = BeautifulSoup(f, **self.bs_kwargs)\n",
    "```\n",
    "a\n",
    "```\n",
    "FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n",
    "```\n",
    "A to i když byl ve virtuálním prostředí balíček BeautifulSoup4 nainstalován. Co ale chybělo byl balíček [lxml](https://pypi.org/project/lxml/). Po jeho nainstalování se ale (u anglického!) textu objevila další chybová hláška:\n",
    "```\n",
    "UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 27841: character maps to <undefined>\n",
    "```\n",
    "To kvůli tomu, že jsem nepoužil parametr *open_encoding* a html soubor se tak otevíral s defaultním kódováním, kterým je utf-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc2256-b41e-4f68-a934-935fba97b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "loader = BSHTMLLoader(\"source_files\\\\Winter white dwarf hamster - Wikipedia.htm\", open_encoding=\"latin1\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba96dc-63d4-4a0d-ba0b-817372e57f13",
   "metadata": {},
   "source": [
    "Opět z nahraného objektu ukážeme pouze začátek a konec:\n",
    "```\n",
    "[Document(page_content='\\n\\n\\nWinter white dwarf hamster - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload\n",
    "...\n",
    "\\nCookie statement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle limited content width\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'source_files\\\\Winter white dwarf hamster - Wikipedia.htm', 'title': 'Winter white dwarf hamster - Wikipedia'})]\n",
    "```\n",
    "Zdá se, že potřeba začištění je tu ještě větší než u balíčku unstructured..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8bb12b-99c3-4fc4-b7d7-e3eadf00d680",
   "metadata": {},
   "source": [
    "##### [Načtení webové stránky](https://python.langchain.com/docs/integrations/document_loaders/web_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750427f-d1b9-496e-82e8-015233d5a0ae",
   "metadata": {},
   "source": [
    "V praxi bychom upřednostňovali, kdybychom vytěžované stránky nemuseli stahovat a kdyby si Langchain informace natahal přímo z webu. Na to slouží *WebBaseLoader*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "85da32b1-83d7-441d-89d5-423a306afbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "647ef698-62bf-4dcc-95d0-696092ed803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fd97daed-2948-46f2-a89e-a976f97011fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b07301-096d-495e-aeb5-973d7d9416f9",
   "metadata": {},
   "source": [
    "Zde vidíme příklad formátu, v jakém je stránka načtena:\n",
    "```\n",
    "[Document(page_content='\\n\\n\\n\\nWinter white dwarf hamster - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload file\\n\\n\\n\\n\\n\\nLanguages\\n\\nLanguage links are at the top of the page across from the title.\n",
    "...\n",
    "\\nContact Wikipedia\\nCode of Conduct\\nMobile view\\nDevelopers\\nStatistics\\nCookie statement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle limited content width\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster', 'title': 'Winter white dwarf hamster - Wikipedia', 'language': 'en'})]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ff565-b138-4af7-8910-533ba5444794",
   "metadata": {},
   "source": [
    "V rámci jednoho volání lze načíst i více stránek:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "...\n",
    "\\nContact Wikipedia\\nCode of Conduct\\nMobile view\\nDevelopers\\nStatistics\\nCookie statement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle limited content width\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster', 'title': 'Winter white dwarf hamster - Wikipedia', 'language': 'en'})]\n",
    "```\n",
    "#%% md\n",
    "V rámci jednoho volání lze načíst i více stránek:\n",
    "#%%\n",
    "loader = WebBaseLoader([\n",
    "    \"https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster\", \n",
    "    \"https://en.wikipedia.org/wiki/Syrian_hamster\"\n",
    "])\n",
    "#%%\n",
    "data = loader.load()\n",
    "#%% md\n",
    "Ukázka výsledku:\n",
    "```\n",
    "[Document(page_content='\\n\\n\\n\\nWinter white dwarf hamster - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\\n\\n\\n\\n\\n\\n\\t\\tContribute\n",
    "...\n",
    "\\nContact Wikipedia\\nCode of Conduct\\nMobile view\\nDevelopers\\nStatistics\\nCookie statement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle limited content width\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster', 'title': 'Winter white dwarf hamster - Wikipedia', 'language': 'en'}),\n"
   ],
   "id": "954b78e7f794f217"
  },
  {
   "cell_type": "markdown",
   "id": "89a3cdb2-655f-4084-b30c-e4f518bef75d",
   "metadata": {},
   "source": [
    "I pro načítání stránek z webu existuje [unstructured loader](https://python.langchain.com/docs/integrations/document_loaders/url)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "...\n",
    "\\nCode of Conduct\\nMobile view\\nDevelopers\\nStatistics\\nCookie statement\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle limited content width\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://en.wikipedia.org/wiki/Syrian_hamster', 'title': 'Golden hamster - Wikipedia', 'language': 'en'})]\n",
    "```\n",
    "#%% md\n",
    "I pro načítání stránek z webu existuje [unstructured loader](https://python.langchain.com/docs/integrations/document_loaders/url).\n",
    "#%%\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "#%%\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster\", \n",
    "    \"https://en.wikipedia.org/wiki/Syrian_hamster\"\n",
    "]\n",
    "#%% md\n",
    "Když člověk nemá nainstalovaný balíček libmagic, objeví se hlášky\n",
    "```\n",
    "libmagic is unavailable but assists in filetype detection on file-like objects. Please consider installing libmagic for better results.\n",
    "Error fetching or processing https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster, exception: Invalid file. The FileType.UNK file type is not supported in partition.\n",
    "libmagic is unavailable but assists in filetype detection on file-like objects. Please consider installing libmagic for better results.\n",
    "Error fetching or processing https://en.wikipedia.org/wiki/Syrian_hamster, exception: Invalid file. The FileType.UNK file type is not supported in partition.\n",
    "```\n",
    "Načtený objekt pak bude prázdným listem.\n",
    "Problém je, že existuje více libmagic ([zde](https://pypi.org/project/python-libmagic/#description) a [zde](https://pypi.org/project/libmagic/#description)), ty jsou ale relativně staré a prakticky od svého vzniku neudržované...  \n",
    "\n",
    "Též existuje balíček [python-magic](https://pypi.org/project/python-magic/#history). Ten už vypadá živěji, nicméně když nainstalujeme jeho nejnovější verzi, obdržíme errory\n",
    "```\n",
    "Error fetching or processing https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster, exception: module 'magic' has no attribute 'from_buffer'\n",
    "Error fetching or processing https://en.wikipedia.org/wiki/Syrian_hamster, exception: module 'magic' has no attribute 'from_buffer'\n",
    "```\n",
    "Instalace starší verze vede též k chybovým hláškám.  \n",
    "Problém lze vyřešit instalací [python-magic-bin](https://pypi.org/project/python-magic-bin). Nicméně jedná se o binárky z repa zapadaného prachem.\n",
    "#%%\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "data = loader.load()\n",
    "#%% md\n",
    "Ukázka:\n",
    "```\n",
    "[Document(page_content='Toggle the table of contents\\n\\nToggle the table of contents\\n\\nWinter white dwarf hamster\\n\\n43 languages\\n\\nالعربية\\n\\nAsturianu\\n\\nБългарски\\n\\nBrezhoneg\\n\\nCatalà\\n\\nCebuano\\n\\nČeština\\n\\nDeutsch\\n\\nDiné bizaad\\n\\nEesti\\n\\nEspañol\\n\\nEuskara\\n\\nفارسی\\n\\nFrançais\\n\\nFrysk\\n\\n한국어\\n\\nՀայերեն\\n\\nHrvatski\\n\\nBahasa Indonesia\\n\\nItaliano\\n\\nעברית\\n\\nҚазақша\\n\\nKotava\\n\\nLatviešu\\n\\nMagyar\\n\\nمصرى\\n\\nNederlands\\n\\n日本語\\n\\nNorsk bokmål\\n\\nNorsk nynorsk\\n\\nPolski\\n\\nPortuguês\\n\\nRomână\\n  \n",
    "...  \n",
    "Red List least concern species\\n\\nPhodopus\\n\\nRodents of Asia\\n\\nMammals described in 1773\\n\\nMammals of Siberia\\n\\nTaxa named by Peter Simon Pallas\\n\\nHidden categories: \\n\\nArticles with short description\\n\\nShort description is different from Wikidata\\n\\nGood articles\\n\\nArticles with \\'species\\' microformats', metadata={'source': 'https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster'}),"
   ],
   "id": "ade1a88985a5da29"
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "31fa2cd8-8851-4337-9678-d0edc164786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845948c4-148d-4061-b304-533c3f46d8c3",
   "metadata": {},
   "source": [
    "Ukázka:\n",
    "```\n",
    "[Document(page_content='Toggle the table of contents\\n\\nToggle the table of contents\\n\\nWinter white dwarf hamster\\n\\n43 languages\\n\\nالعربية\\n\\nAsturianu\\n\\nБългарски\\n\\nBrezhoneg\\n\\nCatalà\\n\\nCebuano\\n\\nČeština\\n\\nDeutsch\\n\\nDiné bizaad\\n\\nEesti\\n\\nEspañol\\n\\nEuskara\\n\\nفارسی\\n\\nFrançais\\n\\nFrysk\\n\\n한국어\\n\\nՀայերեն\\n\\nHrvatski\\n\\nBahasa Indonesia\\n\\nItaliano\\n\\nעברית\\n\\nҚазақша\\n\\nKotava\\n\\nLatviešu\\n\\nMagyar\\n\\nمصرى\\n\\nNederlands\\n\\n日本語\\n\\nNorsk bokmål\\n\\nNorsk nynorsk\\n\\nPolski\\n\\nPortuguês\\n\\nRomână\\n  \n",
    "...  \n",
    "Red List least concern species\\n\\nPhodopus\\n\\nRodents of Asia\\n\\nMammals described in 1773\\n\\nMammals of Siberia\\n\\nTaxa named by Peter Simon Pallas\\n\\nHidden categories: \\n\\nArticles with short description\\n\\nShort description is different from Wikidata\\n\\nGood articles\\n\\nArticles with \\'species\\' microformats', metadata={'source': 'https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster'}),\n",
    " Document(page_content='Toggle the table of contents\\n\\nToggle the table of contents\\n\\nGolden hamster\\n\\n49 languages\\n\\nالعربية\\n\\nAragonés\\n\\nБеларуская (тарашкевіца)\\n\\nБългарски\\n\\nCatalà\\n\\nCebuano\\n\\nČeština\\n\\nCymraeg\\n\\nDeutsch\\n\\nDiné bizaad\\n\\nEesti\\n\\nEspañol\\n\\nEsperanto\\n\\nEuskara\\n\\nفارسی\\n\\nFrançais\\n\\nFrysk\\n\\nGaeilge\\n\\n한국어\\n\\nHrvatski\\n\\nBahasa Indonesia\\n\\nIta  \n",
    " ...  \n",
    " abic-language text\\n\\nAll articles with unsourced statements\\n\\nArticles with unsourced statements from June 2019\\n\\nArticles with unsourced statements from September 2019\\n\\nCommons link is on Wikidata\\n\\nArticles with GND identifiers\\n\\nArticles with J9U identifiers\\n\\nArticles with LCCN identifiers\\n\\nArticles with NKC identifiers\\n\\nArticles containing video clips', metadata={'source': 'https://en.wikipedia.org/wiki/Syrian_hamster'})]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d8ec45ce-fafb-4027-86d9-4959eb81d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredURLLoader(urls=urls, mode=\"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e7f3f5a7-4c6f-4ee7-acaa-ee1974576884",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48824f7a-6dbb-4bfa-886d-b29ac45ff956",
   "metadata": {},
   "source": [
    "Ukázka:\n",
    "```\n",
    "[Document(page_content='Toggle the table of contents', metadata={'filetype': 'text/html', 'page_number': 1, 'url': 'https://en.wikipedia.org/wiki/Winter_white_dwarf_hamster', 'emphasized_text_contents': ['Toggle the table of contents'], 'emphasized_text_tags': ['span'], 'category': 'Title'}),  \n",
    "...  \n",
    " Document(page_content='A hamster wheel is a common type of environmental enrichment, and it is important that hamsters have a wheel in their cage. TVT recommends wheels should be at least 30\\xa0cm for Syrian hamsters, since smaller diameters lead to permanent spinal curvatures, especially in young animals. They also recommend a solid running surface because rungs or mesh can cause injury.[19] A hamster should be able to run on its wheel without arching its back. A hamster that has to run with an arched back can have back pain and spine problems. A variety of toys and cardboard tubes and boxes can help to provide enrichment, as they are energetic and need space to exercise.[20]', metadata={'filetype': 'text/html', 'page_number': 3, 'url': 'https://en.wikipedia.org/wiki/Syrian_hamster', 'link_urls': ['/wiki/Hamster_wheel', '#cite_note-19', '#cite_note-20'], 'link_texts': ['hamster wheel', '[19]', '[20]'], 'category': 'NarrativeText'}),\n",
    " Document(page_content='Most hamsters in American and British pet stores are golden hamsters. Originally, golden hamsters occurred in just one color – the mixture of brown, black, and gold, but they have since developed a variety of color and pattern mutations, including cream, white, blonde, cinnamon, tortoiseshell, black, three different shades of gray, dominant spot, banded, and dilute.[citation needed]', metadata={'filetype': 'text/html', 'page_number': 3, 'url': 'https://en.wikipedia.org/wiki/Syrian_hamster', 'link_urls': ['/wiki/Wikipedia:Citation_needed'], 'link_texts': [None], 'emphasized_text_contents': ['citation needed', 'citation needed'], 'emphasized_text_tags': ['i', 'span'], 'category': 'NarrativeText'}),  \n",
    "...  \n",
    "Document(page_content='Articles containing video clips', metadata={'filetype': 'text/html', 'page_number': 3, 'url': 'https://en.wikipedia.org/wiki/Syrian_hamster', 'link_urls': ['/wiki/Category:Articles_containing_video_clips'], 'link_texts': ['Articles containing video clips'], 'category': 'ListItem'})]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc3413",
   "metadata": {},
   "source": [
    "##### Načtení YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8892bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "69103e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install yt_dlp\n",
    "# ! pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pozor, muze trvat par minut\n",
    "url=\"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\n",
    "save_dir=\"docs/youtube/\"\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url],save_dir),\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content[0:500]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "# Pozor, muze trvat par minut\n",
    "url=\"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\n",
    "save_dir=\"docs/youtube/\"\n",
    "loader = GenericLoader(\n",
    "\n",
    "    YoutubeAudioLoader([url],save_dir),\n",
    "\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "docs = loader.load()"
   ],
   "id": "2294c3e75ca5819a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "docs[0].page_content[0:500]",
   "id": "9fedc14d870bdbcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Notion",
   "id": "42c1b3298f2f6edc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Follow steps [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/integrations/notion) for an example Notion site such as [this one](https://yolospace.notion.site/Blendle-s-Employee-Handbook-e31bff7da17346ee99f531087d8b133f):\n",
    "\n",
    "* Duplicate the page into your own Notion space and export as `Markdown / CSV`.\n",
    "* Unzip it and save it as a folder that contains the markdown file for the Notion page.\n",
    " "
   ],
   "id": "cf10644b874af096"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"docs/Notion_DB\")\n",
    "docs = loader.load()"
   ],
   "id": "7ba4789ef003ac0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(docs[0].page_content[0:200])",
   "id": "9d1884ee84e81d4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "docs[0].metadata",
   "id": "eb43994cdf71a665"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## B. Splittery\n",
    "Obsah stránek by zejména pro výživnější texty mohl být větší, než počet tokenů, které dokáže v jeden okamžik jazykový model zpracovat. Proto se dokumenty musí rozdělit na malé kousky.  \n"
   ],
   "id": "8acaa19feeedb327"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "166985bf-8997-40a1-8a07-ee258f964622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbededcb-2687-4cac-8d37-af985f7d1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 250\n",
    "chunk_overlap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bc5b7e0-ea69-4886-8125-7cff221fd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038deb21-0278-4a83-a7fc-34a15163d6b5",
   "metadata": {},
   "source": [
    "Pokud bychom chtěli splittovat obyčejný text, použili bychom metodu *split_text* instance *CharacterTextSplitter*, přičemž inkriminovaný text by byl předán metodě jako parametr. Viz nasledujici priklad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "181551ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#%%\n",
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n"
   ],
   "id": "73bd392d8cc2c216"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5de92df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 11, which is longer than the specified 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'writing',\n",
       " 'documents,',\n",
       " 'writers',\n",
       " 'will use',\n",
       " 'document',\n",
       " 'structure',\n",
       " 'to group',\n",
       " 'content.',\n",
       " 'This can',\n",
       " 'convey to',\n",
       " 'the',\n",
       " 'reader,',\n",
       " 'which',\n",
       " \"idea's are\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    separator = ' '\n",
    ")\n",
    "char_text_splitter.split_text(some_text)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45697669",
   "metadata": {},
   "source": [
    "### Split_Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4ecfa",
   "metadata": {},
   "source": [
    "Nicméně my máme list obsahující navíc krom samotného textu i metadata o stránkách původního pdfka, s čímž není *split_text* očekávající string kompatibilní. Proto musíme použít metodu *split_documents*.  \n",
    "Jak vlastně splitování funguje?\n",
    " - pokud separátor není v dokumentu (v našem případě dokument = jedna stránka) přítomen, k žádnému rozsekání nedochází. Tj. pokud na vstupu bylo 10 stránek, bude stejný počet i na výstupu, což pro praktické použití optimální není.  \n",
    " - pokud je separátor vzácný, tak se některé dokumenty vůbec nerozsekají (protože v nich separátor není), jiné se rozdělí třeba jen podle jediného výskytu separátoru (i když i pak budou oba vzniklé fragmenty větší než *chunk_size*)\n",
    " - pokud je separátor přítomný často, proběhne rozsekávání tak, aby byla téměř dosažena, ale nikdy překročena *chunk_size* (resp. možná se dokument rozseká podle všech výskytů separátoru, ale pak se na sebe některé fragmenty znova nalepí).\n",
    "\n",
    "Bacha, pokud používáme regexy (*is_separator_regex* = True) a zvolíme nějaký obecný pattern (třeba \"\\\\w\"), tak se nám tímto patternem nahradí všechny relevantní znaky nahradí. Tj. fragment pak třeba vypadá takto:\n",
    "```\n",
    "'\\\\w \\\\w \\\\w \\\\w   \\n \\n \\n \\n \\n\\\\w \\\\w \\\\w, \\\\w. \\\\w., \\\\w \\\\w:  \\n\\\\w \\\\w, \\\\w \\\\w \\\\w \\\\w \\\\w. \\\\w, \\\\w \\\\w \\\\w, \\\\w: \\\\w  \\n\\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w, \\\\w \\\\w, \\\\w \\\\w \\\\w \\\\w/\\\\w  \\n\\\\w \\\\w \\\\w.\\\\w \\n \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w \\\\w  \\\\w \\\\w, \\\\w \\\\w \\n\\\\w \\\\w \\\\w  \\\\w \\\\w \\\\w  \\\\w \\\\w'\n",
    "```\n",
    "Takovému chování zabráníme umístěním parametru *keep_separator*=True do konstruktoru *CharacterTextSplitter*u.  \n",
    "Níže vidíme příklad výstupu metody *split_documents*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c8329132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"./source_notebooks/langchain/Chat With LLM/docs/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "febbb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "69906715",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f1eccef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4d4e9e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05e66b-e70d-4c9d-8d72-dc342f9017ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so \n",
      "I personally work in machine learning, and I've worked on it for about 15 years now, and \n",
      "I actually think that machine learning is the most exciting field of all the computer \n",
      "sciences. So I'm actually always excited about teaching this class. Sometimes I actually \n",
      "think that machine learning is not only the most exciting thing in computer science, but \n",
      "the most exciting thing in all of human endeavor, so maybe a little bias there.  \n",
      "I also want to introduce the TAs, who are all graduate students doing research in or \n",
      "related to the machine learning and all aspects of machine learning. Paul Baumstarck' metadata={'source': './source_notebooks/langchain/Chat With LLM/docs/MachineLearning-Lecture01.pdf', 'page': 0}\n",
      "973\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])\n",
    "print(len(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44b0a4-0b7f-4bfb-af53-8a828ce25e0f",
   "metadata": {},
   "source": [
    "Výše jsme viděli, že *CharacterTextSplitter* není pro práci s obyčejným textem kvůli omezení na jeden separátor moc praktický. Proto je vhodnější použít *RecursiveCharacterTextSplitter*. Ten nemá parametr *separator*, nýbrž *separators*. Do něj se vkládá list potenciálních separátorů. Jeho defaultní hodnotou je [\"\\n\\n\", \"\\n\", \" \", \"\"]. Myšlenka za touto volbou je taková, že se spitter snaží držet pohromadě odstavce (a potom věty a pak slova) tak dlouho, jak to jen jde, kvůli zachování informace schované v textu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd2dac",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5a1dc30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3387ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size =26\n",
    "chunk_overlap = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "613e319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f94947",
   "metadata": {},
   "source": [
    "Why doesn't this split the string below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "461c77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1a68633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "283c9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "209aa856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612e0e1",
   "metadata": {},
   "source": [
    "Ok, this splits the string but we have an overlap specified as 5, but it looks like 3? (try an even number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "87684fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e96a8038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b78118",
   "metadata": {},
   "source": [
    "Z techhle dvou je `RecursiveCharacterTextSplitter` recommended for generic text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "aefc94f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "fd8f4af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e4fa60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6fffa39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52e607",
   "metadata": {},
   "source": [
    "Let's reduce the chunk size a bit and add a period to our separators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "65b6ce0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\",\n",
       " 'closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this',\n",
       " 'string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\"\n",
    "#%%\n",
    "len(some_text)\n",
    "#%%\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size=450,\n",
    "\n",
    "    chunk_overlap=0, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ],
   "id": "a983d0f389cf5c2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "r_splitter.split_text(some_text)\n",
    "#%% md\n",
    "Let's reduce the chunk size a bit and add a period to our separators:\n",
    "#%%\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)\n",
    "#%%\n",
    "r_splitter = RecursiveCharacterTextSplitter("
   ],
   "id": "5217ffc1f8479e2b"
  },
  {
   "cell_type": "markdown",
   "id": "9e8df8d6",
   "metadata": {},
   "source": [
    "##### Splittovani Programovacich Jazyku - Language()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a51cc3-a01c-403c-b3f5-83a50e00d6b3",
   "metadata": {},
   "source": [
    "V případě, že bychom chtěli pracovat nikoli s přirozeným jazykem, ale s počítačovým kódem, mohou nám pomoci již předpřipravené separátory pro několik hlavních programovacích jazyků. Ty získáme naimportováním *Language* z *langchain.text_splitter*. Pokud separátory chceme vidět, musíme použít *get_separators_for_language*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ed617afb-b579-4af7-be99-2e84b283caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6a0fde4e-6531-4718-b93b-0c1d77c42b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nclass ', '\\ndef ', '\\n\\tdef ', '\\n\\n', '\\n', ' ', '']\n"
     ]
    }
   ],
   "source": [
    "print(RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21661ce-713d-49b9-8ed2-de0ae0e54fab",
   "metadata": {},
   "source": [
    "Při samotném praktickém použití vložíme *Language.PYTHON* do parametru *language* metody *from_language* (viz příklad převzatý z dokumentace):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1cbbdc0c-a420-4121-b9b5-b2e38fa7005c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='def hello_world():\\n    print(\"Hello, World!\")'),\n",
       " Document(metadata={}, page_content='# Call the function\\nhello_world()')]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af14e2f6",
   "metadata": {},
   "source": [
    "### Token Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7282d",
   "metadata": {},
   "source": [
    "We can also split on token count explicity, if we want.\n",
    "\n",
    "This can be useful because LLMs often have context windows designated in tokens.\n",
    "\n",
    "Tokens are often ~4 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0b148768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bbb61311",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5edb2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"foo bar bazzyfoo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5081af62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foo', ' bar', ' b', 'az', 'zy', 'foo']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b24a7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b2d1fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ef6d3e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './source_notebooks/langchain/Chat With LLM/docs/MachineLearning-Lecture01.pdf', 'page': 0}, page_content='MachineLearning-Lecture01  \\n')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a966d887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './source_notebooks/langchain/Chat With LLM/docs/MachineLearning-Lecture01.pdf',\n",
       " 'page': 0}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b6e41",
   "metadata": {},
   "source": [
    "### Context aware splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e7657",
   "metadata": {},
   "source": [
    "Chunking aims to keep text with common context together.\n",
    "\n",
    "A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.\n",
    "\n",
    "We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks, as show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be1a26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c1fa6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_document = \"\"\"# Title\\n\\n \\\n",
    "## Chapter 1\\n\\n \\\n",
    "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
    "### Section \\n\\n \\\n",
    "Hi this is Lance \\n\\n \n",
    "## Chapter 2\\n\\n \\\n",
    "Hi this is Molly\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2990d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36de2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84c63563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1'}, page_content='Hi this is Jim  \\nHi this is Joe')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d3c39de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header 1': 'Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section'}, page_content='Hi this is Lance')"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_header_splits[1]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Section \\n\\n \\\n",
    "Hi this is Lance \\n\\n \n",
    "## Chapter 2\\n\\n \\\n",
    "Hi this is Molly\"\"\"\n",
    "#%%\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "#%%\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "#%%\n",
    "md_header_splits[0]\n",
    "#%%\n",
    "md_header_splits[1]\n",
    "#%% md\n",
    "### Dalsi Splittery\n",
    "#%% md\n",
    "Viz langchain.text_splitter.\n",
    "* **MarkdownHeaderTextSplitter()**\n",
    "* **TokenTextSplitter()**\n",
    "* **SentenceTransformersTokenTextSplitter()** - Splitting textu podle tokenu\n",
    "\n",
    "* **NLTKTextSplitter()** - implementuje splittovani textu pomoci knihovny NLTK\n",
    "* **SpacyTextSplitter()** - implementuje splitting text pomoci Spacy\n",
    "#%% md\n",
    "## C. Embeddings, vectorstore"
   ],
   "id": "f23bac23aa9d32e8"
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "031a4874-ecc8-4a75-9d74-a68740c490af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e6b81c13-c1a9-47bf-b219-90e3d728ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./pomocne_soubory/podminky_debetnich_karet.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = loader.load()\n",
    "\n",
    "chunk_size = 250\n",
    "chunk_overlap = 50\n",
    "\n",
    "rec_text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    ")\n",
    "\n",
    "rec_splits = rec_text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381eaf05-1f1d-4b7d-947c-3ccd6506597e",
   "metadata": {},
   "source": [
    "Napřed specifikujeme, jaký model budeme používat pro výpočet embeddingů. *OpenAIEmbeddings* znamená, že použijeme model text-embedding-ada-002 od OpenAI. Ten při klasickém používání ala chatování či doplňování textu není tak mocný jako GPT3.5 nebo aspoň DaVinci. Nicméně při nápočtu embeddingů to zas takovou roli nehraje a naopak se hodí, že je tento model řádově levnější."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6eee558e-2f58-4654-85ed-e8664fe404f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1f58a723-cb3b-48b8-a389-19bf36484192",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e295e",
   "metadata": {},
   "source": [
    "Kdyz uz mame inicializovany embedding, muzeme volat na nase vety embeddingovy model a prevest cele vety na vektor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2935f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff73d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22608a6",
   "metadata": {},
   "source": [
    "A pomoci kosinove podobnosti pak hledat nejpodobnejsi vety:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "284753c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "928c3dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9631511809630344"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "7f856dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.770203137103822"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "533719e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.759054062979165"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding2, embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089a0ee",
   "metadata": {},
   "source": [
    "Nicmene, to by bychom museli provest pro kazdou kombinaci hledaneho vyrazu a predem embeddovanych dokumentu. To by v praxi vedlo k tomu, ze si napiseme funkci, do ktere hodime nami hledany dotaz, ktery se nasledne porovna s nejakou databazi jiz existujicich embeddovanych kusu dokumentu a vrati to $k$ nejvic podobnych dokumentu. Proto v langchain balicku vznikl vectorestore, ktery obsahuje onu databazi embeddovanych dokumentu a taky obsahuje metodu _similarity_search()_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f609539d-1bb1-42d8-80c6-0eea6cd372ad",
   "metadata": {},
   "source": [
    "Pozn.: v případě práce s Azure OpenAI použijeme *AzureOpenAIEmbeddings*. Musí se specifikovat deployment, ve kterém je nasazený model určený na embeddingování. Taktéž je třeba nastavit *chunk_size* na jedničku (jinak se objeví tuším \"too many inputs\" chybová hláška).\n",
    "```python\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(deployment=\"deplyment_name_s_embedding_modelem\", chunk_size=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce2462-d66a-46d5-b34a-949aaa3c547b",
   "metadata": {},
   "source": [
    "Pro použití Chromy musíme napřed nainstalovat odpovídající balíček [chromadb](https://pypi.org/project/chromadb/) (pokud se instalace nezdaří, máte možná příliš novou verzi Pythonu - v době psaní těchto řádků byla verze 3.10 ok, ale 3.12 už ne). Následně je potřeba provést importování."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8f75edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e3c534d5-abf0-47c9-87a6-14db0d70b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ffe2b-92ff-4e7d-8f4f-5b225ef896a5",
   "metadata": {},
   "source": [
    "Chroma je souborová databáze. Musíme tedy specifikovat, kam se budou její soubory (fakticky jde o parquety) ukládat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b51e8698-77d7-4ced-ac35-07205982dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_doc_dir = './embeddings/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "09234030",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./embeddings/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2a17710e-554b-4ee7-aeab-01dc689c4d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=rec_splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=simple_doc_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ceac1-dda0-4924-bdc7-5cf9466e7a81",
   "metadata": {},
   "source": [
    "Na následující příkaz nesmíme zapomenout, neboť bez něj by se vektory fakticky neuložily na disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f4b53a83-2dcd-4a2a-9f66-0cef265ea6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_28553/3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e7800-0d04-4ad5-9880-650f6752994b",
   "metadata": {},
   "source": [
    "Pro načtení uložených embeddingů se použije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "195db05b-d77c-4500-8c25-a59a1b77ec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_28553/3379508841.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(\n",
    "    persist_directory=simple_doc_dir, \n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720889f2-f41e-44ba-b994-8b3793da0888",
   "metadata": {},
   "source": [
    "V běžném provozu asi nebudeme následující příkazy krom občasných kontrol potřebovat. Nicméně ne vždy se člověk v běžném provozu pohybuje...  \n",
    "Pro počet záznamů v konkrétním vectorstoru použijeme metodu *\\_collection.count*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a3114732-f64e-4fc7-8534-ff2185b676fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcad8d4-981d-4599-9fa5-f8d8f197b4d4",
   "metadata": {},
   "source": [
    "Metoda *peek* ukáže několik prvních záznamů (počet je specifikován parametrem *limit*, přičemž defaultní hodnota má velikost 10).\n",
    "```\n",
    "vectordb._collection.peek(limit=3)\n",
    "```\n",
    "Výstup metody není z nejmenších, proto ho zde v surové podobě neukazujeme. Jedná se o slovník, ve kterém jsou postupně tyto klíče a hodnoty:\n",
    "- ids s listem idček jednotlivých fragmentů dokumentů  \n",
    "- embeddings s listem vektorů (alias listů floatů), ve kteréch jsou fragmenty dokumentů zakódované  \n",
    "- metadatas s listem metadat, která nabývají podoby slovníků s klíči \"page\" a \"source\"\n",
    "- documents s listem samotných fragmentů dokumentů v textové podobě\n",
    "```\n",
    "{'ids': ['a737d8ec-39ea-11ee-8841-80ce622bc396',\n",
    "  'a73b7183-39ea-11ee-a74a-80ce622bc396',\n",
    "  'a73b7184-39ea-11ee-bd3c-80ce622bc396'],\n",
    " 'embeddings': [[0.0016977092018350959,\n",
    "   -0.013771715573966503,\n",
    "   0.0062872255221009254,\n",
    "   ....\n",
    "   ....\n",
    "   -0.0042107910849153996,\n",
    "   -0.027450013905763626,\n",
    "   -0.02284945175051689,\n",
    "   0.06665701419115067,\n",
    "   ...]],\n",
    " 'metadatas': [{'page': 0,\n",
    "   'source': 'source_files\\\\podminky_debetnich_karet.pdf'},\n",
    "  {'page': 0, 'source': 'source_files\\\\podminky_debetnich_karet.pdf'},\n",
    "  {'page': 0, 'source': 'source_files\\\\podminky_debetnich_karet.pdf'}],\n",
    " 'documents': ['PODMÍNKY D EBETNÍCH KARET   \\n \\n \\n \\n \\nKomerční bank a, a. s., se sídlem:  \\nPraha 1, Na Pří kopě 33 čp. 969, PSČ 114 07, IČO: 45317054  \\nZAPSAN Á V OBCHODNÍM REJSTŘÍKU VEDENÉM MĚSTSKÝM SOUDEM V PRAZE, ODDÍL B, VLOŽKA 1 360 1/10',\n",
    "  'VER DDT_ PODMPKEV.PDF \\n Tyto Podmínky debetních karet obsahují bližší úpravu práv a povinností vyplývajících z  uzavřené smlouvy, na základě',\n",
    "  'které je poskytnuta  debetní karta v  souladu s  pravidly příslušné Karetní společnosti.  Seznamte se prosím důkladně \\ns tímto dokumentem. Vaše p řípadné dotazy rádi zodpovíme.  \\n \\nČlánek 1. Poskytnutí  debetní karty a její obnova']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2edb80d-5f80-40f5-a92f-c4dc8885f455",
   "metadata": {},
   "source": [
    "Metoda get (výsledek je stejný pro *get* i pro *\\_collection.get*) slouží k získání buďto některých záznamů (to tehdy, když do parametru ids vložíme string či list stringů s idčky) anebo všech záznamů (když idčka nespecifikujeme; počet vrácených fragmentů lze omezit parametrem *limit*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2adb91c1-afc5-4f11-b2cb-8c5bb54d3721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.get(ids=\"a73b7184-39ea-11ee-bd3c-80ce622bc396\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dde3c5-2f3c-4878-83dc-7a7705f98ff8",
   "metadata": {},
   "source": [
    "Defaultně se v odpovědi neobjeví embeddingy. Toto chování je řízené parametrem *include*, který defaultně obsahuje [\"metadatas\", \"documents\"]. Tudíž pokud chceme embeddingy, musíme je do listu explicitně přidat.\n",
    "```\n",
    "vectordb.get(\n",
    "    ids=\"a73b7184-39ea-11ee-bd3c-80ce622bc396\",\n",
    "    include=[\"metadatas\", \"documents\", \"embeddings\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ff5aa-7181-44a3-aece-90e4c5571640",
   "metadata": {},
   "source": [
    "Pokud chceme realizovat filtrování na základě metadat, musíme použít parametr *where*. Do něj vložíme slovník, kde klíčem bude \"page\" nebo \"source\" a hodnotou číslo stránky či zdrojový soubor>\n",
    "```\n",
    "vectordb.get(where={\"page\": 0})\n",
    "```\n",
    "```\n",
    "vectordb.get(where={\"source\": \"source_files\\\\podminky_debetnich_karet.pdf\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf2b5c-bbb7-46d8-9cdc-69273e745fc0",
   "metadata": {},
   "source": [
    "Pro získávání nejpodobnějších dokumentů vstupnímu textu sice slouží metoda *query* s parametrem *query_texts*, nicméně my v praxi budeme spíše používat langchainovou nadstavbu nad touto metodou."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57deb410-67c5-4716-9d44-4d7158ee0e7c",
   "metadata": {},
   "source": [
    "Co musíme udělat, když chceme některý ze záznamů updatovat? Vezměme si náhodně fragment i idčkem \"a73b7184-39ea-11ee-bd3c-80ce622bc396\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7f138cd9-75d5-440d-8c91-b8b8805807ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_fragment = vectordb.get(ids=\"a73b7184-39ea-11ee-bd3c-80ce622bc396\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c0a46-5c05-4d3f-b11c-9b0b0c8c27c6",
   "metadata": {},
   "source": [
    "Jeho metadata vypadají takto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "16ce3dfd-10c9-42a0-9646-c1037cb997fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_fragment[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fa92b-730b-46d0-8f44-9f05f38bc207",
   "metadata": {},
   "source": [
    "Nyní metadata přepíšeme - přidáme další pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8ab5eb26-c33e-4ea7-9994-48e0ffbd9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_fragment[\"metadatas\"] = [{\n",
    "    'page': 0, \n",
    "    'source': 'source_files\\\\podminky_debetnich_karet.pdf',\n",
    "    \"some_info\": \"Terms of use of debit cards\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff86c6-d3e1-4c9e-b59e-afe70c3decf8",
   "metadata": {},
   "source": [
    "Provedeme *update* stejnejmennou metodou (bacha, musí to být v *\\_collection* - *vectordb.update* se vztahuje na objekty typu *Document*). Prvním parametrem je idčko, poté zde máme parametr *metadatas*, do kterého vložíme nová metadata. Mohl by tu být i parametr embeddings (pro update embeddingů) či documents (pro úpravu samotných textů)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a73608c4-42d2-413a-8226-d76dc9293fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Update of nonexisting embedding ID: a73b7184-39ea-11ee-bd3c-80ce622bc396\n"
     ]
    }
   ],
   "source": [
    "vectordb._collection.update(\"a73b7184-39ea-11ee-bd3c-80ce622bc396\", metadatas=some_fragment[\"metadatas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d61657c-a501-4a1b-9e46-862a2a3253ad",
   "metadata": {},
   "source": [
    "Update se opravdu provedl."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    'page': 0, \n",
    "    'source': 'source_files\\\\podminky_debetnich_karet.pdf',\n",
    "\n",
    "    \"some_info\": \"Terms of use of debit cards\"\n",
    "}]\n",
    "#%% md\n"
   ],
   "id": "5f015e9c1c5febfd"
  },
  {
   "cell_type": "markdown",
   "id": "6645d59f-4cdf-497d-8039-87aba5babff6",
   "metadata": {},
   "source": [
    "Podobným způsobem by probíhalo i mazání záznamů, kdy bychom použili metodu *delete*:\n",
    "```\n",
    "vectordb._collection.delete(ids=\"a73b7183-39ea-11ee-a74a-80ce622bc396\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ab4f4",
   "metadata": {},
   "source": [
    "##### VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dedf97e",
   "metadata": {},
   "source": [
    "Jiny vector store, ktery stoji za zminku je DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "3ef3060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3375050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f88eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d1e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_replacement_model = OpenAI(temperature=0, \n",
    "                               model='gpt-3.5-turbo-instruct')\n",
    "\n",
    "response = index.query(query, \n",
    "                       llm = llm_replacement_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10544d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e51244",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd457f3-ccf6-4113-8562-8292a990334a",
   "metadata": {},
   "source": [
    "## D. Similarity search\n",
    "Řekli jsme si, že metodu *query* na hledání podobnosti dokumentů používat nebudeme. Co ale místo ní máme k dispozici?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7adf4d78-ad11-4fe3-9ec4-44baf1eb495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Jak dlouho debentní karta platí?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f61fcd-7387-4b53-9565-cffbafce3139",
   "metadata": {},
   "source": [
    "Metoda *similarity_search* vyhledává *k* nejpodobnějších fragmentů dokumentů k dokumentu vloženému do parametru *query*. Založená je na počítání cosinové podobnosti mezi vektory fragmentů."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "response = index.query(query, \n",
    "\n",
    "                       llm = llm_replacement_model)\n",
    "#%%\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "#%%"
   ],
   "id": "69e3910527fc6078"
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "0c78720b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6 Platnost. Debetní k artu lze používat do posledního dne měsíce a roku doby platnosti na ní uvedené. \\nPoužitím obnovené karty dle čl. 1.8 Podmínek končí platnost karty původní bez ohledu na předchozí větu.'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sim_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7706efa-3e9d-485b-ad58-d2d8e78b3a9a",
   "metadata": {},
   "source": [
    "Lze do ní přidat i parametr *filter*, který se hodí pro omezení množiny, ve které se podobné framenty hledají, a to sice na základě metadat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4c745af6-7efb-48e7-bbf2-faecf99f1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_sim_docs = vectordb.similarity_search(\n",
    "    query=question,\n",
    "    k=3,\n",
    "    filter={\"page\":0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "da743cf4-25f7-4ecd-a7d5-ba5e6ffa18eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='1.6 Platnost. Debetní k artu lze používat do posledního dne měsíce a roku doby platnosti na ní uvedené. \\nPoužitím obnovené karty dle čl. 1.8 Podmínek končí platnost karty původní bez ohledu na předchozí větu.'),\n",
       " Document(metadata={'page': 0, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='platnosti použita, nejsme povinni ji obnovit. \\n1.9 Neposkytnutí nové debetní karty. Nemá-li Držitel zájem o poskytnutí nové debetní karty dle  čl. 1.8'),\n",
       " Document(metadata={'page': 0, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='1.1 Žádost o debetní kartu. O poskytnutí debetní karty můžete požádat pro sebe nebo pro kteroukoli  třetí \\nosobu. V  odůvodněných případech jsme oprávněni vaš i žádost odmít nout a požadovanou kartu')]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sim_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323089e",
   "metadata": {},
   "source": [
    "Nebo jiny hypoteticky priklad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "42d99e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are some movies about aliens made in 1980?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e380321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_docs = vectordb.similarity_search(\n",
    "    query=question,\n",
    "    k=3,\n",
    "    filter={\"year\":1980}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a78e5-3dc4-4591-97d3-6316c7012d92",
   "metadata": {},
   "source": [
    "Metoda *similarity_search* u vrácených fragmentů neukazuje, jak moc jsou dokumentu v *query* podobné. Tuto informaci můžeme ale dostat s metodou *similarity_search_with_score* - ta vrací velikost cosinové podobnosti, tj. menší číslo je lepší a nejlepší je 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0b8a6604-34d5-41cd-8d53-1877e8d95fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'page': 6, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='7/10 \\nVER DDT_PODMPKEV.PDF \\n \\nČlánek 8. Ztráta, odcizení, zadržení debetní karty v bankomatu \\n \\n8.1 Oznamovací povinnost Držitele . V případě ztráty, odcizení nebo zneuž ití debetní karty nebo mobilního'),\n",
       "  0.6371698975563049),\n",
       " (Document(metadata={'page': 4, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='podpisu protokol u o rekl amaci. Reklamace týkající se tr ansakce platební kartou za zboží nebo služby \\nposkytnuté třetí stranou prostřednictvím internetu nebo přímo v obchodním místě musí být při nespolupráci'),\n",
       "  0.6413229703903198),\n",
       " (Document(metadata={'page': 4, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='neuplatníte reklamaci do 30 Obchodních dnů od doručení zpráv o zúčtování (výpisů), ve kterých byla nebo \\nměla být  reklamovaná transa kce uved ena, aniž by vám v tom bránily důvody hodné zvláštníh o zřetele,'),\n",
       "  0.6418606042861938)]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_score_docs = vectordb.similarity_search_with_score(query=question,k=3)\n",
    "sim_score_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17345ec0-dbe8-422e-9f3d-6193b786da49",
   "metadata": {},
   "source": [
    "Podobně funguje *similarity_search_with_relevance_scores*, pouze je cosinová podobnost přepočítána na relevance score. To se nalézá v uzavřeném intervalu mezi 0 a 1, přičemž 0 jsou zcela nepodobné dokumenty, zatímco 1 mají zcela identické dokumenty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "c5160c03-35de-4d50-b388-6c4f550c7807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'page': 6, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='7/10 \\nVER DDT_PODMPKEV.PDF \\n \\nČlánek 8. Ztráta, odcizení, zadržení debetní karty v bankomatu \\n \\n8.1 Oznamovací povinnost Držitele . V případě ztráty, odcizení nebo zneuž ití debetní karty nebo mobilního'),\n",
       "  0.549452844669999),\n",
       " (Document(metadata={'page': 4, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='podpisu protokol u o rekl amaci. Reklamace týkající se tr ansakce platební kartou za zboží nebo služby \\nposkytnuté třetí stranou prostřednictvím internetu nebo přímo v obchodním místě musí být při nespolupráci'),\n",
       "  0.5465161787063055),\n",
       " (Document(metadata={'page': 4, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='neuplatníte reklamaci do 30 Obchodních dnů od doručení zpráv o zúčtování (výpisů), ve kterých byla nebo \\nměla být  reklamovaná transa kce uved ena, aniž by vám v tom bránily důvody hodné zvláštníh o zřetele,'),\n",
       "  0.5461360141327372)]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_rel_score_docs = vectordb.similarity_search_with_relevance_scores(query=question,k=3)\n",
    "sim_rel_score_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad7e48-39c2-4215-a5e8-4b0885bf15c0",
   "metadata": {},
   "source": [
    "Na poněkud odlišném principu funguje *max_marginal_relevance_search* (MMR). Tato metoda se snaží současně optimalizovat podobnost a diverzitu dokumentů. To v praxi znamená, že najde na základě cosinové podobnosti *fetch_k* fragmentů (defaultně 20). Poté je ale seřadí tak, že penalizuje podobné či dokonce identické dokumenty a z nového řazení vrátí *k* fragmentů (defaultně 4). Bohužel zdá se, že žádné číslo kvantifikující podobnost či kvalitu výběru není ve funkci dostupné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "cca412e6-7d5b-4706-88c2-04f91432ce6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 6, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='7/10 \\nVER DDT_PODMPKEV.PDF \\n \\nČlánek 8. Ztráta, odcizení, zadržení debetní karty v bankomatu \\n \\n8.1 Oznamovací povinnost Držitele . V případě ztráty, odcizení nebo zneuž ití debetní karty nebo mobilního'),\n",
       " Document(metadata={'page': 4, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='neuplatníte reklamaci do 30 Obchodních dnů od doručení zpráv o zúčtování (výpisů), ve kterých byla nebo \\nměla být  reklamovaná transa kce uved ena, aniž by vám v tom bránily důvody hodné zvláštníh o zřetele,'),\n",
       " Document(metadata={'page': 7, 'source': './pomocne_soubory/podminky_debetnich_karet.pdf'}, page_content='Secure hesla či přihlašovacích údajů v případě metody pro vytváření el ektronického podpisu, kterou jsme \\nvám vydali na základě Smlou vy o elektronickém podpisu, pro účely 3D Secure autorizace jiné osobě nebo')]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_relev_docs = vectordb.max_marginal_relevance_search(query=question,k=3, fetch_k=20)\n",
    "max_relev_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330fa64-5b93-4c4f-bf81-853b2855ef49",
   "metadata": {},
   "source": [
    "Specialitou je použití komprese na výstupních fragmentech. Fragmenty mohou totiž obsahovat značné množství informací pro určitý dotaz nerelevantních. Užitečné informace se potom v záplavě zbytečností snadno ztratí. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "b518dd81-b07d-45f6-a75d-da2e0aab1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2cd69-c087-418c-acfa-0b9d07057ef5",
   "metadata": {},
   "source": [
    "Abychom viděli, co Langchain dělá, zapneme debugování."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "6341e36f-38bc-4e47-b664-7c26668b4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d682b44-9ba4-445d-89a4-2da035b0508e",
   "metadata": {},
   "source": [
    "Samotnou kompresi (či přesněji osekávání zbytečných slov) realizuje fakticky jazykový model, musíme tudíž vytvoři jeho instanci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "91e75ed3-91c7-4625-a190-21855878f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4291967-b73b-43d7-bcf6-91d57194f4ce",
   "metadata": {},
   "source": [
    "Následně vytvoříme samotný kompresor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b5787b6f-8b6b-4019-9e0a-39d915cf79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d44e08-0ac3-4752-8979-957e1189a63e",
   "metadata": {},
   "source": [
    "Na něm provoláme metodu *get_relevant_documents*. Bohužel jsem nepřišel na to, jak specifikovat počet vrácených fragmentů (parametr *k* nefunguje), takže se vrací defaultní 4.  \n",
    "\"Kompresi\" zajišťuje následující prompt:\n",
    "```\n",
    "Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return NO_OUTPUT. \\n\\nRemember, *DO NOT* edit the extracted parts of the context.\\n\\n> Question: Jak dlouho debentní karta platí?\\n> Context:\\n>>>\\nplatíte  předem a  účtujeme  jej nejdříve 5.  obchodní den  po sjednání karty. V dalších měsících  po dobu \\nplatnosti karty  je cena splatná  ve stejný den . Pokud den splatnosti připa dne na den, který není Obchodním\\n>>>\\nExtracted relevant parts:\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\"Kompresi\" zajišťuje následující prompt:\n",
    "```"
   ],
   "id": "e182d2f96829eb84"
  },
  {
   "cell_type": "markdown",
   "id": "2898c662",
   "metadata": {},
   "source": [
    "Combinovani metod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0559811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "#%%\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query=question)\n",
    "#%%\n",
    "compressed_docs\n",
    "#%%\n",
    "langchain.debug = False\n",
    "#%% md\n",
    "Combinovani metod:\n",
    "#%%\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")\n",
    "#%%\n",
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "print(compressed_docs)\n",
    "#%% md\n",
    "### Potencialni problemy\n",
    "#%% md"
   ],
   "id": "fc420755458ea8b8"
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "c49d3595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MachineLearning-Lecture01.pdf']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"./source_notebooks/langchain/Chat With LLM/docs\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Load PDF\n",
    "\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data"
   ],
   "id": "6fbb3f5954ce1f5"
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "f5e51663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    PyPDFLoader(\"./source_notebooks/langchain/Chat With LLM/docs/MachineLearning-Lecture01.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "#%%\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "#%%\n",
    "splits = text_splitter.split_documents(docs)\n",
    "#%%\n",
    "len(splits)\n",
    "#%%\n",
    "persist_directory = './LangChain/Chat With LLM/docs/chroma/'\n",
    "\n",
    "!rm -rf ./LangChain/Chat With LLM/docs/chroma  # remove old database files if any\n",
    "#%%"
   ],
   "id": "8391e8bf58cc6b7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "#%%"
   ],
   "id": "ac4b24d1dc607ad3"
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "1a627a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \n",
      "refresher for those of you that want one.  \n",
      "Later in this quarter, we'll also use the discussion sections to go over extensions for the \n",
      "material that I'm teaching in the main lectures. So machine learning is a huge field, and \n",
      "there are a few extensions that we really want to teach but didn't have time in the main \n",
      "lectures for.\n"
     ]
    }
   ],
   "source": [
    "print(docs[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460e582",
   "metadata": {},
   "source": [
    "### Jine typy retrievalu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eacf0b",
   "metadata": {},
   "source": [
    "It's worth noting that vectordb as not the only kind of tool to retrieve documents. \n",
    "\n",
    "The `LangChain` retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "ec6dd358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader(\"./source_notebooks/langchain/Chat With LLM/docs/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "fa71f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "601e8fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"Testing, testing. Okay, cool. Thanks.   So all right, online resources. The class has a home page, so it's in on the handouts. I \\nwon't write on the chalkboard — http:// cs229.stanford.edu. And so when there are \\nhomework assignments or things like that, we usually won't sort of — in the mission of \\nsaving trees, we will usually not give out many handouts in class. So homework \\nassignments, homework solutions will be posted online at the course home page.  \\nAs far as this class, I've also written, and I guess I've also revised every year a set of \\nfairly detailed lecture notes that cover the technical content of this class. And so if you \\nvisit the course homepage, you'll also find the detailed lecture notes that go over in detail \\nall the math and equations and so on that I'll be doing in class.  \\nThere's also a newsgroup, su.class.cs229, also written on the handout. This is a \\nnewsgroup that's sort of a forum for people in the class to get to know each other and \\nhave whatever discussions you want to have amongst yourselves. So the class newsgroup \\nwill not be monitored by the TAs and me. But this is a place for you to form study groups \\nor find project partners or discuss homework problems and so on, and it's not monitored \\nby the TAs and me. So feel free to talk trash about this class there.  \\nIf you want to contact the teaching staff, please use the email address written down here, \\ncs229-qa@cs.stanford.edu. This goes to an account that's read by all the TAs and me. So\")"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "070cd0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"yourselves. You can also come and talk to me or the TAs if you want to brainstorm ideas \\nwith us.  \\nOkay. So one more organizational question. I'm curious, how many of you know \\nMATLAB? Wow, cool, quite a lot. Okay. So as part of the — act ually how many of you \\nknow Octave or have used Octave? Oh, okay, much smaller number.  \\nSo as part of this class, especially in the homeworks, we'll ask you to implement a few \\nprograms, a few machine learning algorithms as part of the homeworks. And most of  those homeworks will be done in either MATLAB or in Octave, which is sort of — I \\nknow some people call it a free version of MATLAB, which it sort of is, sort of isn't.  \\nSo I guess for those of you that haven't seen MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to \\nwrite codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your own home computer or something if you \\ndon't have a MATLAB license, for the purposes of this class, there's also — [inaudible] \\nwrite that down [inaudible] MATLAB — there' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than \\nMATLAB, but it's free, and for the purposes of this class, it will work for just about \\neverything.\")"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "docs_svm[0]\n",
    "#%%\n",
    "question = \"what did they say about matlab?\"\n",
    "\n",
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]\n",
    "#%% md\n",
    "## E. Question answering\n",
    "Uživatel asi nebude chtít, aby mu stroj na jeho otázku vyplivl více či méně souvislé fragmenty dokumentů. Bude si spíše přát odpověď v přirozeném jazyce. Této úloze se budeme věnovat nyní.  \n",
    "\n",
    "Nejprve si připravme věci, se kterými jsme se seznámili již v předchozík podkapitolách.\n",
    "#%%\n",
    "from langchain.vectorstores import Chroma"
   ],
   "id": "c3606fa6bde1c902"
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d7a5213a-0764-4d27-8538-d0fcc62721d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "cf74b26d-268f-470a-b3b1-05d5c4d16bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa5454-81ea-48f8-9f40-c9eefac5fa72",
   "metadata": {},
   "source": [
    "Otázku do něj vložíme takto:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "20e5ce67e11676d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=llm_model_name, temperature=0)"
   ],
   "id": "2f938723c0827857"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(vectordb._collection.count())",
   "id": "3371f9c60da4342c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)\n",
    "#%% md\n",
    "Následně si vytvoříme chain pro question answering - [RetrievalQA](https://python.langchain.com/docs/use_cases/question_answering/how_to/vector_db_qa).\n",
    "#%%\n",
    "from langchain.chains import RetrievalQA\n",
    "#%%\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "#%% md\n",
    "Otázku do něj vložíme takto:\n",
    "#%%"
   ],
   "id": "39550d4d83bc9c75"
  },
  {
   "cell_type": "markdown",
   "id": "cd09b547-0428-4dcd-ac0d-bbc450fd9f20",
   "metadata": {},
   "source": [
    "Ok, odpověď v přirozeném jazyce jsme dostali. Co když bychom ale chtěli upravit systémový prompt? A jak si zobrazíme fragmenty, ze kterých jazykový model skládal odpověď?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7626304-d182-4fb2-a013-b9e92865da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "qa_chain_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cefe70-d47b-4cac-b706-3d911e5450ef",
   "metadata": {},
   "source": [
    "Zdrojové fragmenty dostaneme díky parametru *return_source_documents* rovnému True. Šablonu se svého druhu systémovým promptem pak do chainu vložíme skrze parametr *chain_type_kwargs*. Ten obsahuje slovník, ve kterém musí být pro klíč \"prompt\" vložena námi specifikovaná šablona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "d03cd7c5-5d8a-44a7-ba72-8bdd0364e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": qa_chain_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9e188b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "some_context = \" Neco \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535390a-6a15-46c2-b709-0f97225fb256",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": question, \"context\": some_context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f6439491-e36d-423e-bcf6-5842071ff5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are major topics for this class?',\n",
       " 'result': 'The major topics for this class include human anatomy, physiology, and medical terminology. We will also cover common diseases and treatments related to the human body. Thanks for asking!',\n",
       " 'source_documents': []}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "6f291bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20891b",
   "metadata": {},
   "source": [
    "##### RetrievalQA chain types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd114c",
   "metadata": {},
   "source": [
    "V obecnosti je princip nasledujici: \n",
    "* Polozi se otazka\n",
    "* Podivame se do \"storu\" s chunkama a vybereme ty nejvic potrebny\n",
    "* Spolecne s otazkou vlozime tyhle chunky do LLM, aby nam vygeneroval smysluplnou odpoved.\n",
    "\n",
    "Tomuhle _chain_type_ se rika **stuff**, protoze se takrikajic vsechno nastuffuje do LLMka, at si s tim poradi. To ma samozrejme obvious nevyhody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    ...,\n",
    "    chain_type=\"stuff\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8a9be",
   "metadata": {},
   "source": [
    "Proto existuji alternativni metody.\n",
    "1. **map_reduce** \n",
    "   1. kazdy dokument zvlast posle do LLMka, spolecne s otazkou, aby se pokusil odpovedet na otazku. Hacek je v tom, ze pokud se odpoved naleza rozkrocena mezi nekolik dokumentu, tak tu odpoved retrieval nenajde. Ale vyhoda je, ze do LLMka jsme schopni takhle poslat libovolny pocet dokumentu.\n",
    "2. **refine** \n",
    "   1. taky se nekolikrat vola LLMko, ale: vezme se prvni dokument, spolecne s otazkou. Pak se vezme druhy dokument a prompt (vnitrne) se upravi, o nasledujici text: \"Mas moznost vylepsit nasledujici odpoved (pokud mozno) s nasledujicim kontextem.\". A to se dale opakuje o vsechny dokumenty v rade. \n",
    "3. **map_rerank**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1d3a66",
   "metadata": {},
   "source": [
    "![chain_type](./pomocne_soubory/chain_type.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118968e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"map_reduce\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f90be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"refine\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"map_rerank\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac2260",
   "metadata": {},
   "source": [
    "##### RetrievalQA limitations\n",
    " \n",
    "QA fails to preserve conversational history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23840e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907416f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614fade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"why are those prerequesites needed?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883c3c2",
   "metadata": {},
   "source": [
    "Note, The LLM response varies. Some responses **do** include a reference to probability which might be gleaned from referenced documents. The point is simply that the model does not have access to past questions or answers, this will be covered in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b79d9-1eb1-4a9c-868b-73ce24c78980",
   "metadata": {},
   "source": [
    "Určité parametry můžeme vložit i do *vectordb.as_retriever()*. Například pokud bychom chtěli přejít od defaultního similarity searche na max marginal relevance search, přidáme tam parametr *search_type* s hodnotou \"mmr\". Další paramtery už nejsou samostatné, nýbrž se nalézají ve slovníku v parametru *search_kwargs*. Může jít o počet vrácených fragmentů (parametr *k*), *fetch_k* pro MMR, *score_threshold* pro similarity search na vrácení dostatečně kvalitích fragmentů či v případě filtrování dle metadat parametr filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce977a-a183-43da-903e-6b7baf85161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 20}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": qa_chain_prompt}\n",
    ")\n",
    "result = qa_chain({\"query\": question})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6ea01",
   "metadata": {},
   "source": [
    "## F. Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce3183-cf64-4490-a107-c3728cd6a2bc",
   "metadata": {},
   "source": [
    "Pokud chceme mít chatbota, se kterým lze diskutovat déle než jednu dialogovou výměnu, musíme si vytvořit paměťový objekt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780c702-288f-4924-ba70-4607e4fc48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "conv_buff_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88fe7d-06ca-488c-85c7-f6a544437f14",
   "metadata": {},
   "source": [
    "Je třeba též použít jiný chain - *ConversationalRetrievalChain*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e645ba2-4cb0-44c9-afd3-b168b4a2848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915bda8-1de3-4e82-ba0e-60d0877d165b",
   "metadata": {},
   "source": [
    "Parametry jsou (krom přidání paměti) stejné jako v předchozím chainu. Nicméně v pozadí přibyl další krok, ve kterém chain vezme historii i novou uživatelovu otázku a zkondenzuje je do otázky přežvýkané. Tento krok má následující podobu: \n",
    "```\n",
    "[llm/start] [1:chain:ConversationalRetrievalChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
    "{\n",
    "  \"prompts\": [\n",
    "    \"Human: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: Jak dlouho debentní karta platí?\\nAssistant: Debetní karta platí do posledního dne měsíce a roku doby platnosti, která je uvedena na kartě.\\nFollow Up Input: Jak se to liší v případě, kdy máme kreditní kartu?\\nStandalone question:\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "díky čemuž vznikne otázka\n",
    "```\n",
    "\"Jak dlouho platí kreditní karta?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f424f86-d6a5-452d-bb71-91c24b669106",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    memory=conv_buff_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813aed2-9869-41e3-bae0-679c3f0d54f9",
   "metadata": {},
   "source": [
    "Pozn.: bacha, slovník vstupující do provolávání chainu musí mít klíč \"question\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d205400-d889-4038-b542-80b329772604",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is probability a class topic?\"\n",
    "result = memory_chain({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f2688-b584-4215-9085-f43cc68b61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077423e-498f-4000-b3a5-37e9c8b656d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"why are those prerequesites needed?\"\n",
    "result = memory_chain({\"question\": question_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a2301-42c9-4fb4-adc0-7ebcbe511c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd31842-ec17-4272-a881-263566598c6c",
   "metadata": {},
   "source": [
    "BTW odpověď je asi správně, ale v celém dokumentu o kreditních kartách nebyla ani zmínka.  \n",
    "Pozn.: pokud chceme vidět zdrojové dokumenty, musíme vložit do konstruktoru *ConversationalRetrievalChain* parametr *verbose* s hodnotou True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2607c-0549-4ef7-b464-5cdd50d6cf2d",
   "metadata": {},
   "source": [
    "### Automatické používání metadat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2463fd85",
   "metadata": {},
   "source": [
    "##### Vitkuv Priklad"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "result\n",
    "#%%\n",
    "question_2 = \"why are those prerequesites needed?\"\n",
    "result = memory_chain({\"question\": question_2})\n",
    "#%%\n",
    "result\n",
    "#%% md\n",
    "BTW odpověď je asi správně, ale v celém dokumentu o kreditních kartách nebyla ani zmínka.  \n",
    "\n",
    "Pozn.: pokud chceme vidět zdrojové dokumenty, musíme vložit do konstruktoru *ConversationalRetrievalChain* parametr *verbose* s hodnotou True.\n",
    "#%% md"
   ],
   "id": "d1fdb923a06e081e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d827c-4107-4bb1-892a-387afb48cfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 250\n",
    "chunk_overlap = 50\n",
    "\n",
    "rec_text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    ")\n",
    "\n",
    "rec_splits = rec_text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd928eb9-122b-4568-be95-90725a71ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "simple_doc_dir = \"embeddings\\\\two_cards\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84cf2e6-699b-4fd5-abed-5e69a425ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=rec_splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=simple_doc_dir\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=rec_splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=simple_doc_dir\n",
    ")\n",
    "vectordb.persist()"
   ],
   "id": "aaa4ddba9c76cdd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vectordb = Chroma(persist_directory=simple_doc_dir, embedding_function=embedding)",
   "id": "bc77490429634efc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "chat = ChatOpenAI(model_name=llm_model_name, temperature=0)\n",
    "#%%\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Answer in Czech language. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "qa_chain_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,"
   ],
   "id": "13f22b55a1384d05"
  },
  {
   "cell_type": "markdown",
   "id": "734a0759-653a-4a76-a932-985347923052",
   "metadata": {},
   "source": [
    "Pozn.: prosím o ignorování pole \"topic\" a \"product\" v metadatech - jedná se o produkt následujících kroků. zde jsou irrelevantní."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f326d8-c143-4677-a8f9-292d203cb755",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593283c-1e98-4514-ae4e-455574fd2444",
   "metadata": {},
   "source": [
    "Všimněte si, že i když byla odpověď asi správná, dostal se mezi nejvhodnější fragmenty i jeden fragment z pdfka o kartách kreditních.  \n",
    "Jaké je řešení? Inu, lze použít metadata. Pokud člověk pracuje s dokumenty s lehce zpracovatelným názvem (typu \"podminky_produkt_1.pdf\", \"podminky_produkt_2.pdf\") a má štěstí, nemusí přidávat žádná nová metadata. I v našem poměrně primitivním případě to ale moc nefungovalo. \n",
    "Musíme tedy data ve vektorové databázi obohatit o nové metadatové pole."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "credit_cards_ids = vectordb.get(where={\"source\": \"source_files\\\\Podminky-osobnich-kreditnich-karet.pdf\"})[\"ids\"]\n"
   ],
   "id": "9bc9677c0f4fc87a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97baedf9-b99d-492e-b5bb-1f01152e7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_metadata in debit_cards_metadatas:\n",
    "    one_metadata[\"product\"] = \"debentní karta\"\n",
    "\n",
    "for one_metadata in credit_cards_metadatas:\n",
    "    one_metadata[\"product\"] = \"kreditní karta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a209740-e2ff-4894-b6c1-aa068c2bd85a",
   "metadata": {},
   "source": [
    "Provedeme update a změny databáze uložíme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e7b0e-1708-4624-9fea-668018bf4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb._collection.update(ids=debit_cards_ids, metadatas=debit_cards_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1e14f-e9dc-45f1-91d8-e7961da476a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb._collection.update(ids=credit_cards_ids, metadatas=credit_cards_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6895e-82fa-40b2-94bb-4eaacf6b8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4b3c3-3c2b-46ed-8efa-a36726509bfc",
   "metadata": {},
   "source": [
    "Pro získání fragmentů dokumentů, u kterých už budou hrát roli metadata, musíme použít *SelfQueryRetriever*. Ten vyžaduje mít nainstalovaný balíček [lark](https://pypi.org/project/lark/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1eee2a-68e9-47ad-a71d-0465e89a301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eecc10-ff24-4628-adac-8d7c8a7ba822",
   "metadata": {},
   "source": [
    "Musíme mu mimo jiné podhodit popis metadat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df768a-b7b3-4acc-9472-4e52cc054b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"product\",\n",
    "        description=\"Product name\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The document source\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the document\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b156c6-50a2-4201-9bc5-ba8bb1ad36a8",
   "metadata": {},
   "source": [
    "Též do *SelfQueryRetriever* vložíme parametr *document_contents*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c759823-ea14-40ed-bc05-10ae212138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Bank products\"\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=chat,\n",
    "    vectorstore=vectordb,\n",
    "    document_contents=document_content_description,\n",
    "    metadata_field_info=metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a7b13-911b-4928-8721-fbc0b068442f",
   "metadata": {},
   "source": [
    "No a když jsem tohle všechno udělal, tak... to stejně nefungovalo :D. Možná jsem se špatně dotazoval, možná tenhle typ úlohy funguje snáze v angličtině. Zkusil jsem tak editovat soubor {jméno_vašeho_environmentu}\\Lib\\site-packages\\langchain\\chains\\query_constructor\\prompt.py, ve kterém je šablona za tvoření queriny do Chromy zodpovědná. No, snad jsem přitom nic nerozbil...   \n",
    "Změněné byly tyto části souboru:\n",
    "```\n",
    "BANK_DATA_SOURCE = \"\"\"\\\n",
    "```json\n",
    "{\n",
    "    \"content\": \"Informace k bankovnímu produktu\",\n",
    "    \"attributes\": {\n",
    "        \"produkt\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Název produktu\"\n",
    "        },\n",
    "        \"zamereni\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Zaměření dokumentu, jedna z možností \\\"ceník\\\", \\\"všeobecné informace\\\"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\\\n",
    "\"\"\".replace(\n",
    "    \"{\", \"{{\"\n",
    ").replace(\n",
    "    \"}\", \"}}\"\n",
    ")\n",
    "\n",
    "FULL_ANSWER = \"\"\"\\\n",
    "```json\n",
    "{{\n",
    "    \"query\": \"cena spotřebitelský úvěr\",\n",
    "    \"filter\": \"and(eq(\\\\\"produkt\\\\\", \\\\\"spotřebitelský úvěr\\\\\"), \\\n",
    "eq(\\\\\"zamereni\\\\\", \\\\\"ceník\\\\\"))\"\n",
    "}}\n",
    "```\\\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_EXAMPLES = [\n",
    "    {\n",
    "        \"i\": 1,\n",
    "        \"data_source\": BANK_DATA_SOURCE,\n",
    "        \"user_query\": \"Kolik stojí vyřízení spotřebitelského úvěru\",\n",
    "        \"structured_request\": FULL_ANSWER,\n",
    "    },\n",
    "    {\n",
    "        \"i\": 2,\n",
    "        \"data_source\": BANK_DATA_SOURCE,\n",
    "        \"user_query\": \"V kolik jede vlak?\",\n",
    "        \"structured_request\": NO_FILTER_ANSWER,\n",
    "    },\n",
    "]\n",
    "\n",
    "EXAMPLES_WITH_LIMIT = [\n",
    "    {\n",
    "        \"i\": 1,\n",
    "        \"data_source\": BANK_DATA_SOURCE,\n",
    "        \"user_query\": \"Kolik stojí vyřízení spotřebitelského úvěru\",\n",
    "        \"structured_request\": FULL_ANSWER,\n",
    "    },\n",
    "    {\n",
    "        \"i\": 2,\n",
    "        \"data_source\": BANK_DATA_SOURCE,\n",
    "        \"user_query\": \"V kolik jede vlak\",\n",
    "        \"structured_request\": NO_FILTER_ANSWER,\n",
    "    },\n",
    "    {\n",
    "        \"i\": 3,\n",
    "        \"data_source\": BANK_DATA_SOURCE,\n",
    "        \"user_query\": \"What are three songs about love\",\n",
    "        \"structured_request\": WITH_LIMIT_ANSWER,\n",
    "    },\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008aeaf2-9a92-45f9-b532-b7aa899d36d6",
   "metadata": {},
   "source": [
    "Pro dotaz \"Co musím udělat pro blokaci debentní karty?\" (popř. \"Co musím udělat pro blokaci debentní karty od KB?\") se vygeneruje\n",
    "```\n",
    "\"```json\\n{\\n    \\\"query\\\": \\\"blokace debentní karta\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\"\n",
    "```\n",
    "Tj. querina správná, ale filtr se neuplatnil.\n",
    "Při dotazu \"Co musím udělat pro blokaci produktu debentní karta?\" je už filtr správný\n",
    "```\n",
    "\"```json\\n{\\n    \\\"query\\\": \\\"blokace debentní karta\\\",\\n    \\\"filter\\\": \\\"eq(\\\\\\\"product\\\\\\\", \\\\\\\"debentní karta\\\\\\\")\\\"\\n}\\n```\"\n",
    "```\n",
    "Ale popravdě nedokážu si představit, že by v praxi uživatel používal takto pro model nápovědný dotaz.  \n",
    "Pro zajímavost - na dotaz \"Kolik budu platit u pojištění Merlin od KB?\" by vznikl filtr\n",
    "```\n",
    "\"```json\\n{\\n    \\\"query\\\": \\\"platit pojištění Merlin KB\\\",\\n    \\\"filter\\\": \\\"and(eq(\\\\\\\"product\\\\\\\", \\\\\\\"Merlin\\\\\\\"), eq(\\\\\\\"source\\\\\\\", \\\\\\\"KB\\\\\\\"))\\\"\\n}\\n```\"\n",
    "```\n",
    "Asi by to chtělo ten šablonový prompt ze souboru trochu vylepšit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa99d2e-b609-4d9b-aef3-a50224001f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Co musím udělat pro blokaci produktu debentní karta?\"\n",
    "result = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1095a-fa7d-4b3d-967a-9c0d74238ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af23c5",
   "metadata": {},
   "source": [
    "##### Originalni prikald z Coursery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b44144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"./LangChain/Chat With LLM/docs/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"./LangChain/Chat With LLM/docs/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"./LangChain/Chat With LLM/docs/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"./LangChain/Chat With LLM/docs/MachineLearning-Lecture03.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = './LangChain/Chat With LLM/docs/chroma/'\n",
    "!rm -rf ./LangChain/Chat With LLM/docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aed2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a764ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47175c",
   "metadata": {},
   "source": [
    "**Note:** The default model for `OpenAI` (\"from langchain.llms import OpenAI\") is `text-davinci-003`. Due to the deprication of OpenAI's model `text-davinci-003` on 4 January 2024, you'll be using OpenAI's recommended replacement model `gpt-3.5-turbo-instruct` instead."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\","
   ],
   "id": "2de774e86ada0811"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc0e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2634be3-73c0-4007-8739-037d275a0c6a",
   "metadata": {},
   "source": [
    "### Prefix před fragmenty\n",
    "Co se týče problému nalezení fragmentu ke správné problematice, lze na internetu nalézt doporučení, aby se jméno problematiky vložilo na začátek každého fragmentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5f7d0-6a5f-4c80-ac7e-1becf9764493",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 250\n",
    "chunk_overlap = 50\n",
    "\n",
    "rec_text_splitter = RecursiveCharacterTextSplitter(        \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = chunk_overlap,\n",
    ")\n",
    "\n",
    "loader_1 = PyPDFLoader(\"source_files\\\\podminky_debetnich_karet.pdf\")\n",
    "pages_1 = loader_1.load()\n",
    "rec_splits_1 = rec_text_splitter.split_documents(pages_1)\n",
    "for one_doc in rec_splits_1:\n",
    "    one_doc.page_content = \"debentní karta \\t \" + one_doc.page_content\n",
    "\n",
    "\n",
    "loader_2 = PyPDFLoader(\"source_files\\\\Podminky-osobnich-kreditnich-karet.pdf\")\n",
    "pages_2 = loader_2.load()\n",
    "rec_splits_2 = rec_text_splitter.split_documents(pages_2)\n",
    "for one_doc in rec_splits_2:\n",
    "    one_doc.page_content = \"kreditní karta \\t \" + one_doc.page_content\n",
    "\n",
    "all_splits = rec_splits_1 + rec_splits_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b715121f-a854-4b93-9a61-cf7d4b9a8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "doc_dir = \"embeddings\\\\fragment_prefix\\\\\"\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=doc_dir\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780c816-81f6-43c3-8ba1-20a5b36b113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name=llm_model_name, temperature=0)\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Answer in Czech language. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "qa_chain_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": qa_chain_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a79a28d-f268-4308-8770-abeca9ff593f",
   "metadata": {},
   "source": [
    "Zdá se, že funkčnost takového přístupu je omezená, nicméně pro úplné zatracení či naopak vychválení do nebes by to chtělo větší otestování."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94e76f-65a1-4b67-8955-836ccfb29932",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"Co musím udělat pro blokaci debentní karty?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e34f32-0887-4d4f-b43f-6d02c9703ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"Co dělat s kreditkou zaseklou v bankomatu?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342d226-193d-4774-a81b-8ea9f3cf012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\": \"Co dělat s debetní kartou zaseklou v bankomatu?\"})\n",
    "result"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory \n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7, model_name=llm_model_name)\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are Sauron from 'Lord of the Ring' novel.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")"
   ],
   "id": "9745eb23cc6c221"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conversation = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=window_memory\n",
    ")\n",
    "#%%\n",
    "conversation.invoke(input=\"Hi, who are you?\")\n",
    "#%%\n",
    "conversation.invoke(input=\"I have One ring, but I will never give it to you!\")\n",
    "#%% md\n",
    "### Q&A chatbot\n",
    "Skript načítací záznamy do databáze se nachází níže. Možná někoho zarazí metoda listu *extend*. Ta na konec svého mateřského listu vloží všechny elementy iterable objektu (např. listu) ze svého argumentu. Tj. například pro mateřský list [1, 2, 3] a dodatečný list [4, 5] vede extend na [1, 2, 3, 4, 5], zatímco append by skončil s [1, 2, 3, [4, 5]]. No a toto je chtění chování, když si uvědomíme, že *loader.load()* nám dá list Document objektů, které odpovídají jednotlivým stránkám jednoho pdfka.\n",
    "#%%\n",
    "import os\n",
    "import dotenv\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"source_files\\\\penzijni-plan-1-1-16.pdf\"),\n",
    "    PyPDFLoader(\"source_files\\\\penzijni-plan-3-1-16.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 100\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"ČLÁNEK\", \"\\n\\n\", \"\\n\", \".\"]\n",
    ")\n",
    "\n",
    "splits = r_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = \"embeddings\\\\chroma_persist_dir\\\\\"\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "#%% md\n",
    "Skript realizující dotazování\n",
    "#%%\n",
    "import os\n",
    "import dotenv\n",
    "import openai\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "persist_directory = \"embeddings\\\\chroma_persist_dir\\\\\"\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum. Keep the answer as concise as possible. Answer in Czech language.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful answer:\"\"\"\n"
   ],
   "id": "cd640887e873260d"
  },
  {
   "cell_type": "markdown",
   "id": "e4bc91b1-4822-4982-8517-c2c21d57ca83",
   "metadata": {},
   "source": [
    "Skript realizující dotazování"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd61480-0f12-418b-88d8-d7f7aa2fd67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import openai\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "persist_directory = \"embeddings\\\\chroma_persist_dir\\\\\"\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum. Keep the answer as concise as possible. Answer in Czech language.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful answer:\"\"\"\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0, model_name=llm_model_name) \n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_message=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "qa_chain_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": qa_chain_prompt}\n",
    ")\n",
    "\n",
    "while True:\n",
    "    question = input(\"Napiš otázku (exit pro opuštění chatu):\\n\")\n",
    "    if question == \"exit\":\n",
    "        break \n",
    "    result = qa_chain.invoke({\"query\":question})\n",
    "    print(result[\"result\"])\n",
    "    print()\n",
    "    print(\"Zdrojové dokumenty:\")\n",
    "    for one_source_doc in result[\"source_documents\"]:\n",
    "        print(\"--------------------------\")\n",
    "        print(\"    \", one_source_doc)\n",
    "        print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a91fab",
   "metadata": {},
   "source": [
    "#### Coursera Chatbot vcetne GUIcka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a62f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1da441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(file, chain_type, k):\n",
    "    # load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    # split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    # define embedding\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    # define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    # create a chatbot chain. Memory is managed externally.\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=llm_name, temperature=0), \n",
    "        chain_type=chain_type, \n",
    "        retriever=retriever, \n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query  = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self,  **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.loaded_file = \"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"\n",
    "        self.qa = load_db(self.loaded_file,\"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:  # init or no file specified :\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        else:\n",
    "            file_input.save(\"temp.pdf\")  # local copy\n",
    "            self.loaded_file = file_input.filename\n",
    "            button_load.button_style=\"outline\"\n",
    "            self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "            button_load.button_style=\"solid\"\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "\n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend([(query, result[\"answer\"])])\n",
    "        self.db_query = result[\"generated_question\"]\n",
    "        self.db_response = result[\"source_documents\"]\n",
    "        self.answer = result['answer'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        inp.value = ''  #clears loading indicator when cleared\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "\n",
    "    @param.depends('db_query ', )\n",
    "    def get_lquest(self):\n",
    "        if not self.db_query :\n",
    "            return pn.Column(\n",
    "                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n",
    "                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n",
    "            )\n",
    "        return pn.Column(\n",
    "            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n",
    "            pn.pane.Str(self.db_query )\n",
    "        )\n",
    "\n",
    "    @param.depends('db_response', )\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return \n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for doc in self.db_response:\n",
    "            rlist.append(pn.Row(pn.pane.Str(doc)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    @param.depends('convchain', 'clr_history') \n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n",
    "        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n",
    "        for exchange in self.chat_history:\n",
    "            rlist.append(pn.Row(pn.pane.Str(exchange)))\n",
    "        return pn.WidgetBox(*rlist, width=600, scroll=True)\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "jpg_pane = pn.pane.Image( './img/convchain.jpg')\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=300),\n",
    "\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "tab2= pn.Column("
   ],
   "id": "2616aba8fa5393c1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce250221",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./pomocne_soubory/OutdoorClothingCatalog_1000.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdc005",
   "metadata": {},
   "source": [
    "### Create our QandA application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d27f6d5",
   "metadata": {},
   "source": [
    "Zacneme tim, ze si vytvorime RAG aplikaci pro dotazovani nad csv dokumentem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cac472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    ")\n",
    "dashboard\n",
    "#%% md\n",
    "## G. Evaluace\n",
    "#%% md\n",
    "LangChain platfroma obsahuje i nastroje pro debugging a evaluaci. Nicmene, jeden z benefitu LLM je, ze se da pouzit i pro evaluaci.\n",
    "#%%\n",
    "file = \"./pomocne_soubory/OutdoorClothingCatalog_1000.csv\"\n"
   ],
   "id": "caf4ecc82b11b2a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Create our QandA application\n",
    "#%% md"
   ],
   "id": "3bc80d0587e35ca3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d1a40",
   "metadata": {},
   "source": [
    "### Hard-coded examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
    "        have side pockets?\",\n",
    "        \"answer\": \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What collection is the Ultra-Lofty \\\n",
    "        850 Stretch Down Hooded Jacket from?\",\n",
    "        \"answer\": \"The DownTek collection\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154c2e1",
   "metadata": {},
   "source": [
    "### LLM-Generated examples"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "llm = ChatOpenAI(temperature = 0.0, model=llm_model_name)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=index.vectorstore.as_retriever(), \n"
   ],
   "id": "e278401e3df7838b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e61669",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b22459",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8912a",
   "metadata": {},
   "source": [
    "### Combine examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples += new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(examples[0][\"query\"])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "examples = [\n",
    "    {\n",
    "\n",
    "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
    "        have side pockets?\","
   ],
   "id": "3fbf3709ee00747c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    },\n",
    "    {\n",
    "        \"query\": \"What collection is the Ultra-Lofty \\\n",
    "\n",
    "        850 Stretch Down Hooded Jacket from?\",\n",
    "        \"answer\": \"The DownTek collection\"\n",
    "    }\n",
    "]"
   ],
   "id": "b5bab256af353bd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850174a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4967a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model_name)\n",
    "eval_chain = QAEvalChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ce492",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_outputs = eval_chain.evaluate(examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7764abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3b63c",
   "metadata": {},
   "source": [
    "### LangChain evaluation platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ea6e1",
   "metadata": {},
   "source": [
    "The LangChain evaluation platform, LangChain Plus, can be accessed here https://www.langchain.plus/.  \n",
    "Use the invite code `lang_learners_2023`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21ff6c",
   "metadata": {},
   "source": [
    "## LangChain Agents"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()\n",
    "#%%\n",
    "graded_outputs[0]\n",
    "#%% md"
   ],
   "id": "6ff9006074f37c81"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e20907",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185fc8e",
   "metadata": {},
   "source": [
    "Dva tooly, ktery loadujeme jsou pro pouziti kalkulacky v ramci jazykoveho modelu a druhy je Wikipedia, coz nam umoznuje volat Wikipedii primo z jazykoveho modelu."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "\n",
    "from langchain.python import PythonREPL\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ],
   "id": "83905f4276686783"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Temperature nastavujeme na 0, protoze to chceme pouzit jako reasoning agenta.\n",
    "#%%"
   ],
   "id": "d52ab11a3fc5d087"
  },
  {
   "cell_type": "markdown",
   "id": "9f82f12b",
   "metadata": {},
   "source": [
    "Tenhle priklad zavola action \"calculator\" a posle mu dve cisla, na ktere ma provest operaci."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2460ee00",
   "metadata": {},
   "source": [
    "#### Wikipedia example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cca944",
   "metadata": {},
   "source": [
    "Naopak, tenhle priklad zavola action \"wikipedia\" a posle mu dotaz, na ktery ma odpovedet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "result = agent(question) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c0fb6",
   "metadata": {},
   "source": [
    "#### Python Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bc5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bb369",
   "metadata": {},
   "source": [
    "\"PythonREPLTool\" je takovy jupyter notebook, ktery bezi v jazykovem modelu. Tady se muze hodit, kdyz chceme nejaky kod spustit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_list = [[\"Harrison\", \"Chase\"], \n",
    "                 [\"Lang\", \"Chain\"],\n",
    "                 [\"Dolly\", \"Too\"],\n",
    "                 [\"Elle\", \"Elem\"], \n",
    "                 [\"Geoff\",\"Fusion\"], \n",
    "                 [\"Trance\",\"Former\"],\n",
    "                 [\"Jen\",\"Ayai\"]\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de47407",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Python Agent\n",
    "#%%\n",
    "agent = create_python_agent(\n",
    "\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "#%% md"
   ],
   "id": "4869f887c5e629f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3203021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39faf5c0",
   "metadata": {},
   "source": [
    "Pro psani vlastnich toolu je dulezity, aby obsahovali podrobou dokumentaci. Tady je priklad, jak by takova dokumentace mohla vypadat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def time(text: str) -> str:\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "    tools + [time], \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ded274",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "\n",
    "The agent will sometimes come to the wrong conclusion (agents are a work in progress!). \n",
    "\n",
    "If it does, please try running it again."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#%%\n",
    "@tool\n",
    "def time(text: str) -> str:\n",
    "\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\"
   ],
   "id": "d9591043e37642ea"
  },
  {
   "cell_type": "markdown",
   "id": "61db396f",
   "metadata": {},
   "source": [
    "## Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe71c30",
   "metadata": {},
   "source": [
    "Nova functionalita LangChainu, ktera umoznuje predavat LLM modelu nejen text, ale i nejake funkce, ktere ma vykonat. Tady se podivame na nekolik prikladu, jak toho dosahnout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d453e5",
   "metadata": {},
   "source": [
    "Zadefinujeme si novou vlastni funkci, ktera bude vracet pocasi. OpenAI pridala novou API, _functions_, ktera umoznuje volat nejake funkce primo z LLM modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "247e5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ec8d7",
   "metadata": {},
   "source": [
    "Description v obou pripadech definice nize je velmi velmi dulezita. Ta totiz umoznuje LLM rozhodnout, zda a kdy volat nasi funkci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfd7f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47c5b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c373ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=llm_model_name,\n",
    "    functions=functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c8f97e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Au61QjuKqYwK3i2GjJeSOvZmZUJyA', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=FunctionCall(arguments='{\"location\":\"Boston, MA\"}', name='get_current_weather'), tool_calls=None))], created=1737932880, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_72ed7ab54c', usage=CompletionUsage(completion_tokens=18, prompt_tokens=79, total_tokens=97, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n"
   ],
   "id": "72dda5a10e72f807"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },"
   ],
   "id": "ac4e72c0876a079e"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efbcf99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": {\"location\": \"Boston, MA\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4f89a",
   "metadata": {},
   "source": [
    "Je treba rict, ze LLM za nas tu funkci volat nebude. To musime udelat my. Ale LLM nam pomuze s tim, ze nam rekne, zda (a pripadne kterou, v pripade vice funkci) bychom ji mohli volat. A s jakyma parametrama. Nicmene, je treba dodat, ze popisy funkci se pocitaji do context limitu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "def7151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfedaedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=llm_model_name,\n",
    "    functions=functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f26ec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Au622wQlTKGiVEa2d9KkIXkl9gMwo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1737932918, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_72ed7ab54c', usage=CompletionUsage(completion_tokens=11, prompt_tokens=74, total_tokens=85, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220deb1b",
   "metadata": {},
   "source": [
    "Jak vidno, kdyz do LLM vlozime vetu, ktera nevyzaduje pocasi, tak LLM nevraci parametr _fuction_call_. Tohle lze osetrit parametrem, ktery bud nakaze nebo zakaze volani funkci.\n",
    "\n",
    "Jedna se o parametr **function_call**, jehoz defaultni hodnota je \"auto\". Naopak, pokud chceme, aby LLM nevolal funkci, musime tento parametr nastavit na \"none\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b90e2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Au62G4DTSNboYk6bRcM0Tne5lfLHk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1737932932, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_72ed7ab54c', usage=CompletionUsage(completion_tokens=11, prompt_tokens=74, total_tokens=85, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=llm_model_name,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f943453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Au62HmPVClJnr3eTRvJStde7XzmJq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1737932933, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_bd83329f63', usage=CompletionUsage(completion_tokens=10, prompt_tokens=75, total_tokens=85, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=llm_model_name,\n",
    "    functions=functions,\n",
    "    function_call=\"none\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cea24d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Au62JqISHQNbOn4c2YAHZscMdtQUw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Which unit of temperature would you like the weather to be reported in: Celsius or Fahrenheit?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1737932935, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_bd83329f63', usage=CompletionUsage(completion_tokens=19, prompt_tokens=79, total_tokens=98, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather in Boston?\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=llm_model_name,\n",
    "    functions=functions,\n",
    "    function_call=\"none\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b451d",
   "metadata": {},
   "source": [
    "Pokud chceme LLM donuti, aby volal funkci, musime tento parametr nastavit na dictionary se jmenem funkce, kterou ma volat.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ab98254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Au62LqcHABBMF9N2HEjLwYWsvr6vz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=FunctionCall(arguments='{\"location\":\"New York, NY\"}', name='get_current_weather'), tool_calls=None))], created=1737932937, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_72ed7ab54c', usage=CompletionUsage(completion_tokens=9, prompt_tokens=84, total_tokens=93, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=llm_model_name,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde1c8e",
   "metadata": {},
   "source": [
    "Nasledujici priklad ukazuje, jak se to da pouzit v praxi. Zavolame LLM, to nam vrati parametry, ktery vlozime do funkce a vysledek opet vracime do LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66ecfeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Au62MIpWm9z0eExG9u5LLbFcxfhXU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=FunctionCall(arguments='{\"location\":\"Boston, MA\"}', name='get_current_weather'), tool_calls=None))], created=1737932938, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_bd83329f63', usage=CompletionUsage(completion_tokens=8, prompt_tokens=89, total_tokens=97, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=llm_model_name,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e871ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = json.loads(response_message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b310334b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChatCompletionMessage(content=None, refusal=None, role=\\'assistant\\', audio=None, function_call=FunctionCall(arguments=\\'{\"location\":\"Boston, MA\"}\\', name=\\'get_current_weather\\'), tool_calls=None)'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "db115cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6643167",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = json.loads(response_message.function_call.arguments)\n",
    "observation = get_current_weather(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f17e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"content\": observation,\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "193aa4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Au62PBRvCzkd4B7e51n7q6kPhDzdI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The weather in Boston is currently 72°F and sunny, with some windy conditions. Enjoy your day!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1737932941, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_bd83329f63', usage=CompletionUsage(completion_tokens=22, prompt_tokens=76, total_tokens=98, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=llm_model_name\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f9293d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Boston is currently 72°F and sunny, with some windy conditions. Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa7d09",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8950f2b",
   "metadata": {},
   "source": [
    "LangChain zavadi novou jazykovou konstrukci, ktera ma zjednodusit volani dilcich casti langchainu. Dokonce to pokrocilo tak, ze predchozi kapitoly nahore jsou jiz deprecated. Tahle konstrukce se jmenuje _runnable_. Pouziva se pomoci pipy. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#%%\n",
    "args = json.loads(response_message.function_call.arguments)\n",
    "observation = get_current_weather(args)\n",
    "#%%\n",
    "messages.append(\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": \"get_current_weather\",\n",
    "\n",
    "            \"content\": observation,\n",
    "        }\n",
    ")\n",
    "#%%\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=llm_model_name\n",
    ")\n",
    "print(response)\n",
    "#%%"
   ],
   "id": "e7d5c3d0b7722356"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a937be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca33c65",
   "metadata": {},
   "source": [
    "Nasledujici import je zakladni kamen LCEL, resp. chainu.\n",
    "* ChatPromptTemplate - jasny\n",
    "* ChatOpenAI - jasny\n",
    "* StrOutputParser - predela odpoved z modelu na string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fdad121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f00f0d",
   "metadata": {},
   "source": [
    "### Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a79881a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f954d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb12215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the bear break up with his girlfriend? \\n\\nBecause she was unbearable!'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%% md\n",
    "Nasledujici import je zakladni kamen LCEL, resp. chainu.\n",
    "* ChatPromptTemplate - jasny\n",
    "* ChatOpenAI - jasny\n",
    "* StrOutputParser - predela odpoved z modelu na string\n",
    "#%%\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ],
   "id": "8f0b0af4396d5512"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Simple Chain",
   "id": "5bebb638fb4d85fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "#%%\n",
    "chain = prompt | model | output_parser"
   ],
   "id": "a6ce5eb14efc7c41"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fd3c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e643a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "#%%\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "2c4cf435a2256640"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "retriever.get_relevant_documents(\"where did harrison work?\")",
   "id": "8dfc38af6bbe3672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "retriever.get_relevant_documents(\"what do bears like to eat\")",
   "id": "5ea351c666ae9859"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ],
   "id": "2fd75a1488e957dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain.schema.runnable import RunnableMap",
   "id": "f8ce156e94e4e284"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser\n",
    "#%%\n",
    "chain.invoke({\"question\": \"where did harrison work?\"})\n",
    "#%%\n",
    "inputs = RunnableMap({"
   ],
   "id": "14953d5cb148dba1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})\n",
    "#%%\n",
    "inputs.invoke({\"question\": \"where did harrison work?\"})\n",
    "#%% md\n",
    "### Bind\n",
    "\n",
    "an OpenAI Functions\n",
    "#%%\n",
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "#%%\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "# normalne bysme zavolali jen ChatOpenAI(), ale protoze chceme pouzit funkce, pridame jeste .bind() metodu"
   ],
   "id": "3f6d3abb070afb36"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b1c7227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'weather_search'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 64, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-42cfbeda-b281-4eaf-b979-7f07faf4df7f-0', usage_metadata={'input_tokens': 64, 'output_tokens': 17, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"what is the weather in sf\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3732a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "38d50d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30edf1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\","
   ],
   "id": "a69a30b9990cce5c"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebd6ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b2ef8",
   "metadata": {},
   "source": [
    "**Note**: Due to the deprecation of OpenAI's model `text-davinci-001` on 4 January 2024, you'll be using OpenAI's recommended replacement model `gpt-3.5-turbo-instruct` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aff8d7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_66226/2498289737.py:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  simple_model = OpenAI(\n"
     ]
    }
   ],
   "source": [
    "simple_model = OpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1000, \n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "simple_chain = simple_model | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a4dde1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f93bd7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n{\\n    \"title\": \"Autumn Leaves\",\\n    \"author\": \"Emily Dickinson\",\\n    \"first_line\": \"The leaves are falling, one by one\"\\n}\\n\\n{\\n    \"title\": \"The Ocean\\'s Song\",\\n    \"author\": \"Pablo Neruda\",\\n    \"first_line\": \"I hear the ocean\\'s song, a symphony of waves\"\\n}\\n\\n{\\n    \"title\": \"A Winter\\'s Night\",\\n    \"author\": \"Robert Frost\",\\n    \"first_line\": \"The snow falls softly, covering the ground\"\\n}'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.invoke(challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9782a",
   "metadata": {},
   "source": [
    "**Note**: The next line is expected to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "59d83e96",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 9 column 1 (char 125)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJSONDecodeError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[93], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43msimple_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchallenge\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:3022\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3020\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   3021\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3022\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3023\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3024\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:4711\u001B[0m, in \u001B[0;36mRunnableLambda.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   4697\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001B[39;00m\n\u001B[1;32m   4698\u001B[0m \n\u001B[1;32m   4699\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4708\u001B[0m \u001B[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001B[39;00m\n\u001B[1;32m   4709\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 4711\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_with_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4712\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_invoke\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4713\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4714\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4715\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4716\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4717\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   4718\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   4719\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke a coroutine function synchronously.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4720\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse `ainvoke` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   4721\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:1925\u001B[0m, in \u001B[0;36mRunnable._call_with_config\u001B[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001B[0m\n\u001B[1;32m   1921\u001B[0m     context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[1;32m   1922\u001B[0m     context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, child_config)\n\u001B[1;32m   1923\u001B[0m     output \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m   1924\u001B[0m         Output,\n\u001B[0;32m-> 1925\u001B[0m         \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1926\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1927\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1928\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1929\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1930\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1931\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1932\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1933\u001B[0m     )\n\u001B[1;32m   1934\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1935\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/config.py:396\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    395\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:4565\u001B[0m, in \u001B[0;36mRunnableLambda._invoke\u001B[0;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[1;32m   4563\u001B[0m                 output \u001B[38;5;241m=\u001B[39m chunk\n\u001B[1;32m   4564\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4565\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4566\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m   4567\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4568\u001B[0m \u001B[38;5;66;03m# If the output is a Runnable, invoke it\u001B[39;00m\n\u001B[1;32m   4569\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, Runnable):\n",
      "File \u001B[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/config.py:396\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    395\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/langchain/lib/python3.10/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n",
      "File \u001B[0;32m~/miniconda3/envs/langchain/lib/python3.10/json/decoder.py:340\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    338\u001B[0m end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n\u001B[1;32m    339\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n\u001B[0;32m--> 340\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExtra data\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, end)\n\u001B[1;32m    341\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "\u001B[0;31mJSONDecodeError\u001B[0m: Extra data: line 9 column 1 (char 125)"
     ]
    }
   ],
   "source": [
    "simple_chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d0dfe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc7a2d7aed4975db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.llms import OpenAI\n",
    "import json"
   ],
   "id": "2405628e9b74fc54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Note**: Due to the deprecation of OpenAI's model `text-davinci-001` on 4 January 2024, you'll be using OpenAI's recommended replacement model `gpt-3.5-turbo-instruct` instead.\n",
    "#%%\n",
    "simple_model = OpenAI("
   ],
   "id": "e8c45dff2f18b6aa"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3a6f9811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the bear bring a flashlight to the party?\\nBecause he heard it was going to be a \"beary\" good time!'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "62f8596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the bear join the band? \\n\\nBecause he had the best bearitone voice!',\n",
       " 'Why did the frog take the bus to work? Because his car got toad away!']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "684654b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " did\n",
      " the\n",
      " bear\n",
      " wear\n",
      " a\n",
      " fur\n",
      " coat\n",
      " to\n",
      " the\n",
      " party\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " he\n",
      " didn\n",
      "'t\n",
      " want\n",
      " to\n",
      " be\n",
      " under\n",
      "d\n",
      "ressed\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9c2e265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the bear break up with his girlfriend? \\n\\nBecause she couldn't bear his bad puns!\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8403d89",
   "metadata": {},
   "source": [
    "## Kompletace znalosti z predchozich kapitol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7a8fd",
   "metadata": {},
   "source": [
    "### Pydantic Syntax\n",
    "\n",
    "Pydantic je velmi podobna standardni Pythonovske knihovne dataclasses, ale pridava na validaci dat = tim se mysli, ze se kontroluje datovy typ, ktery do classy vkladame. Viz priklady nize. \n",
    "\n",
    "Dokumentace k Pydantic je [zde](https://docs.pydantic.dev/latest/concepts/models/)\n",
    "\n",
    "Dokumentace k Python Dataclasses je [zde](https://realpython.com/python-data-classes/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "35b49ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self, name: str, age: int, email: str):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.email = email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b34f7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = User(name=\"Joe\",age=32, email=\"joe@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "59ab402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joe'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "92f23b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = User(name=\"Joe\",age=\"bar\", email=\"joe@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c23b1abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bar'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "695eee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pUser(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bc06f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo_p = pUser(name=\"Jane\", age=32, email=\"jane@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4911064e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jane'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo_p.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bc413",
   "metadata": {},
   "source": [
    "**Note**: The next cell is expected to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "75f01509",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for pUser\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bar', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/int_parsing",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[112], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m foo_p \u001B[38;5;241m=\u001B[39m \u001B[43mpUser\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mJane\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbar\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43memail\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjane@gmail.com\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/pydantic/main.py:214\u001B[0m, in \u001B[0;36mBaseModel.__init__\u001B[0;34m(self, **data)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001B[39;00m\n\u001B[1;32m    213\u001B[0m __tracebackhide__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 214\u001B[0m validated_self \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__pydantic_validator__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mself_instance\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m validated_self:\n\u001B[1;32m    216\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    217\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA custom validator is returning a value other than `self`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReturning anything other than `self` from a top level model validator isn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt supported when validating via `__init__`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    220\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m    221\u001B[0m     )\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for pUser\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bar', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/int_parsing"
     ]
    }
   ],
   "source": [
    "foo_p = pUser(name=\"Jane\", age=\"bar\", email=\"jane@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c96b5a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Class(BaseModel):\n",
    "    students: List[pUser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1205ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Class(\n",
    "    students=[pUser(name=\"Jane\", age=32, email=\"jane@gmail.com\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5309e384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class(students=[pUser(name='Jane', age=32, email='jane@gmail.com')])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f2243",
   "metadata": {},
   "source": [
    "### Pydantic to OpenAI function definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "37debf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherSearch(BaseModel):\n",
    "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
    "    airport_code: str = Field(description=\"airport code to get weather for\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d202ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f0227bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_function = convert_pydantic_to_openai_function(WeatherSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "06d9bbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch',\n",
       " 'description': 'Call this with an airport code to get the weather at that airport',\n",
       " 'parameters': {'properties': {'airport_code': {'description': 'airport code to get weather for',\n",
       "    'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "001f3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherSearch1(BaseModel):\n",
    "    airport_code: str = Field(description=\"airport code to get weather for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91bbf8",
   "metadata": {},
   "source": [
    "**Note**: The next cell is expected to generate an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "30fefa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch1',\n",
       " 'description': '',\n",
       " 'parameters': {'properties': {'airport_code': {'description': 'airport code to get weather for',\n",
       "    'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(WeatherSearch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e97a1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherSearch2(BaseModel):\n",
    "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
    "    airport_code: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "661c3f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch2',\n",
       " 'description': 'Call this with an airport code to get the weather at that airport',\n",
       " 'parameters': {'properties': {'airport_code': {'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(WeatherSearch2)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    airport_code: str = Field(description=\"airport code to get weather for\")\n",
    "#%%"
   ],
   "id": "db3adff2544307fa"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "423fd73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 69, 'total_tokens': 87, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-afa2f864-5aa3-4901-83af-6aa30c7c89c6-0', usage_metadata={'input_tokens': 69, 'output_tokens': 18, 'total_tokens': 87, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_function.invoke(\"what is the weather in sf?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ce85e",
   "metadata": {},
   "source": [
    "### Forcing it to use a function\n",
    "\n",
    "We can force the model to use a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1319e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9c7b2585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 79, 'total_tokens': 87, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-17151073-6b6c-46f0-9350-854f00cb41fb-0', usage_metadata={'input_tokens': 79, 'output_tokens': 8, 'total_tokens': 87, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_forced_function.invoke(\"what is the weather in sf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ca8a90e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 74, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2138c535-cee7-4069-9d43-66d12076423e-0', usage_metadata={'input_tokens': 74, 'output_tokens': 8, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_forced_function.invoke(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9940a9",
   "metadata": {},
   "source": [
    "### Using in a chain\n",
    "\n",
    "We can use this model bound to function in a chain as we normally would"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "76c60426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9009a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9d0927e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model_with_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "19410e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 75, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-d16ea0c5-263e-4152-a5e5-8cead4e806be-0', usage_metadata={'input_tokens': 75, 'output_tokens': 18, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is the weather in sf?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05087a32",
   "metadata": {},
   "source": [
    "### Using multiple functions\n",
    "\n",
    "Even better, we can pass a set of function and let the LLM decide which to use based on the question context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "207e0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtistSearch(BaseModel):\n",
    "    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n",
    "    artist_name: str = Field(description=\"name of artist to look up\")\n",
    "    n: int = Field(description=\"number of results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d963501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    convert_pydantic_to_openai_function(WeatherSearch),\n",
    "    convert_pydantic_to_openai_function(ArtistSearch),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8639b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_functions = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "430a99a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 116, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-d8927dfb-01d2-4cd6-a82a-86db214a3685-0', usage_metadata={'input_tokens': 116, 'output_tokens': 18, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_functions.invoke(\"what is the weather in sf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fd950f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"artist_name\":\"Taylor Swift\",\"n\":3}', 'name': 'ArtistSearch'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 118, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-2f1d351d-0097-4056-8b9b-987adc80e04c-0', usage_metadata={'input_tokens': 118, 'output_tokens': 22, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_functions.invoke(\"what are three songs by taylor swift?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4175ab8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 111, 'total_tokens': 122, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-772e5ad4-b9fe-4e1a-b768-4d9f2745b9ad-0', usage_metadata={'input_tokens': 111, 'output_tokens': 11, 'total_tokens': 122, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_functions.invoke(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7fbbf",
   "metadata": {},
   "source": [
    "## Tagging and Extraction Using OpenAI functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78360081",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e29ce",
   "metadata": {},
   "source": [
    "Ackoli jsme si rekli, ze LLM nevola funkce, ktere mu zadame, je tu urcita trida funkci, ktere to dela za nas. Specificky jde o funkce delajici Tagging a Extraction. Tyhle funkce nemusime definovat, staci LLM zadat jen strkturu, jakou chceme, a on je za nas zavola. Neplati to samozrejme o jinych funkcich, napriklad funkce na pocasi, kterou jsme definovali vyse."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%% md\n",
    "## Tagging and Extraction Using OpenAI functions"
   ],
   "id": "739a1e108ffc69f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tagging",
   "id": "4d0cc0c65bd7cfd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9f98ca45f829d32b"
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ac44426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "34d75391",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b3884fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "acfa5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "\n",
    "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
    "\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")\n",
    "#%%\n",
    "convert_pydantic_to_openai_function(Tagging)\n",
    "#%% md"
   ],
   "id": "109da89dffef1183"
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "93022d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4316a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7fd051f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'neg', 'language': 'it'}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647681f",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "Extraction is similar to tagging, but used for extracting multiple pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ea7fbefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: Optional[int] = Field(description=\"person's age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "97066e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    people: List[Person] = Field(description=\"List of info about people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f64c213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Information',\n",
       " 'description': 'Information to extract.',\n",
       " 'parameters': {'properties': {'people': {'description': 'List of info about people',\n",
       "    'items': {'description': 'Information about a person.',\n",
       "     'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
       "      'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "       'description': \"person's age\"}},\n",
       "     'required': ['name', 'age'],\n",
       "     'type': 'object'},\n",
       "    'type': 'array'}},\n",
       "  'required': ['people'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1b761710",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
    "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "from typing import Optional\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    name: str = Field(description=\"person's name\")\n",
    "\n",
    "    age: Optional[int] = Field(description=\"person's age\")\n",
    "#%%"
   ],
   "id": "c382cb87221ed348"
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e97ec856",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5df2b3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"people\":[{\"name\":\"Joe\",\"age\":30},{\"name\":\"Martha\",\"age\":null}]}', 'name': 'Information'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 112, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3b5f973e-a241-4a4b-9def-c2349f5cadd0-0', usage_metadata={'input_tokens': 112, 'output_tokens': 22, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
    "\n",
    "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})\n",
    "#%%"
   ],
   "id": "f21ffefd0d67ba93"
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "223db312",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0d3c17c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78e1fa",
   "metadata": {},
   "source": [
    "### Doing it for real\n",
    "\n",
    "We can apply tagging to a larger body of text.\n",
    "\n",
    "For example, let's load this blog post and extract tag information from a sub-set of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d81e083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "af00f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f8168f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = doc.page_content[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "95167fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful gene\n"
     ]
    }
   ],
   "source": [
    "print(page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "93b8a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
    "    keywords: str = Field(description=\"Provide keywords related to the content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "db4dddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_tagging_function = [\n",
    "    convert_pydantic_to_openai_function(Overview)\n",
    "]\n",
    "tagging_model = model.bind(\n",
    "    functions=overview_tagging_function,\n",
    "    function_call={\"name\":\"Overview\"}\n",
    ")\n",
    "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9c7ac209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'The article discusses building autonomous agents powered by LLM (large language model) as the core controller, with components like planning, memory, and tool use. It explores task decomposition, self-reflection, and challenges in implementing LLM-powered agents.',\n",
       " 'language': 'English',\n",
       " 'keywords': 'LLM, autonomous agents, planning, memory, tool use, task decomposition, self-reflection, challenges'}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7f0706ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "    title: str\n",
    "    author: Optional[str]\n",
    "\n",
    "\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: List[Paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "40d211c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_extraction_function = [\n",
    "    convert_pydantic_to_openai_function(Info)\n",
    "]\n",
    "extraction_model = model.bind(\n",
    "    functions=paper_extraction_function, \n",
    "    function_call={\"name\":\"Info\"}\n",
    ")\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: List[Paper]\n",
    "#%%\n",
    "paper_extraction_function = [\n",
    "\n",
    "    convert_pydantic_to_openai_function(Info)\n",
    "]\n",
    "extraction_model = model.bind("
   ],
   "id": "432c40ea693b1d1b"
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f2e1c2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': None},\n",
       " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': None},\n",
       " {'title': 'LLM+P (Liu et al. 2023)', 'author': None},\n",
       " {'title': 'ReAct (Yao et al. 2023)', 'author': None},\n",
       " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': None},\n",
       " {'title': 'Chain of Hindsight (CoH; Liu et al. 2023)', 'author': None},\n",
       " {'title': 'Algorithm Distillation (AD; Laskin et al. 2023)', 'author': None}]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b12a4703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "263f788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5c766e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "efdff324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d88a5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "66524b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1574be17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n"
     ]
    }
   ],
   "source": [
    "print(splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5fe0ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "2e72640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = RunnableLambda(\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cef1cb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'hi'}]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ae8a5",
   "metadata": {},
   "source": [
    "A to delame proto, ze chceme vypustit model na list of documents. Proto na vstupu bude list dokumentu, ktere chceme zpracovat, pustime na ne extraction_chain, ale musime pridat metodu .map(), aby se aplikovala na kazdy dokument zvlast. A pomoci funkce flatten() z listu listu udelame list."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\n",
    "#%%\n",
    "splits = text_splitter.split_text(doc.page_content)\n",
    "#%%\n",
    "len(splits)\n",
    "#%%\n",
    "def flatten(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list\n",
    "#%%\n",
    "flatten([[1, 2], [3, 4]])\n",
    "#%%\n",
    "print(splits[0])\n",
    "#%%"
   ],
   "id": "db9b19f7657d77b7"
  },
  {
   "cell_type": "markdown",
   "id": "776d0128",
   "metadata": {},
   "source": [
    "## Tools and Routing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "prep = RunnableLambda(\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
    ")"
   ],
   "id": "9a3b9afbbb60054e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "prep.invoke(\"hi\")",
   "id": "4fc9b85c6eaf9447"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c33be9260b0095b8"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855121f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for weather online\"\"\"\n",
    "    return \"42f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cacef87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69f6c223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Search for weather online'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2f2742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'title': 'Query', 'type': 'string'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d4bc67",
   "metadata": {},
   "source": [
    "Za pouziti pydantic modelu si muzeme pripravit rovnou celou strukturu do jsonu, jako v predchozim pripade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cfbd543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"Thing to search for\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9bfdb",
   "metadata": {},
   "source": [
    "A tohle schema pak vstupuje do toolu jako argument."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Mezi takove funkce patri:\n",
    "* Search Tools\n",
    "* Math Tools\n",
    "* SQL Tools\n",
    "* ...\n",
    "\n",
    "#%% md\n",
    "### Tools\n",
    "#%%\n",
    "from langchain.agents import tool"
   ],
   "id": "684a4771008c00f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tool dekorator udela z funkce objekt, ktery danou funkci prevede na langchain callable.",
   "id": "fe628799c45a1248"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for weather online\"\"\"\n",
    "    return \"42f\""
   ],
   "id": "c03c6a1834eeb38b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "search.name",
   "id": "89176ee71645a956"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "search.description",
   "id": "35ea2c9b8efc539c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "search.args",
   "id": "8ee5bcf79a8db250"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Za pouziti pydantic modelu si muzeme pripravit rovnou celou strukturu do jsonu, jako v predchozim pripade.",
   "id": "1773574d0f2d5ad2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"Thing to search for\")\n"
   ],
   "id": "7056476f404fe108"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A tohle schema pak vstupuje do toolu jako argument.",
   "id": "c2364502469a9460"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@tool(args_schema=SearchInput)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for the weather online.\"\"\"\n",
    "    return \"42f\""
   ],
   "id": "5de63ccb0349c06f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "search.args",
   "id": "bc191514f79c7ae8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A porad je to langchain callable",
   "id": "5fbe29e1bd49b0b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "search.run(\"sf\")",
   "id": "8b35b2e552b327f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Udelame si takovy vetsi priklad, ktery bude vracet teplotu v zavislosti na zadane zemepisne delce a sirce.",
   "id": "172d95dbc3eb1ed3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "import datetime\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "\n",
    "        'latitude': latitude,\n",
    "\n",
    "        'longitude': longitude,\n",
    "\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.utcnow()\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'\n",
    "#%%\n",
    "get_current_temperature.name\n",
    "#%%\n",
    "get_current_temperature.description\n",
    "#%%\n",
    "get_current_temperature.args\n",
    "#%%"
   ],
   "id": "bfd6b75a8fa19920"
  },
  {
   "cell_type": "markdown",
   "id": "05a3e385",
   "metadata": {},
   "source": [
    "A druhy takovy tool bude vyhledavani na Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e11b3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5b2e821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search_wikipedia'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b1ae84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run Wikipedia search and get page summaries.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c070076c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'search_wikipedia',\n",
       " 'description': 'Run Wikipedia search and get page summaries.',\n",
       " 'parameters': {'properties': {'query': {'type': 'string'}},\n",
       "  'required': ['query'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_tool_to_openai_function(search_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f6703f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Page: LangChain\\nSummary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n\\nPage: Milvus (vector database)\\nSummary: Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\\nMilvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.\\n\\n\\n\\nPage: Retrieval-augmented generation\\nSummary: Retrieval-Augmented Generation (RAG) is a technique that grants generative artificial intelligence models information retrieval capabilities. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to augment information drawn from its own vast, static training data. This allows LLMs to use domain-specific and/or updated information.  \\nUse cases include providing chatbot access to internal company data or giving factual information only from an authoritative source.\\n\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia({\"query\": \"langchain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ded30d",
   "metadata": {},
   "source": [
    "Casto jsou funkce, ktere chceme pouzivat za nejakym API. Existuje snadna cesta jak takovy api definition prevest na openai spec. V nasledujicim prikladu mame funkci zadanou jako api, vcetne vsech volani. A prevedeme ji na langchain object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0126590",
   "metadata": {},
   "source": [
    "Pozor, tady dochazi k verzi mismatch pro balicek pydantic and langchain.utilities.openai. Je potreba mit bud pydantic <2 nebo nainstalovat balicek: \"openapi-pydantic==0.3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac606255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions.openapi import openapi_spec_to_openai_fn\n",
    "from langchain.utilities.openapi import OpenAPISpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a9be6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "{\n",
    "  \"openapi\": \"3.0.0\",\n",
    "  \"info\": {\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"title\": \"Swagger Petstore\",\n",
    "    \"license\": {\n",
    "      \"name\": \"MIT\"\n",
    "    }\n",
    "  },\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"url\": \"http://petstore.swagger.io/v1\"\n",
    "    }\n",
    "  ],\n",
    "  \"paths\": {\n",
    "    \"/pets\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"List all pets\",\n",
    "        \"operationId\": \"listPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"limit\",\n",
    "            \"in\": \"query\",\n",
    "            \"description\": \"How many items to return at one time (max 100)\",\n",
    "            \"required\": false,\n",
    "            \"schema\": {\n",
    "              \"type\": \"integer\",\n",
    "              \"maximum\": 100,\n",
    "              \"format\": \"int32\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"A paged array of pets\",\n",
    "            \"headers\": {\n",
    "              \"x-next\": {\n",
    "                \"description\": \"A link to the next page of responses\",\n",
    "                \"schema\": {\n",
    "                  \"type\": \"string\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pets\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"post\": {\n",
    "        \"summary\": \"Create a pet\",\n",
    "        \"operationId\": \"createPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"201\": {\n",
    "            \"description\": \"Null response\"\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"/pets/{petId}\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"Info for a specific pet\",\n",
    "        \"operationId\": \"showPetById\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"petId\",\n",
    "            \"in\": \"path\",\n",
    "            \"required\": true,\n",
    "            \"description\": \"The id of the pet to retrieve\",\n",
    "            \"schema\": {\n",
    "              \"type\": \"string\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"Expected response to a valid request\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pet\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"components\": {\n",
    "    \"schemas\": {\n",
    "      \"Pet\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"id\",\n",
    "          \"name\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int64\"\n",
    "          },\n",
    "          \"name\": {\n",
    "            \"type\": \"string\"\n",
    "          },\n",
    "          \"tag\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"Pets\": {\n",
    "        \"type\": \"array\",\n",
    "        \"maxItems\": 100,\n",
    "        \"items\": {\n",
    "          \"$ref\": \"#/components/schemas/Pet\"\n",
    "        }\n",
    "      },\n",
    "      \"Error\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"code\",\n",
    "          \"message\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"code\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int32\"\n",
    "          },\n",
    "          \"message\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b4e2194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.0 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n"
     ]
    }
   ],
   "source": [
    "spec = OpenAPISpec.from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "323f3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_openai_functions, pet_callables = openapi_spec_to_openai_fn(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaeb5144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'listPets',\n",
       "  'description': 'List all pets',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'params': {'type': 'object',\n",
       "     'properties': {'limit': {'type': 'integer',\n",
       "       'maximum': 100.0,\n",
       "       'schema_format': 'int32',\n",
       "       'description': 'How many items to return at one time (max 100)'}},\n",
       "     'required': []}}}},\n",
       " {'name': 'createPets',\n",
       "  'description': 'Create a pet',\n",
       "  'parameters': {'type': 'object', 'properties': {}}},\n",
       " {'name': 'showPetById',\n",
       "  'description': 'Info for a specific pet',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'path_params': {'type': 'object',\n",
       "     'properties': {'petId': {'type': 'string',\n",
       "       'description': 'The id of the pet to retrieve'}},\n",
       "     'required': ['petId']}}}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pet_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "358287f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ce315ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0).bind(functions=pet_openai_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e98e409f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"params\":{\"limit\":3}}', 'name': 'listPets'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 123, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-d3f18532-bb0d-4034-9d8e-79034ae1ae3a-0', usage_metadata={'input_tokens': 123, 'output_tokens': 17, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what are three pets names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "680ccdff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"path_params\":{\"petId\":\"42\"}}', 'name': 'showPetById'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 126, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-72f079b2-d3df-40fd-bd87-40a3f50c9e41-0', usage_metadata={'input_tokens': 126, 'output_tokens': 20, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"tell me about pet with id 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5513f9",
   "metadata": {},
   "source": [
    "### Routing\n",
    "\n",
    "In lesson 3, we show an example of function calling deciding between two candidate functions.\n",
    "\n",
    "Given our tools above, let's format these as OpenAI functions and show this same behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f19ca1",
   "metadata": {},
   "source": [
    "Vsimnete si, ze tyto funkce taky LLM nevola, ale vraci _function_call_ a argumenty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "108b07cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    format_tool_to_openai_function(f) for f in [\n",
    "        search_wikipedia, get_current_temperature\n",
    "    ]\n",
    "]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98e5bfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 105, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-048471fb-c2e3-4244-aadb-95a8c36b8114-0', usage_metadata={'input_tokens': 105, 'output_tokens': 26, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what is the weather in sf right now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ff517d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Langchain\"}', 'name': 'search_wikipedia'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 101, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-02224ce0-6d6a-41e4-aea0-4405c33796bd-0', usage_metadata={'input_tokens': 101, 'output_tokens': 17, 'total_tokens': 118, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what is langchain\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#%% md\n",
    "### Routing\n",
    "\n",
    "In lesson 3, we show an example of function calling deciding between two candidate functions.\n",
    "\n",
    "Given our tools above, let's format these as OpenAI functions and show this same behavior.\n",
    "#%% md\n",
    "Vsimnete si, ze tyto funkce taky LLM nevola, ale vraci _function_call_ a argumenty.\n"
   ],
   "id": "52400eb457d34115"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "functions = [\n",
    "    format_tool_to_openai_function(f) for f in [\n",
    "        search_wikipedia, get_current_temperature\n",
    "    ]\n",
    "]"
   ],
   "id": "f4cf708a7aa0ad2d"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95ccfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3fc4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\": \"what is the weather in sf right now\"})"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "model.invoke(\"what is the weather in sf right now\")\n"
   ],
   "id": "4564487f3e2973ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.invoke(\"what is langchain\")",
   "id": "de78a796f4c4399"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65602704548c97e9"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1750fc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.agents.AgentFinish"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfdfcaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'Well, hello there! How can I assist you today?'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.return_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f92991",
   "metadata": {},
   "source": [
    "Tady udelame routing: podivame se, jestli type je AgentFinish, tj. nevola zadnou funkci. A pokud neni, najde volanou funkci a zavola ji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4af9ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "def route(result):\n",
    "    if isinstance(result, AgentFinish):\n",
    "        return result.return_values['output']\n",
    "    else:\n",
    "        tools = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }\n",
    "        return tools[result.tool].run(result.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67b28d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser() | route"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "type(result)\n",
    "#%%\n",
    "result.tool\n",
    "#%%\n",
    "result.tool_input\n",
    "#%%\n",
    "\n",
    "get_current_temperature(result.tool_input)\n"
   ],
   "id": "b1afbf45e343daaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "result = chain.invoke({\"input\": \"hi!\"})"
   ],
   "id": "f3c534d04c99d64a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "type(result)",
   "id": "f86c9950fa4edab6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "result.return_values",
   "id": "ee80b3749b9b908f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tady udelame routing: podivame se, jestli type je AgentFinish, tj. nevola zadnou funkci. A pokud neni, najde volanou funkci a zavola ji.\n",
    "#%%\n",
    "from langchain.schema.agent import AgentFinish\n",
    "def route(result):\n",
    "    if isinstance(result, AgentFinish):\n",
    "        return result.return_values['output']\n",
    "    else:\n",
    "        tools = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }\n",
    "        return tools[result.tool].run(result.tool_input)\n",
    "#%%"
   ],
   "id": "ccba33a53fe2d74c"
  },
  {
   "cell_type": "markdown",
   "id": "3aa88561",
   "metadata": {},
   "source": [
    "Ted to dame vsechno dohromady. Zkombinuje tools a chat memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32a822",
   "metadata": {},
   "source": [
    "Opáčko:\n",
    "* agents: \n",
    "  * kombinace llm a code\n",
    "  * llm rozhodne, jakou cast kodu zavolat\n",
    "* agent loop:\n",
    "  * rozhodne, ktery tool zavolat\n",
    "  * sleduje vysledek volani toolu\n",
    "  * opakuje, dokud neni splneno stopping criterea\n",
    "* Stopping condition muze byt:\n",
    "  * llm se rozhodne, ze staci\n",
    "  * hard-coded\n",
    "\n",
    "V tehle casti:\n",
    "* si pripravime nekolik toolu\n",
    "* napiseme agenta s pomoci LCEL\n",
    "* pouzijeme agent_executor, ktery\n",
    "  * vytvori agent loop\n",
    "  * prida overhead jako jsou logy, error handling, early stopping, tracing, ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "36f5605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3007c3",
   "metadata": {},
   "source": [
    "Tady si vytvorime jiz znamou funkci na hledani teploty na zaklade polohy."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.utcnow()\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'\n",
    "#%% md\n",
    "Tady si vytvorime jiz znamy tool na hledani wikipedie\n",
    "#%%\n",
    "import wikipedia\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:"
   ],
   "id": "894566b0dba3661e"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a077deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_temperature, search_wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "31c4e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "        ):\n",
    "            pass\n",
    "\n",
    "    if not summaries:\n",
    "\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ],
   "id": "339cc32afc64ebb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tools = [get_current_temperature, search_wikipedia]",
   "id": "b772b4ac8a4c0296"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "#%% md"
   ],
   "id": "26f8c1d4e043502e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#%%\n",
    "functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "prompt = ChatPromptTemplate.from_messages(["
   ],
   "id": "f894600cb9f50b3b"
  },
  {
   "cell_type": "markdown",
   "id": "aef29de4",
   "metadata": {},
   "source": [
    "Tady zavolame chain a do _agent_scratchpad_ vkladame prazdny list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a28084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = chain.invoke({\n",
    "    \"input\": \"what is the weather is sf?\",\n",
    "    \"agent_scratchpad\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c700c541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02d7cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = get_current_temperature(result1.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90f14516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 15.1°C'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5a4f6310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.agents.AgentActionMessageLog"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b1b0a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89a9616b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 112, 'total_tokens': 138, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-f3085b87-c572-4177-b7d3-bfd85e1840df-0')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.message_log"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "#%%\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "#%% md"
   ],
   "id": "dd405c89ce015543"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b0f0415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 15.1°C.'}, log='The current temperature in San Francisco is 15.1°C.')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ec7f1",
   "metadata": {},
   "source": [
    "Ted to cele zabalime do agent loopu, ktery se bude volat tak dlouho dokud nenarazi na _AgentFinish_ type, ktery znamena, ze uz neni treba volat zadnou dalsi funkci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "449578b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = chain.invoke({\n",
    "            \"input\": user_input, \n",
    "            \"agent_scratchpad\": format_to_openai_functions(intermediate_steps)\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d025d",
   "metadata": {},
   "source": [
    "Funkce _RunnablePassthrough_ vytvori dictionary, ktery umi langchain pouzivat. Takze abychom nemeli neznamou funkci v nasem _run_agent_, tak budeme mit chain, do kteryho vstupuje dictionary tvoreny promennoou _agent_scratchpad_ a ten se preda chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "df6dfb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "08015b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke({\n",
    "            \"input\": user_input, \n",
    "            \"intermediate_steps\": intermediate_steps\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "225437ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 13.6°C.'}, log='The current temperature in San Francisco is 13.6°C.')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is the weather is sf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d748c127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. It is used for tasks such as document analysis and summarization, chatbots, and code analysis.'}, log='LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. It is used for tasks such as document analysis and summarization, chatbots, and code analysis.')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is langchain?\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke({\n",
    "            \"input\": user_input, \n",
    "\n",
    "            \"intermediate_steps\": intermediate_steps\n",
    "        })"
   ],
   "id": "2ecea127eb03ce0a"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3ad35cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': 'Langchain'}`\n",
      "\n",
      "\n",
      "\u001B[0m\u001B[33;1m\u001B[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "\n",
      "Page: Milvus (vector database)\n",
      "Summary: Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\n",
      "Milvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.\n",
      "\n",
      "\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval-Augmented Generation (RAG) is a technique that grants generative artificial intelligence models information retrieval capabilities. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to augment information drawn from its own vast, static training data. This allows LLMs to use domain-specific and/or updated information.  \n",
      "Use cases include providing chatbot access to internal company data or giving factual information only from an authoritative source.\n",
      "\n",
      "\u001B[0m\u001B[32;1m\u001B[1;3mLangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. It is used for tasks such as document analysis and summarization, chatbots, and code analysis.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is langchain?',\n",
       " 'output': 'LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. It is used for tasks such as document analysis and summarization, chatbots, and code analysis.'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dd15165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mNice to meet you, Bob! How can I assist you today?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is bob',\n",
       " 'output': 'Nice to meet you, Bob! How can I assist you today?'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"my name is bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68823d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mI'm sorry, I don't have access to your personal information like your name. How can I assist you today?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name',\n",
       " 'output': \"I'm sorry, I don't have access to your personal information like your name. How can I assist you today?\"}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is my name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348787f",
   "metadata": {},
   "source": [
    "Nicmene, tenhle chain nema pamet. Abychom mohli udrzovat konverzaci, musime pridat pamet. Tedy _ChatMemory_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef119ed0",
   "metadata": {},
   "source": [
    "Musime pridat novou promennou _chat_memory_, ktera bude obsahovat chatovaci historii. Opet pouzijeme _MessagesPlaceholder_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "45ff6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c6c38505",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b556982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tf/tzhjhrlj1_x14gcsq_wsn4580000gn/T/ipykernel_4857/4052124774.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e9496c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent_chain, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "59bf9af08ab26cfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),"
   ],
   "id": "8309a9456fd48e80"
  },
  {
   "cell_type": "markdown",
   "id": "6e2e8685",
   "metadata": {},
   "source": [
    "Zaverecni priklad s dashboardem a chatbotem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32c54783",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_your_own(query: str) -> str:\n",
    "    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n",
    "    print(type(query))\n",
    "    return query[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f17cb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_temperature, search_wikipedia, create_your_own]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "af7da78b79d8834f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "920b5a8c1c183919"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce51b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5a5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8333e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2191a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e540cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c82056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3ea124b594ab7cfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc72261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a213d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3776bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6765c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "edae2fc8cb0d22dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3d9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e66fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1dfd267c896a8669"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace4433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfeeb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912bafdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c981e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f72409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97b5fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70bb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7a203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
