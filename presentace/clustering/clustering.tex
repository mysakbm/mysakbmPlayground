\documentclass{beamer}

%\usetheme[hideothersubsections]{UNLTheme}
\beamertemplatenavigationsymbolsempty

\mode<presentation> {
    \usetheme[hideothersubsections]{UNLTheme}
    %\usetheme{montpellier}
    \setbeamercovered{transparent}
}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{multimedia}
\usepackage{amsbsy}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{amsthm}
%\usepackage{showkeys}
\usepackage{gensymb}
\usepackage{amsmath} % balíček pro pokročilou matem. sazbu
\usepackage{epsfig} % balíčky pro vkládání grafických souborů typu EPS
\usepackage{graphicx}
\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\newcommand{\e}{\mathtt{e}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\proceseta}{\boldsymbol{\eta}}
\newcommand{\dr}{\, \mathrm{d}}
\newtheorem{veta}{Theorem}[section]
\newtheorem{defin}[veta]{Definition}

\title[Clustering Techniques in ML]{Clustering Techniques in Machine Learning}
\author{Author Name}
\institute{Institute/Organization}
\date{\today}

% % For better math spacing and fonts
% \setlength{\abovedisplayskip}{5pt}
% \setlength{\belowdisplayskip}{5pt}

\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

% 1. Introduction
\section{Introduction}
\begin{frame}{Definition}
    \textbf{Clustering:} Grouping unlabeled data points into subsets (clusters) where points in the same cluster are more similar to each other than to those in other clusters.
    
    \textbf{Mathematical Setup:}
    Given $X = \{x_1, x_2, \dots, x_n\}$ with each $x_i \in \mathbb{R}^d$, clustering aims to partition $X$ into $k$ disjoint subsets (clusters) $C = \{C_1, C_2, \dots, C_k\}$ such that:
    \[
        C_i \cap C_j = \emptyset \quad \forall i \neq j, \quad\text{and}\quad \bigcup_{i=1}^k C_i = X.
    \]
\end{frame}

\begin{frame}{Applications}
    \begin{itemize}
        \item Customer segmentation
        \item Image compression and object grouping
        \item Document/topic clustering in NLP
        \item Bioinformatics (gene expression data analysis)
    \end{itemize}
\end{frame}

% 2. Distance and Similarity Measures
\section{Distance and Similarity Measures}
\begin{frame}{Common Metrics}
    \textbf{Euclidean Distance (L2):}
    \[
        d(x,y) = \sqrt{\sum_{m=1}^{d}(x_m - y_m)^2}
    \]
    
    \textbf{Manhattan Distance (L1):}
    \[
        d(x,y) = \sum_{m=1}^{d}|x_m - y_m|
    \]
    
    \textbf{Cosine Similarity:}
    \[
        \text{sim}(x,y) = \frac{x \cdot y}{\|x\|\|y\|}
    \]
    
    \textit{Note:} Choice of metric influences clustering results, especially in high dimensions.
\end{frame}


% 3. Major Clustering Families
\section{Major Clustering Families}
\begin{frame}{Overview of Clustering Methods}
    \begin{table}[htbp]
        \begin{tabular}{l l p{6cm}}
            \hline
            \textbf{Method Type} & \textbf{Examples}       \\
            \hline
            Centroid-Based       & K-means, K-medoids      \\
            Hierarchical         & Agglomerative, Divisive \\
            Density-Based        & DBSCAN, OPTICS          \\
            Distribution-Based   & Gaussian Mixtures (GMM) \\
            Graph-Based          & Spectral Clustering     \\
            Deep/Embedding-Based & DEC, VAE-based methods  \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}


% 4. Centroid-Based Clustering
\section{Centroid-Based Clustering}
\subsection{K-Means}
\begin{frame}{K-Means}
    \textbf{Objective:}
    Minimize the Within-Cluster Sum of Squares (WCSS):
    \[
        J = \sum_{j=1}^{k} \sum_{x_i \in C_j} \|x_i - \mu_j\|^2,
    \]
    where $\mu_j$ is the centroid of cluster $C_j$.
    
    \textbf{Algorithm:}
    \begin{enumerate}
        \item Initialize $k$ centroids $\mu_j$.
        \item Assign each point $x_i$ to the closest centroid:
              \[
                  C_j^{(t)} = \{x_i : \|x_i - \mu_j^{(t)}\| \leq \|x_i - \mu_l^{(t)}\|, \forall l\}.
              \]
        \item Update centroids:
              \[
                  \mu_j^{(t+1)} = \frac{1}{|C_j^{(t)}|}\sum_{x_i \in C_j^{(t)}} x_i.
              \]
        \item Repeat until convergence.
    \end{enumerate}
\end{frame}

\begin{frame}{K-Means Pros/Cons}
    \begin{itemize}
        \item \textbf{Pros:} Simple, fast, widely used.
        \item \textbf{Cons:} Sensitive to initialization, primarily finds spherical clusters.
    \end{itemize}
\end{frame}

\subsection{K-Medoids}
\begin{frame}{K-Medoids}
    Similar to K-means, but uses actual data points as centers (medoids).
    
    \textbf{Update Step:}
    \[
        \tilde{\mu}_j = \arg\min_{x \in C_j} \sum_{x_i \in C_j} d(x_i, x)
    \]
    
    \textbf{Pros:} More robust to outliers than K-means.
\end{frame}

% 5. Hierarchical Clustering
\section{Hierarchical Clustering}
\begin{frame}{Hierarchical Clustering}
    \textbf{Concept:} Build a hierarchy of clusters without specifying $k$ upfront.
    
    \textbf{Agglomerative Clustering:}
    \begin{itemize}
        \item Start with each point as its own cluster.
        \item Iteratively merge the two closest clusters until one cluster remains.
    \end{itemize}
    
    \textbf{Linkage Methods:}
    \[
        \text{Single: } d(C_a, C_b) = \min_{x \in C_a, y \in C_b} d(x,y)
    \]
    \[
        \text{Complete: } d(C_a, C_b) = \max_{x \in C_a, y \in C_b} d(x,y)
    \]
    \[
        \text{Average: } d(C_a, C_b) = \frac{1}{|C_a||C_b|}\sum_{x \in C_a}\sum_{y \in C_b} d(x,y)
    \]
\end{frame}

\begin{frame}{Hierarchical Clustering Pros/Cons}
    \begin{itemize}
        \item \textbf{Pros:} No need to pre-specify $k$, interpretable dendrogram.
        \item \textbf{Cons:} High complexity, no backtracking once merged.
    \end{itemize}
\end{frame}

% 6. Density-Based Clustering: DBSCAN
\section{Density-Based Clustering}
\subsection{DBSCAN}
\begin{frame}{DBSCAN Concept}
    \textbf{Idea:} Identifies "core" points in dense regions.
    
    \textbf{Parameters:}
    \begin{itemize}
        \item $\epsilon$: Neighborhood radius
        \item MinPts: Minimum points within $\epsilon$ for a core point
    \end{itemize}
    
    \textbf{Core Point:} If at least MinPts are within $\epsilon$ of it.  
    \textbf{Reachability:} Points reachable via a chain of core points are in the same cluster.  
    Non-reachable points are noise.
\end{frame}

\begin{frame}{DBSCAN Algorithm Steps}
    \begin{enumerate}
        \item Identify core points.
        \item Form clusters by connecting core points within $\epsilon$.
        \item Assign non-core points to clusters if within $\epsilon$ of a core point.
        \item Unreachable points are noise.
    \end{enumerate}
    
    \begin{itemize}
        \item \textbf{Pros:} Detects arbitrarily shaped clusters, finds outliers.  
        \item \textbf{Cons:} Sensitive to $\epsilon$ and MinPts, struggles with varying density.    
    \end{itemize}
\end{frame}

\begin{frame}{Density-Based Variants}
    \textbf{OPTICS:} Handles varying densities, outputs an ordering.  
    \textbf{HDBSCAN:} Hierarchical density-based clustering, no fixed $\epsilon$ needed.
\end{frame}

% 7. Distribution-Based Clustering: GMM
\section{Distribution-Based Clustering}
\subsection{Gaussian Mixture Models}
\begin{frame}{GMM Concept}
    Assume data from a mixture of $k$ Gaussians:
    \[
        p(x) = \sum_{j=1}^{k} \pi_j \mathcal{N}(x|\mu_j, \Sigma_j).
    \]
    
    \textbf{EM Algorithm:} \\
    \textbf{E-step:}
    \[
        \gamma_{ij} = \frac{\pi_j \mathcal{N}(x_i|\mu_j,\Sigma_j)}{\sum_{l=1}^{k}\pi_l \mathcal{N}(x_i|\mu_l,\Sigma_l)}
    \]
    
    \textbf{M-step:}
    \[
        \pi_j := \frac{1}{n}\sum_{i=1}^{n}\gamma_{ij}, \quad
        \mu_j := \frac{\sum_{i=1}^{n}\gamma_{ij}x_i}{\sum_{i=1}^{n}\gamma_{ij}}, \quad
        \Sigma_j := \frac{\sum_{i=1}^{n}\gamma_{ij}(x_i-\mu_j)(x_i-\mu_j)^T}{\sum_{i=1}^{n}\gamma_{ij}}
    \]
\end{frame}

\begin{frame}{GMM Pros/Cons}
    \begin{itemize}
        \item \textbf{Pros:} Can model complex cluster shapes, probabilistic interpretation.
        \item \textbf{Cons:} May converge to local maxima, assumes Gaussianity.
    \end{itemize}
\end{frame}

% 8. Graph-Based Clustering: Spectral Clustering
\section{Graph-Based Clustering}
\subsection{Spectral Clustering}
\begin{frame}{Spectral Clustering Idea}
    \textbf{Idea:} Use eigenvectors of a similarity graph’s Laplacian matrix to cluster points. \\
    \textbf{Steps:}
    \begin{enumerate}
        \item Construct similarity graph $W$:
              \[
                  w_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right).
              \]
        \item Compute Laplacian $L = D - W$, where $D_{ii} = \sum_j w_{ij}$.
        \item Compute eigenvectors of $L$.
        \item Use top $k$ eigenvectors as features and cluster (e.g., with K-means).
    \end{enumerate}
    
    \textbf{Pros:} Finds non-linearly separable clusters.  \\
    \textbf{Cons:} Eigen-decomposition can be costly, parameter sensitive.
\end{frame}

% 9. Neural Network-Based Clustering
\section{Neural Network-Based Clustering}
\begin{frame}{Deep Embedding Clustering (DEC)}
    \textbf{Idea:} Modern methods integrate deep learning to produce suitable embeddings for clustering.\\
    Jointly optimize a reconstruction loss (via autoencoder) and a clustering loss (e.g., Kullback–Leibler divergence).
    
    \textbf{Loss:}
    \[
        L = L_r + \lambda L_c
    \]
    \begin{itemize}
        \item $L_r$: Reconstruction loss, $\|X - \hat{X}\|_F^2$
        \item $L_c$: KL divergence between soft assignments $Q$ and a target distribution $P$
    \end{itemize}
\end{frame}

% 10. Cluster Validation Metrics
\section{Cluster Validation Metrics}
\begin{frame}{Internal Validation}
    \textbf{Silhouette Coefficient:}
    $
        s(i) = \frac{b(i)-a(i)}{\max\{a(i),b(i)\}}
    $
    \begin{itemize}
        \item $a(i)$: mean intra-cluster distance  
        \item $b(i)$: min mean distance to any other cluster
    \end{itemize} 
\end{frame}

\begin{frame}{Internal Validation}
    \textbf{Davies-Bouldin Index:}
    $
        DB = \frac{1}{k}\sum_{i=1}^k \max_{j \neq i} \frac{\sigma_i + \sigma_j}{\|\mu_i - \mu_j\|},
    $
    where:
    \begin{itemize}
        \item $k$ is the number of clusters,
        \item $\sigma_i$ is the average distance of all points in cluster $i$ to the centroid of cluster $i$,
        \item $\mu_i$ is the centroid of cluster $i$,
        \item $\|\mu_i - \mu_j\|$ is the distance between the centroids of clusters $i$ and $j$.
    \end{itemize}
\end{frame}
\begin{frame}{Validation Summary}
    \textbf{Silhouette Score:}
    \begin{itemize}
        \item \textbf{Value Range:} [-1, 1]
        \item \textbf{Interpretation:}
        \begin{itemize}
            \item $\approx 1.0$: Point is well-clustered
            \item $\approx 0.0$: Point is on cluster boundary
            \item $\approx -1.0$: Point might be in wrong cluster
        \end{itemize}
        \item \textbf{Best Use Case:} Evaluating individual point placement
        \item \textbf{Complexity:} $O(n^2)$
    \end{itemize}
\end{frame}

\begin{frame}{Validation Summary}
    \textbf{Davies-Bouldin Index:}
    \begin{itemize}
        \item \textbf{Value Range:} [0, $\infty$)
        \item \textbf{Interpretation:}
        \begin{itemize}
            \item Close to 0: Better clustering
            \item Larger values: Worse clustering
        \end{itemize}
        \item \textbf{Best Use Case:} Comparing different clustering results
        \item \textbf{Complexity:} $O(k^2)$ where $k$ = number of clusters
    \end{itemize}
\end{frame}

\begin{frame}{Validation Summary}
    \begin{table}[htbp]
        \centering
        \tiny
        \begin{tabular}{|l|c|c|}
            \hline
            \textbf{Aspect} & \textbf{Silhouette Score} & \textbf{Davies-Bouldin Index} \\
            \hline
            Range & [-1, 1] & [0, $\infty$) \\
            Optimal Value & 1 & 0 \\
            Measures & Point-level cohesion and separation & Cluster-level separation \\
            Complexity & $O(n^2)$ & $O(k^2)$ \\
            Best Use Case & Evaluating individual point placement & Comparing different clustering results \\
            \hline
        \end{tabular}
    \end{table}
  
\end{frame}

\begin{frame}{External Validation}
    With ground truth available:
    
    \textbf{Rand Index (RI):}
    \[
        RI = \frac{TP + TN}{TP + TN + FP + FN}.
    \]
    
    \textbf{Adjusted Rand Index (ARI):} Adjusts RI for chance.
\end{frame}

% 11. Complexity and Scalability
\section{Complexity and Scalability}
\begin{frame}{Complexity Overview}
    \begin{table}[htbp]
        \centering
        \begin{tabular}{l l p{4cm}}
            \hline
            \textbf{Algorithm}  & \textbf{Complexity}        & \textbf{Notes}        \\
            \hline
            K-means             & $O(n k d)$ per iteration   & Fast, simple          \\
            Hierarchical        & $O(n^3)$ (naive)           & Often for smaller $n$ \\
            DBSCAN              & $O(n \log n)$ to $O(n^2)$  & Depends on indexing   \\
            GMM (EM)            & $O(n k d^2)$ per iteration & Covariance matters    \\
            Spectral Clustering & $O(n^3)$                   & Eigen-decomposition   \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}

% 12. Advanced Techniques and Trends
\section{Advanced Techniques and Trends}
\begin{frame}{Advanced Techniques}
    \begin{itemize}
        \item \textbf{Dimensionality Reduction (PCA, t-SNE, UMAP):} Handle high-dimensional data.
        \item \textbf{Kernel Methods:} Clustering in nonlinear feature spaces.
        \item \textbf{Fuzzy Clustering (Fuzzy C-means):} Soft membership:
              \[
                  J_m = \sum_{j=1}^k \sum_{i=1}^n u_{ij}^m \|x_i - \mu_j\|^2.
              \]
        \item \textbf{Semi-Supervised Clustering:} Must-link/cannot-link constraints guide clustering.
    \end{itemize}
\end{frame}

% 13. Conclusion
\section{Conclusion}
\begin{frame}{Conclusion}
    \begin{itemize}
        \item Wide range of clustering algorithms available.
        \item Choice depends on data shape, scale, and domain knowledge.
        \item Use validation metrics and possibly dimensionality reduction.
        \item Experimentation is key to discovering meaningful structure.
    \end{itemize}
\end{frame}

\end{document}
