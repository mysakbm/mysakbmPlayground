\documentclass{beamer}

%\usetheme[hideothersubsections]{UNLTheme}
\beamertemplatenavigationsymbolsempty

\mode<presentation> {
    \usetheme[hideothersubsections]{UNLTheme}
    %\usetheme{montpellier}
    \setbeamercovered{transparent}
}

\usepackage[cp1250]{inputenc}
\usepackage[IL2]{fontenc}
\usepackage[english]{babel}
\usepackage{color}
\usepackage{multimedia}
\usepackage{amsbsy}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{amsthm}
%\usepackage{showkeys}
\usepackage{gensymb}
\usepackage{amsmath} % balíček pro pokročilou matem. sazbu
\usepackage{epsfig} % balíčky pro vkládání grafických souborů typu EPS
\usepackage{graphicx}
\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\newcommand{\e}{\mathtt{e}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\proceseta}{\boldsymbol{\eta}}
\newcommand{\dr}{\, \mathrm{d}}
\newtheorem{veta}{Theorem}[section]
\newtheorem{defin}[veta]{Definition}


\title{CatBoost}
\author{Michael Mat\v{e}j\r{u}}
\institute[KB]{AI Squad}
\date{\today}

\begin{document}

    \begin{frame}
        \titlepage
    \end{frame}

    \begin{frame}
        \frametitle{Outline}
        \tableofcontents
    \end{frame}


    \section{Motivation}
    \begin{frame}
        \frametitle{Motivation}
        \begin{itemize}
            \item MRMR Feature Selection Algorithm implemented by Smazzanti
            \pause
            \item For categorical encoding he used three target based algorithms
            \pause
            \item And inspite of all common sense it works in Kaggle Challanges
            \pause
            \item CatBoost is an algorithm for gradient boosting on decision trees.
            \pause
            \item It is developed by Yandex since 2009, resp. 2014-2015.
            \pause
            \item In 2016 it went open-source.
            \pause
            \item JetBrains uses CatBoost for Code Completion
            \pause
            \item Cloudflare uses CatBoost for bot detection
            \pause
            \item Careem uses CatBoost to predict future destinations of the rides
        \end{itemize}
    \end{frame}


    \section{Overview of Gradient Boosting}
    \begin{frame}
        \frametitle{Overview of Gradient Boosting}
        Decition trees are weak learners. However, it has been shown that "combination" of weak
        learners you can achieve good results. What the the most common combinatons?
        \begin{itemize}
            \item Bagging: This technique builds different models in parallel using random
            subsets of data and deterministically aggregates the predictions of all predictors.
            \item Boosting: This technique is iterative, sequential, and adaptive as each
            predictor fixes its predecessor's error.
            \item Stacking: It is a meta-learning technique that involves combining predictions
            from multiple machine learning algorithms, like bagging and boosting.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{CatBoost Features}
        \begin{enumerate}
            \item \textbf{Symmetric trees}: CatBoost builds symmetric (balanced) trees, unlike
            XGBoost and
            LightGBM.
            \pause
            \item \textbf{Ordered boosting}: CatBoost uses the concept of ordered boosting, a
            permutation
            -driven approach to train model on a subset of data while calculating residuals on
            another subset, thus preventing target leakage and over-fitting.
            \pause
            \item \textbf{Native feature support}: CatBoost supports all kinds of features be it
            numeric,
            categorical, or text and saves time and effort of preprocessing.
        \end{enumerate}
    \end{frame}

    \begin{frame}[fragile]
        \frametitle{Catboost Encoder}
        \begin{itemize}
            \item Catboost overcomes the target leakage by introducing time into dataset - the
            order of the observations.
            \pause
            \item $\hat{x}^k_i = \frac{\sum_{j = 0}^{j \leq i} (y_j \cdot (x_j == k)) - y_i +
            prior}{\sum_{j = 0}^{j \leq i} (x_j == k) + 1}$
            \pause
            \item To prevent the over-fitting, the process is repeated several times on shuffled
            dataset and results are averaged.
            \pause
            \item Catboost "on-the-fly" encoding is one of the core advantages of CatBoost.
            \pause
        \end{itemize}

        \begin{lstlisting}
CBE_encoder = CatBoostEncoder()
train_cbe = CBE_encoder.fit_transform(train[feature_list], target)
test_cbe = CBE_encoder.transform(test[feature_list])
        \end{lstlisting}
    \end{frame}

    \begin{frame}
        \frametitle{Feature Processing}
        \begin{itemize}
            \item \textbf{Numerical Featuers}: The same way like the others - i.e. XGBoost.
            \pause
            \item \textbf{Categorical Features}: Supported One-Hot Encoding and so-called CatBoost
            encoder. Also, greedy search for combinations. CatBoost automatically combines
            categorical features, most times two or three.
            \pause
            \item \textbf{Text features}: CatBoost also handles text features (containing regular
            text) by providing inherent text preprocessing using Bag-of-Words (BoW), Naive
            -Bayes, and BM-25 (for multiclass) to extract words from text data, create
            dictionaries (letter, words, grams), and transform them into numeric features.
        \end{itemize}
    \end{frame}

    \begin{frame}
        \frametitle{CatBoost Features 2}
        \begin{enumerate}
            \setcounter{enumi}{4}
            \item \textbf{Ranking}: Ranking can be broadly done under three objective functions:
            Pointwise, Pairwise, and Listwise.
            \pause
            \item CatBoost's ranking mode variations:
            \begin{itemize}
                \item Ranking (YetiRank, YetiRankPairwise)
                \item Pairwise (PairLogit, PairLogitPairwise)
                \item Ranking + Classification (QueryCrossEntropy)
                \item Ranking + Regression (QueryRMSE)
                \item Select top 1 candidate (QuerySoftMax)
            \end{itemize}
            \pause
            \item CatBoost also provides ranking benchmarks comparing CatBoost, XGBoost and
            LightGBM with different ranking variations.
            \pause
            \item \textbf{The Usual Ones}: Speed, Feature Importance, Model Analysis (SHAP)
        \end{enumerate}
    \end{frame}


    \section{Conclusion}
    \begin{frame}
        \fontsize{6.5pt}{6.5}\selectfont
        \frametitle{Conclusion}
        Used Dataset "Cat-in-Dat".
        \begin{table}[htb]
            \begin{center}
            {\renewcommand{\arraystretch}{1.5}
            \renewcommand{\tabcolsep}{0.05cm}
                \begin{tabular}[c]{|l|c|c|c|}
                    \hline
                    \textbf{Description}                         & \textbf{Training Time} & \textbf{Prediction Time} & \textbf{ROC AUC Score} \\
                    \hline
                    Default Random Forest                        & 5.173537               & 0.264041                 & 0.600149               \\
                    \hline
                    Default LightGBM without categorical support & 1.026472               & 0.072304                 & 0.635199               \\
                    \hline
                    Default LightGBM with categorical support    & 2.491392               & 0.073969                 & 0.644861               \\
                    \hline
                    Default XGBoost                              & 6.096581               & 0.017769                 & 0.649817               \\
                    \hline
                    Default Catboost without categorical support & 18.193569              & 0.022442                 & 0.655684               \\
                    \hline
                    Default Catboost with categorical support    & 170.324903             & 0.296049                 & 0.673017               \\
                    \hline
                \end{tabular}}
            \end{center}
        \end{table}


    \end{frame}


    \section{End of Story}
    \begin{frame}
        \frametitle{The End}
        Thank you for your attention and patience.
    \end{frame}


\end{document}
