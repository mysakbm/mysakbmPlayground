{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "Recently, my attention got caught by an article about mRMR method for feature selection on\n",
    "[Towards Data Science](https://towardsdatascience.com/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b). I started to investigate a [git repo from smazzanti](https://github.com/smazzanti/mrmr) which can\n",
    "be installed by\n",
    "``` python\n",
    "pip install mrmr_selection\n",
    "```\n",
    "\n",
    "Because it can take and process even categorical variables I was curious, what method is used.\n",
    " In the source code, there are 3 categorical encoders from package\n",
    " ``` python\n",
    "pip install category-encoders\n",
    "```\n",
    "\n",
    "Those are Leave One Out, James-Stein, Target-Encoder. The features selection itself worked quite\n",
    "well on my work dataset which I know back-to-forth. So, I have created this workshop to summarize\n",
    " my knowledge about categorical variables encoding.\n",
    "\n",
    "# Sources\n",
    "I'll try to post all resources I used for this tutorial, however, if I missed one, I'm sorry to\n",
    "the author. There is a lot of information on the internet, it is easy to forget some citations.\n",
    "\n",
    "- [Benchmarking Categorical Encoders](https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8)\n",
    "\n",
    "- [CategoricalEncodingBenchmark](https://github.com/DenisVorotyntsev/CategoricalEncodingBenchmark)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- no feature preprocessing\n",
    "- Use KFold(5) for CV (+ more fold get better score)\n",
    "- LR (C=0.1, solver=lbfgs)\n",
    "- RF\n",
    "- XGB\n",
    "\n",
    "|Encoder|LB Score|\n",
    "|-|-|\n",
    "|TE|0.78018|\n",
    "|WOE|0.78861|\n",
    "|LOOE|0.79382|\n",
    "|James-Stein|0.77843|\n",
    "|Catboost|0.79164|\n",
    "|One-Hot|0.77973|\n",
    "\n",
    "\n",
    "\n",
    "### Category-Encoders\n",
    "\n",
    "1. Label Encoder\n",
    "2. One-Hot Encoder\n",
    "3. Sum Encoder\n",
    "4. Helmert Encoder\n",
    "5. Frequency Encoder\n",
    "6. Target Encoder\n",
    "7. M-Estimate Encoder\n",
    "8. Weight Of Evidence Encoder\n",
    "9. James-Stein Encoder\n",
    "10. Leave-one-out Encoder\n",
    "11. Catboost Encoder\n",
    "---\n",
    "- Validation (Benchmark)\n",
    "    - single LR\n",
    "    - LR with Cross Validation\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Category-Encoders \n",
    "\n",
    "A set of scikit-learn-style transformers for encoding categorical variables into numeric by means of different techniques."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# If you want to test this on your local notebook\n",
    "# http://contrib.scikit-learn.org/categorical-encoding/\n",
    "# !pip install category-encoders"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.m_estimate import MEstimateEncoder\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.helmert import HelmertEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders.james_stein import JamesSteinEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "\n",
    "TEST = False"
   ],
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "execution": {
     "iopub.status.busy": "2023-06-26T12:16:23.162767Z",
     "iopub.execute_input": "2023-06-26T12:16:23.163098Z",
     "iopub.status.idle": "2023-06-26T12:16:23.891365Z",
     "shell.execute_reply.started": "2023-06-26T12:16:23.163049Z",
     "shell.execute_reply": "2023-06-26T12:16:23.889702Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:25:15.755143Z",
     "start_time": "2023-06-27T09:25:14.691926Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "read csv and doing some preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "train = pd.read_csv('../../../data/cat-in-the-dat/train.csv')\n",
    "test = pd.read_csv('../../../data/cat-in-the-dat/test.csv')\n",
    "target = train['target']\n",
    "train_id = train['id']\n",
    "test_id = test['id']\n",
    "train.drop(['target', 'id'], axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:16:33.783880Z",
     "iopub.execute_input": "2023-06-26T12:16:33.784217Z",
     "iopub.status.idle": "2023-06-26T12:16:37.030251Z",
     "shell.execute_reply.started": "2023-06-26T12:16:33.784164Z",
     "shell.execute_reply": "2023-06-26T12:16:37.029439Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:25:19.326871Z",
     "start_time": "2023-06-27T09:25:18.374983Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 646 ms, sys: 98.6 ms, total: 745 ms\n",
      "Wall time: 950 ms\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train.shape"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:27:51.365243Z",
     "iopub.execute_input": "2023-06-26T12:27:51.365647Z",
     "iopub.status.idle": "2023-06-26T12:27:51.372112Z",
     "shell.execute_reply.started": "2023-06-26T12:27:51.365579Z",
     "shell.execute_reply": "2023-06-26T12:27:51.371096Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:25:24.006454Z",
     "start_time": "2023-06-27T09:25:24.001902Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(300000, 23)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:26:42.903005Z",
     "iopub.execute_input": "2023-06-26T12:26:42.903394Z",
     "iopub.status.idle": "2023-06-26T12:26:43.305693Z",
     "shell.execute_reply.started": "2023-06-26T12:26:42.903350Z",
     "shell.execute_reply": "2023-06-26T12:26:43.304386Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:25:25.985878Z",
     "start_time": "2023-06-27T09:25:25.849707Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "        bin_0  bin_1  bin_2 bin_3 bin_4  nom_0      nom_1    nom_2  \\\n0           0      0      0     T     Y  Green   Triangle    Snake   \n1           0      1      0     T     Y  Green  Trapezoid  Hamster   \n2           0      0      0     F     Y   Blue  Trapezoid     Lion   \n3           0      1      0     F     Y    Red  Trapezoid    Snake   \n4           0      0      0     F     N    Red  Trapezoid     Lion   \n...       ...    ...    ...   ...   ...    ...        ...      ...   \n299995      0      0      0     T     N    Red  Trapezoid    Snake   \n299996      0      0      0     F     Y  Green  Trapezoid     Lion   \n299997      0      0      0     F     Y   Blue       Star  Axolotl   \n299998      0      1      0     F     Y  Green     Square  Axolotl   \n299999      0      0      0     F     Y   Blue  Trapezoid      Dog   \n\n             nom_3     nom_4  ...      nom_8      nom_9 ord_0        ord_1  \\\n0          Finland   Bassoon  ...  c389000ab  2f4cb3d51     2  Grandmaster   \n1           Russia     Piano  ...  4cd920251  f83c56c21     1  Grandmaster   \n2           Russia  Theremin  ...  de9c9f684  ae6800dd0     1       Expert   \n3           Canada      Oboe  ...  4ade6ab69  8270f0d71     1  Grandmaster   \n4           Canada      Oboe  ...  cb43ab175  b164b72a7     1  Grandmaster   \n...            ...       ...  ...        ...        ...   ...          ...   \n299995       India      Oboe  ...  7508f4ef1  e027decef     1  Contributor   \n299996      Russia     Piano  ...  397dd0274  80f1411c8     2       Novice   \n299997      Russia      Oboe  ...  5d7806f53  314dcc15b     3       Novice   \n299998  Costa Rica     Piano  ...  1f820c7ce  ab0ce192b     1       Master   \n299999      Russia   Bassoon  ...  c13d3ebdf  ad1af2b45     3  Contributor   \n\n              ord_2  ord_3 ord_4 ord_5 day month  \n0              Cold      h     D    kr   2     2  \n1               Hot      a     A    bF   7     8  \n2          Lava Hot      h     R    Jc   7     2  \n3       Boiling Hot      i     D    kW   2     1  \n4          Freezing      a     R    qP   7     8  \n...             ...    ...   ...   ...  ..   ...  \n299995     Freezing      k     K    dh   3     8  \n299996     Freezing      h     W    MO   3     2  \n299997  Boiling Hot      o     A    Bn   7     9  \n299998  Boiling Hot      h     W    uJ   3     8  \n299999     Freezing      i     R    tP   1     3  \n\n[300000 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>nom_0</th>\n      <th>nom_1</th>\n      <th>nom_2</th>\n      <th>nom_3</th>\n      <th>nom_4</th>\n      <th>...</th>\n      <th>nom_8</th>\n      <th>nom_9</th>\n      <th>ord_0</th>\n      <th>ord_1</th>\n      <th>ord_2</th>\n      <th>ord_3</th>\n      <th>ord_4</th>\n      <th>ord_5</th>\n      <th>day</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>T</td>\n      <td>Y</td>\n      <td>Green</td>\n      <td>Triangle</td>\n      <td>Snake</td>\n      <td>Finland</td>\n      <td>Bassoon</td>\n      <td>...</td>\n      <td>c389000ab</td>\n      <td>2f4cb3d51</td>\n      <td>2</td>\n      <td>Grandmaster</td>\n      <td>Cold</td>\n      <td>h</td>\n      <td>D</td>\n      <td>kr</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>T</td>\n      <td>Y</td>\n      <td>Green</td>\n      <td>Trapezoid</td>\n      <td>Hamster</td>\n      <td>Russia</td>\n      <td>Piano</td>\n      <td>...</td>\n      <td>4cd920251</td>\n      <td>f83c56c21</td>\n      <td>1</td>\n      <td>Grandmaster</td>\n      <td>Hot</td>\n      <td>a</td>\n      <td>A</td>\n      <td>bF</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Blue</td>\n      <td>Trapezoid</td>\n      <td>Lion</td>\n      <td>Russia</td>\n      <td>Theremin</td>\n      <td>...</td>\n      <td>de9c9f684</td>\n      <td>ae6800dd0</td>\n      <td>1</td>\n      <td>Expert</td>\n      <td>Lava Hot</td>\n      <td>h</td>\n      <td>R</td>\n      <td>Jc</td>\n      <td>7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Red</td>\n      <td>Trapezoid</td>\n      <td>Snake</td>\n      <td>Canada</td>\n      <td>Oboe</td>\n      <td>...</td>\n      <td>4ade6ab69</td>\n      <td>8270f0d71</td>\n      <td>1</td>\n      <td>Grandmaster</td>\n      <td>Boiling Hot</td>\n      <td>i</td>\n      <td>D</td>\n      <td>kW</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Red</td>\n      <td>Trapezoid</td>\n      <td>Lion</td>\n      <td>Canada</td>\n      <td>Oboe</td>\n      <td>...</td>\n      <td>cb43ab175</td>\n      <td>b164b72a7</td>\n      <td>1</td>\n      <td>Grandmaster</td>\n      <td>Freezing</td>\n      <td>a</td>\n      <td>R</td>\n      <td>qP</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>299995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>T</td>\n      <td>N</td>\n      <td>Red</td>\n      <td>Trapezoid</td>\n      <td>Snake</td>\n      <td>India</td>\n      <td>Oboe</td>\n      <td>...</td>\n      <td>7508f4ef1</td>\n      <td>e027decef</td>\n      <td>1</td>\n      <td>Contributor</td>\n      <td>Freezing</td>\n      <td>k</td>\n      <td>K</td>\n      <td>dh</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>299996</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Green</td>\n      <td>Trapezoid</td>\n      <td>Lion</td>\n      <td>Russia</td>\n      <td>Piano</td>\n      <td>...</td>\n      <td>397dd0274</td>\n      <td>80f1411c8</td>\n      <td>2</td>\n      <td>Novice</td>\n      <td>Freezing</td>\n      <td>h</td>\n      <td>W</td>\n      <td>MO</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>299997</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Blue</td>\n      <td>Star</td>\n      <td>Axolotl</td>\n      <td>Russia</td>\n      <td>Oboe</td>\n      <td>...</td>\n      <td>5d7806f53</td>\n      <td>314dcc15b</td>\n      <td>3</td>\n      <td>Novice</td>\n      <td>Boiling Hot</td>\n      <td>o</td>\n      <td>A</td>\n      <td>Bn</td>\n      <td>7</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>299998</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Green</td>\n      <td>Square</td>\n      <td>Axolotl</td>\n      <td>Costa Rica</td>\n      <td>Piano</td>\n      <td>...</td>\n      <td>1f820c7ce</td>\n      <td>ab0ce192b</td>\n      <td>1</td>\n      <td>Master</td>\n      <td>Boiling Hot</td>\n      <td>h</td>\n      <td>W</td>\n      <td>uJ</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>299999</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Blue</td>\n      <td>Trapezoid</td>\n      <td>Dog</td>\n      <td>Russia</td>\n      <td>Bassoon</td>\n      <td>...</td>\n      <td>c13d3ebdf</td>\n      <td>ad1af2b45</td>\n      <td>3</td>\n      <td>Contributor</td>\n      <td>Freezing</td>\n      <td>i</td>\n      <td>R</td>\n      <td>tP</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>300000 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "feature_list = list(train.columns) # you can custumize later."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:16:38.605908Z",
     "iopub.execute_input": "2023-06-26T12:16:38.606264Z",
     "iopub.status.idle": "2023-06-26T12:16:38.611142Z",
     "shell.execute_reply.started": "2023-06-26T12:16:38.606202Z",
     "shell.execute_reply": "2023-06-26T12:16:38.610153Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:25:28.315797Z",
     "start_time": "2023-06-27T09:25:28.308437Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### notation\n",
    "\n",
    "- $y$ and $y+$ — the total number of observations and the total number of positive observations (y=1);\n",
    "- $x_i$, $y_i$ — the i-th value of category and target;\n",
    "- $n$ and $n+$ — the number of observations and the number of positive observations (y=1) for a given value of a categorical column;\n",
    "- $a$ — a regularization hyperparameter (selected by a user), prior — an average value of the target."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Label Encoder (LE), Ordinary Encoder(OE)\n",
    "\n",
    "One of the most common encoding methods.\n",
    "\n",
    "An encoding method that converts categorical data into numbers. For M categories in one column\n",
    "will be one column with M numbers.\n",
    "\n",
    "The disadvantage is that the labels are ordered randomly (in the existing order of the data),\n",
    "which can add noise while assigning an unexpected order between labels. In other words, the data\n",
    "becomes ordinary (ordinal, ordered) data, which can lead to unintended consequences.\n",
    "\n",
    "The difference between LabelEncoder and OrdinalEncoder is that both do the same thing, however,\n",
    "LabelEncoder works on 1D tupple ```(n, )```, while OrdinalEncoder works on 2D array ``` (n_samples,\n",
    "m_columns)```. Historacaly, LabelEncoder was used to encode targer (hence the tuple).\n",
    "\n",
    "The code is very simple, and when you encode a specific column you can proceed as follows:\n",
    "\n",
    "``` python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label = LabelEncoder()\n",
    "\n",
    "train[column_name] = label.fit_transform(train[column_name])\n",
    "```\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "LE_encoder = OrdinalEncoder(feature_list)\n",
    "train_le = LE_encoder.fit_transform(train)\n",
    "test_le = LE_encoder.transform(test)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:31:39.258442Z",
     "iopub.execute_input": "2023-06-26T12:31:39.259181Z",
     "iopub.status.idle": "2023-06-26T12:31:44.312006Z",
     "shell.execute_reply.started": "2023-06-26T12:31:39.259111Z",
     "shell.execute_reply": "2023-06-26T12:31:44.310984Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:25:34.510697Z",
     "start_time": "2023-06-27T09:25:31.706777Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.31 s, sys: 242 ms, total: 2.56 s\n",
      "Wall time: 2.8 s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(300000, 23)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_le.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:25:34.516212Z",
     "start_time": "2023-06-27T09:25:34.513068Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_le"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:31:53.134601Z",
     "iopub.execute_input": "2023-06-26T12:31:53.135088Z",
     "iopub.status.idle": "2023-06-26T12:31:53.419230Z",
     "shell.execute_reply.started": "2023-06-26T12:31:53.135040Z",
     "shell.execute_reply": "2023-06-26T12:31:53.417294Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:25:34.559308Z",
     "start_time": "2023-06-27T09:25:34.527865Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "        bin_0  bin_1  bin_2  bin_3  bin_4  nom_0  nom_1  nom_2  nom_3  nom_4  \\\n0           0      0      0      1      1      1      1      1      1      1   \n1           0      1      0      1      1      1      2      2      2      2   \n2           0      0      0      2      1      2      2      3      2      3   \n3           0      1      0      2      1      3      2      1      3      4   \n4           0      0      0      2      2      3      2      3      3      4   \n...       ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n299995      0      0      0      1      2      3      2      1      6      4   \n299996      0      0      0      2      1      1      2      3      2      2   \n299997      0      0      0      2      1      2      5      6      2      4   \n299998      0      1      0      2      1      1      4      6      4      2   \n299999      0      0      0      2      1      2      2      5      2      1   \n\n        ...  nom_8  nom_9  ord_0  ord_1  ord_2  ord_3  ord_4  ord_5  day  \\\n0       ...      1      1      2      1      1      1      1      1    2   \n1       ...      2      2      1      1      2      2      2      2    7   \n2       ...      3      3      1      2      3      1      3      3    7   \n3       ...      4      4      1      1      4      3      1      4    2   \n4       ...      5      5      1      1      5      2      3      5    7   \n...     ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n299995  ...   2110   5077      1      4      5      9      6     27    3   \n299996  ...   1288   7421      2      3      5      1     21    150    3   \n299997  ...     44   4958      3      3      4     13      2     75    7   \n299998  ...    792   9383      1      5      4      1     21    146    3   \n299999  ...   1093    955      3      4      5      3      3    100    1   \n\n        month  \n0           2  \n1           8  \n2           2  \n3           1  \n4           8  \n...       ...  \n299995      8  \n299996      2  \n299997      9  \n299998      8  \n299999      3  \n\n[300000 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>nom_0</th>\n      <th>nom_1</th>\n      <th>nom_2</th>\n      <th>nom_3</th>\n      <th>nom_4</th>\n      <th>...</th>\n      <th>nom_8</th>\n      <th>nom_9</th>\n      <th>ord_0</th>\n      <th>ord_1</th>\n      <th>ord_2</th>\n      <th>ord_3</th>\n      <th>ord_4</th>\n      <th>ord_5</th>\n      <th>day</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>299995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1</td>\n      <td>6</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2110</td>\n      <td>5077</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5</td>\n      <td>9</td>\n      <td>6</td>\n      <td>27</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>299996</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1288</td>\n      <td>7421</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>21</td>\n      <td>150</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>299997</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n      <td>6</td>\n      <td>2</td>\n      <td>4</td>\n      <td>...</td>\n      <td>44</td>\n      <td>4958</td>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>13</td>\n      <td>2</td>\n      <td>75</td>\n      <td>7</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>299998</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>2</td>\n      <td>...</td>\n      <td>792</td>\n      <td>9383</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>21</td>\n      <td>146</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>299999</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1093</td>\n      <td>955</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>100</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>300000 rows × 23 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. One-Hot Encoder (OHE, dummy encoder)\n",
    "\n",
    "\n",
    "So what can you do to give values by category instead of ordering them?\n",
    "\n",
    "If you have data with specific category values, you can create a column. For M categories OHE\n",
    "creates M columns with zeroes everywhere except for the row with given category.\n",
    "\n",
    "Since only the row containing the content is given as 1, it is called one-hot encoding. Also\n",
    "called dummy encoding in the sense of creating a dummy. There is alternative, that you can drop\n",
    "the first category (or the most frequent) to reduce the dimensionality. Some models cannot cope\n",
    "with full N columns which are basically collinear.\n",
    "\n",
    "Using pandas for this type of encoding since sklearn had problems with memory.\n",
    "\n",
    "``` python\n",
    "traintest = pd.concat([train, test])\n",
    "dummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\n",
    "train_ohe = dummies.iloc[:train.shape[0], :]\n",
    "test_ohe = dummies.iloc[train.shape[0]:, :]\n",
    "train_ohe = train_ohe.sparse.to_coo().tocsr()\n",
    "test_ohe = test_ohe.sparse.to_coo().tocsr()\n",
    "```\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %%time\n",
    "# this method didn't work because of RAM memory.\n",
    "# so we have to use pd.dummies\n",
    "# OHE_encoder = OneHotEncoder(feature_list)\n",
    "# train_ohe = OHE_encoder.fit_transform(train)\n",
    "# test_ohe = OHE_encoder.transform(test)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-27T09:13:18.121371Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.41 s, sys: 240 ms, total: 4.65 s\n",
      "Wall time: 4.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "traintest = pd.concat([train, test])\n",
    "dummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\n",
    "train_ohe = dummies.iloc[:train.shape[0], :]\n",
    "test_ohe = dummies.iloc[train.shape[0]:, :]\n",
    "train_ohe = train_ohe.sparse.to_coo().tocsr()\n",
    "test_ohe = test_ohe.sparse.to_coo().tocsr()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:25:40.927188Z",
     "start_time": "2023-06-27T09:25:36.255611Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "        bin_0_1  bin_1_1  bin_2_1  bin_3_T  bin_4_Y  nom_0_Green  nom_0_Red  \\\n0             0        0        0        1        1            1          0   \n1             0        1        0        1        1            1          0   \n2             0        0        0        0        1            0          0   \n3             0        1        0        0        1            0          1   \n4             0        0        0        0        0            0          1   \n...         ...      ...      ...      ...      ...          ...        ...   \n299995        0        0        0        1        0            0          1   \n299996        0        0        0        0        1            1          0   \n299997        0        0        0        0        1            0          0   \n299998        0        1        0        0        1            1          0   \n299999        0        0        0        0        1            0          0   \n\n        nom_1_Polygon  nom_1_Square  nom_1_Star  ...  month_3  month_4  \\\n0                   0             0           0  ...        0        0   \n1                   0             0           0  ...        0        0   \n2                   0             0           0  ...        0        0   \n3                   0             0           0  ...        0        0   \n4                   0             0           0  ...        0        0   \n...               ...           ...         ...  ...      ...      ...   \n299995              0             0           0  ...        0        0   \n299996              0             0           0  ...        0        0   \n299997              0             0           1  ...        0        0   \n299998              0             1           0  ...        0        0   \n299999              0             0           0  ...        1        0   \n\n        month_5  month_6  month_7  month_8  month_9  month_10  month_11  \\\n0             0        0        0        0        0         0         0   \n1             0        0        0        1        0         0         0   \n2             0        0        0        0        0         0         0   \n3             0        0        0        0        0         0         0   \n4             0        0        0        1        0         0         0   \n...         ...      ...      ...      ...      ...       ...       ...   \n299995        0        0        0        1        0         0         0   \n299996        0        0        0        0        0         0         0   \n299997        0        0        0        0        1         0         0   \n299998        0        0        0        1        0         0         0   \n299999        0        0        0        0        0         0         0   \n\n        month_12  \n0              0  \n1              0  \n2              0  \n3              0  \n4              0  \n...          ...  \n299995         0  \n299996         0  \n299997         0  \n299998         0  \n299999         0  \n\n[300000 rows x 16529 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0_1</th>\n      <th>bin_1_1</th>\n      <th>bin_2_1</th>\n      <th>bin_3_T</th>\n      <th>bin_4_Y</th>\n      <th>nom_0_Green</th>\n      <th>nom_0_Red</th>\n      <th>nom_1_Polygon</th>\n      <th>nom_1_Square</th>\n      <th>nom_1_Star</th>\n      <th>...</th>\n      <th>month_3</th>\n      <th>month_4</th>\n      <th>month_5</th>\n      <th>month_6</th>\n      <th>month_7</th>\n      <th>month_8</th>\n      <th>month_9</th>\n      <th>month_10</th>\n      <th>month_11</th>\n      <th>month_12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>299995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>299996</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>299997</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>299998</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>299999</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>300000 rows × 16529 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies.iloc[:train.shape[0], :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T09:26:09.788550Z",
     "start_time": "2023-06-27T09:26:09.225798Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Sum Encoder (Deviation Encoder, Effect Encoder)\n",
    "\n",
    "**Sum Encoder** compares the mean of the dependent variable (target) for a given level of a categorical column to the overall mean of the target. \n",
    "\n",
    "Sum Encoding is very similar to OHE and both of them are commonly used in Linear Regression (LR) types of models.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# %%time\n",
    "# this method didn't work because of RAM memory. \n",
    "# SE_encoder =SumEncoder(feature_list)\n",
    "# train_se = SE_encoder.fit_transform(train[feature_list], target)\n",
    "# test_se = SE_encoder.transform(test[feature_list])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Helmert Encoder\n",
    "\n",
    "**Helmert Encoding** is a third commonly used type of categorical encoding for regression along with OHE and Sum Encoding. \n",
    "\n",
    "It compares each level of a categorical variable to the mean of the subsequent levels. \n",
    "\n",
    "This type of encoding can be useful in certain situations where levels of the categorical variable are ordered. (not this dataset)\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# %%time\n",
    "# this method didn't work because of RAM memory. \n",
    "# HE_encoder = HelmertEncoder(feature_list)\n",
    "# train_he = HE_encoder.fit_transform(train[feature_list], target)\n",
    "# test_he = HE_encoder.transform(test[feature_list])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Frequency Encoder\n",
    "\n",
    "This method encodes by frequency.\n",
    "\n",
    "Create a new feature with the number of categories from the training data.\n",
    "\n",
    "I will not proceed separately in this data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Target Encoder\n",
    "\n",
    "This is a work in progress for many kernels.\n",
    "\n",
    "The encoded category values are calculated according to the following formulas:\n",
    "\n",
    "$$s = \\frac{1}{1+exp(-\\frac{n-mdl}{a})}$$\n",
    "\n",
    "$$\\hat{x}^k = prior * (1-s) + s * \\frac{n^{+}}{n}$$\n",
    "\n",
    "- mdl means **'min data in leaf'**\n",
    "- a means **'smooth parameter, power of regularization'**\n",
    "\n",
    "Target Encoder is a powerful, but it has a huuuuuge disadvantage \n",
    "\n",
    "> **target leakage**: it uses information about the target. \n",
    "\n",
    "To reduce the effect of target leakage, \n",
    "\n",
    "- Increase regularization\n",
    "- Add random noise to the representation of the category in train dataset (some sort of augmentation)\n",
    "- Use Double Validation (using other validation)\n",
    "\n",
    "Let's use while being careful about overfitting.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "TE_encoder = TargetEncoder()\n",
    "train_te = TE_encoder.fit_transform(train[feature_list], target)\n",
    "test_te = TE_encoder.transform(test[feature_list])\n",
    "\n",
    "train_te.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:40:41.582068Z",
     "iopub.execute_input": "2023-06-26T12:40:41.582442Z",
     "iopub.status.idle": "2023-06-26T12:40:51.803648Z",
     "shell.execute_reply.started": "2023-06-26T12:40:41.582389Z",
     "shell.execute_reply": "2023-06-26T12:40:51.802601Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 10.2 s, sys: 1.67 s, total: 11.9 s\nWall time: 10.2 s\n",
     "output_type": "stream"
    },
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "   bin_0  bin_1  bin_2     bin_3     bin_4     nom_0     nom_1     nom_2  \\\n0      0      0      0  0.302537  0.290107  0.327145  0.360978  0.307162   \n1      0      1      0  0.302537  0.290107  0.327145  0.290054  0.359209   \n2      0      0      0  0.309384  0.290107  0.241790  0.290054  0.293085   \n3      0      1      0  0.309384  0.290107  0.351052  0.290054  0.307162   \n4      0      0      0  0.309384  0.333773  0.351052  0.290054  0.293085   \n\n      nom_3     nom_4  ...     nom_8     nom_9  ord_0     ord_1     ord_2  \\\n0  0.242813  0.237743  ...  0.372694  0.368421      2  0.403885  0.257877   \n1  0.289954  0.304164  ...  0.189189  0.076924      1  0.403885  0.326315   \n2  0.289954  0.353951  ...  0.223022  0.172414      1  0.317175  0.403126   \n3  0.339793  0.329472  ...  0.325123  0.227273      1  0.403885  0.360961   \n4  0.339793  0.329472  ...  0.376812  0.200000      1  0.403885  0.225214   \n\n      ord_3     ord_4     ord_5  day  month  \n0  0.306993  0.208354  0.401186    2      2  \n1  0.206599  0.186877  0.303880    7      8  \n2  0.306993  0.351864  0.206843    7      2  \n3  0.330148  0.208354  0.355985    2      1  \n4  0.206599  0.351864  0.404345    7      8  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>nom_0</th>\n      <th>nom_1</th>\n      <th>nom_2</th>\n      <th>nom_3</th>\n      <th>nom_4</th>\n      <th>...</th>\n      <th>nom_8</th>\n      <th>nom_9</th>\n      <th>ord_0</th>\n      <th>ord_1</th>\n      <th>ord_2</th>\n      <th>ord_3</th>\n      <th>ord_4</th>\n      <th>ord_5</th>\n      <th>day</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.302537</td>\n      <td>0.290107</td>\n      <td>0.327145</td>\n      <td>0.360978</td>\n      <td>0.307162</td>\n      <td>0.242813</td>\n      <td>0.237743</td>\n      <td>...</td>\n      <td>0.372694</td>\n      <td>0.368421</td>\n      <td>2</td>\n      <td>0.403885</td>\n      <td>0.257877</td>\n      <td>0.306993</td>\n      <td>0.208354</td>\n      <td>0.401186</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.302537</td>\n      <td>0.290107</td>\n      <td>0.327145</td>\n      <td>0.290054</td>\n      <td>0.359209</td>\n      <td>0.289954</td>\n      <td>0.304164</td>\n      <td>...</td>\n      <td>0.189189</td>\n      <td>0.076924</td>\n      <td>1</td>\n      <td>0.403885</td>\n      <td>0.326315</td>\n      <td>0.206599</td>\n      <td>0.186877</td>\n      <td>0.303880</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.241790</td>\n      <td>0.290054</td>\n      <td>0.293085</td>\n      <td>0.289954</td>\n      <td>0.353951</td>\n      <td>...</td>\n      <td>0.223022</td>\n      <td>0.172414</td>\n      <td>1</td>\n      <td>0.317175</td>\n      <td>0.403126</td>\n      <td>0.306993</td>\n      <td>0.351864</td>\n      <td>0.206843</td>\n      <td>7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.351052</td>\n      <td>0.290054</td>\n      <td>0.307162</td>\n      <td>0.339793</td>\n      <td>0.329472</td>\n      <td>...</td>\n      <td>0.325123</td>\n      <td>0.227273</td>\n      <td>1</td>\n      <td>0.403885</td>\n      <td>0.360961</td>\n      <td>0.330148</td>\n      <td>0.208354</td>\n      <td>0.355985</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.333773</td>\n      <td>0.351052</td>\n      <td>0.290054</td>\n      <td>0.293085</td>\n      <td>0.339793</td>\n      <td>0.329472</td>\n      <td>...</td>\n      <td>0.376812</td>\n      <td>0.200000</td>\n      <td>1</td>\n      <td>0.403885</td>\n      <td>0.225214</td>\n      <td>0.206599</td>\n      <td>0.351864</td>\n      <td>0.404345</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. M-Estimate Encoder\n",
    "\n",
    "**M-Estimate Encoder** is a **simplified version of Target Encoder**. It has only one hyperparameter (Wrong Fomular but did good work?!)\n",
    "\n",
    "$$\\hat{x}^k = \\frac{n^+ + prior * m}{y^+ + m}$$\n",
    "\n",
    "The higher value of m results into stronger shrinking. Recommended values for m is in the range of 1 to 100.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "MEE_encoder = MEstimateEncoder()\n",
    "train_mee = MEE_encoder.fit_transform(train[feature_list], target)\n",
    "test_mee = MEE_encoder.transform(test[feature_list])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:41:37.779881Z",
     "iopub.execute_input": "2023-06-26T12:41:37.780202Z",
     "iopub.status.idle": "2023-06-26T12:41:47.299274Z",
     "shell.execute_reply.started": "2023-06-26T12:41:37.780162Z",
     "shell.execute_reply": "2023-06-26T12:41:47.298191Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 9.54 s, sys: 1.11 s, total: 10.7 s\nWall time: 9.51 s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_mee.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:41:47.302166Z",
     "iopub.execute_input": "2023-06-26T12:41:47.302621Z",
     "iopub.status.idle": "2023-06-26T12:41:47.335220Z",
     "shell.execute_reply.started": "2023-06-26T12:41:47.302529Z",
     "shell.execute_reply": "2023-06-26T12:41:47.333962Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "   bin_0  bin_1  bin_2     bin_3     bin_4     nom_0     nom_1     nom_2  \\\n0      0      0      0  0.302537  0.290107  0.327145  0.360976  0.307162   \n1      0      1      0  0.302537  0.290107  0.327145  0.290055  0.359207   \n2      0      0      0  0.309384  0.290107  0.241791  0.290055  0.293085   \n3      0      1      0  0.309384  0.290107  0.351051  0.290055  0.307162   \n4      0      0      0  0.309384  0.333773  0.351051  0.290055  0.293085   \n\n      nom_3     nom_4  ...     nom_8     nom_9  ord_0     ord_1     ord_2  \\\n0  0.242815  0.237744  ...  0.372448  0.365294      2  0.403884  0.257879   \n1  0.289954  0.304164  ...  0.190231  0.093277      1  0.403884  0.326314   \n2  0.289954  0.353950  ...  0.223319  0.176863      1  0.317175  0.403125   \n3  0.339792  0.329472  ...  0.325029  0.229020      1  0.403884  0.360960   \n4  0.339792  0.329472  ...  0.376471  0.202941      1  0.403884  0.225215   \n\n      ord_3     ord_4     ord_5  day  month  \n0  0.306993  0.208379  0.400998    2      2  \n1  0.206602  0.186884  0.303881    7      8  \n2  0.306993  0.351861  0.206881    7      2  \n3  0.330147  0.208379  0.355965    2      1  \n4  0.206602  0.351861  0.404310    7      8  \n\n[5 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>nom_0</th>\n      <th>nom_1</th>\n      <th>nom_2</th>\n      <th>nom_3</th>\n      <th>nom_4</th>\n      <th>...</th>\n      <th>nom_8</th>\n      <th>nom_9</th>\n      <th>ord_0</th>\n      <th>ord_1</th>\n      <th>ord_2</th>\n      <th>ord_3</th>\n      <th>ord_4</th>\n      <th>ord_5</th>\n      <th>day</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.302537</td>\n      <td>0.290107</td>\n      <td>0.327145</td>\n      <td>0.360976</td>\n      <td>0.307162</td>\n      <td>0.242815</td>\n      <td>0.237744</td>\n      <td>...</td>\n      <td>0.372448</td>\n      <td>0.365294</td>\n      <td>2</td>\n      <td>0.403884</td>\n      <td>0.257879</td>\n      <td>0.306993</td>\n      <td>0.208379</td>\n      <td>0.400998</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.302537</td>\n      <td>0.290107</td>\n      <td>0.327145</td>\n      <td>0.290055</td>\n      <td>0.359207</td>\n      <td>0.289954</td>\n      <td>0.304164</td>\n      <td>...</td>\n      <td>0.190231</td>\n      <td>0.093277</td>\n      <td>1</td>\n      <td>0.403884</td>\n      <td>0.326314</td>\n      <td>0.206602</td>\n      <td>0.186884</td>\n      <td>0.303881</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.241791</td>\n      <td>0.290055</td>\n      <td>0.293085</td>\n      <td>0.289954</td>\n      <td>0.353950</td>\n      <td>...</td>\n      <td>0.223319</td>\n      <td>0.176863</td>\n      <td>1</td>\n      <td>0.317175</td>\n      <td>0.403125</td>\n      <td>0.306993</td>\n      <td>0.351861</td>\n      <td>0.206881</td>\n      <td>7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.351051</td>\n      <td>0.290055</td>\n      <td>0.307162</td>\n      <td>0.339792</td>\n      <td>0.329472</td>\n      <td>...</td>\n      <td>0.325029</td>\n      <td>0.229020</td>\n      <td>1</td>\n      <td>0.403884</td>\n      <td>0.360960</td>\n      <td>0.330147</td>\n      <td>0.208379</td>\n      <td>0.355965</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.333773</td>\n      <td>0.351051</td>\n      <td>0.290055</td>\n      <td>0.293085</td>\n      <td>0.339792</td>\n      <td>0.329472</td>\n      <td>...</td>\n      <td>0.376471</td>\n      <td>0.202941</td>\n      <td>1</td>\n      <td>0.403884</td>\n      <td>0.225215</td>\n      <td>0.206602</td>\n      <td>0.351861</td>\n      <td>0.404310</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- UPDATED : error founded in libarary https://github.com/scikit-learn-contrib/category_encoders/issues/200\n",
    "\n",
    "$$\\hat{x}^k = \\frac{n^+ + prior * m}{n+ + m}$$\n",
    "\n",
    "Thanks to [@ansh422](https://www.kaggle.com/ansh422)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Weight of Evidence Encoder \n",
    "\n",
    "**Weight Of Evidence** is a commonly used target-based encoder in credit scoring. \n",
    "\n",
    "It is a measure of the “strength” of a grouping for separating good and bad risk (default). \n",
    "\n",
    "It is calculated from the basic odds ratio:\n",
    "\n",
    "``` python\n",
    "a = Distribution of Good Credit Outcomes\n",
    "b = Distribution of Bad Credit Outcomes\n",
    "WoE = ln(a / b)\n",
    "```\n",
    "\n",
    "However, if we use formulas as is, it might lead to **target leakage**(and overfit).\n",
    "\n",
    "To avoid that, regularization parameter a is induced and WoE is calculated in the following way:\n",
    "\n",
    "$$nomiinator = \\frac{n^+ + a}{y^+ + 2*a}$$\n",
    "\n",
    "$$denominator = ln(\\frac{nominator}{denominator})$$\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "WOE_encoder = WOEEncoder()\n",
    "train_woe = WOE_encoder.fit_transform(train[feature_list], target)\n",
    "test_woe = WOE_encoder.transform(test[feature_list])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:48:44.960027Z",
     "iopub.execute_input": "2023-06-26T12:48:44.960367Z",
     "iopub.status.idle": "2023-06-26T12:48:54.562514Z",
     "shell.execute_reply.started": "2023-06-26T12:48:44.960326Z",
     "shell.execute_reply": "2023-06-26T12:48:54.561365Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 9.59 s, sys: 1.16 s, total: 10.8 s\nWall time: 9.6 s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_woe"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:49:03.736084Z",
     "iopub.execute_input": "2023-06-26T12:49:03.736707Z",
     "iopub.status.idle": "2023-06-26T12:49:03.882929Z",
     "shell.execute_reply.started": "2023-06-26T12:49:03.736638Z",
     "shell.execute_reply": "2023-06-26T12:49:03.881994Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "execution_count": 13,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        bin_0  bin_1  bin_2     bin_3     bin_4     nom_0     nom_1     nom_2  \\\n0           0      0      0 -0.015794 -0.075416  0.098327  0.248359  0.006058   \n1           0      1      0 -0.015794 -0.075416  0.098327 -0.075660  0.240683   \n2           0      0      0  0.016454 -0.075416 -0.323420 -0.075660 -0.060990   \n3           0      1      0  0.016454 -0.075416  0.205037 -0.075660  0.006058   \n4           0      0      0  0.016454  0.128285  0.205037 -0.075660 -0.060990   \n...       ...    ...    ...       ...       ...       ...       ...       ...   \n299995      0      0      0 -0.015794  0.128285  0.205037 -0.075660  0.006058   \n299996      0      0      0  0.016454 -0.075416  0.098327 -0.075660 -0.060990   \n299997      0      0      0  0.016454 -0.075416 -0.323420  0.022286  0.061193   \n299998      0      1      0  0.016454 -0.075416  0.098327  0.151411  0.061193   \n299999      0      0      0  0.016454 -0.075416 -0.323420 -0.075660 -0.305193   \n\n           nom_3     nom_4  ...     nom_8     nom_9  ord_0     ord_1  \\\n0      -0.317803 -0.345614  ...  0.302749  0.333932      2  0.430146   \n1      -0.076148 -0.008087  ... -0.600377 -1.052362      1  0.430146   \n2      -0.076148  0.217747  ... -0.417323 -0.607677      1  0.052724   \n3       0.155252  0.108884  ...  0.096879 -0.338013      1  0.430146   \n4       0.155252  0.108884  ...  0.321353 -0.468414      1  0.430146   \n...          ...       ...  ...       ...       ...    ...       ...   \n299995  0.249803  0.108884  ... -0.192161  0.190831      1 -0.132258   \n299996 -0.076148 -0.008087  ... -0.076648 -0.502316      2 -0.321986   \n299997 -0.076148  0.108884  ...  0.146495  0.030982      3 -0.321986   \n299998  0.041196 -0.008087  ... -0.125022  0.288812      1  0.222693   \n299999 -0.076148 -0.345614  ... -0.594254  0.899483      3 -0.132258   \n\n           ord_2     ord_3     ord_4     ord_5  day  month  \n0      -0.237516  0.005297 -0.514545  0.420532    2      2  \n1       0.094611 -0.526005 -0.650766 -0.008737    7      8  \n2       0.426997  0.005297  0.208660 -0.523234    7      2  \n3       0.248265  0.111980 -0.514545  0.227089    2      1  \n4      -0.416062 -0.526005  0.208660  0.432324    7      8  \n...          ...       ...       ...       ...  ...    ...  \n299995 -0.416062  0.266667 -0.195294  0.033989    3      8  \n299996 -0.416062  0.005297  0.453411 -0.415855    3      2  \n299997  0.248265  0.552543 -0.650766 -0.705175    7      9  \n299998  0.248265  0.005297  0.453411  0.572812    3      8  \n299999 -0.416062  0.111980  0.208660  0.487578    1      3  \n\n[300000 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>nom_0</th>\n      <th>nom_1</th>\n      <th>nom_2</th>\n      <th>nom_3</th>\n      <th>nom_4</th>\n      <th>...</th>\n      <th>nom_8</th>\n      <th>nom_9</th>\n      <th>ord_0</th>\n      <th>ord_1</th>\n      <th>ord_2</th>\n      <th>ord_3</th>\n      <th>ord_4</th>\n      <th>ord_5</th>\n      <th>day</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.015794</td>\n      <td>-0.075416</td>\n      <td>0.098327</td>\n      <td>0.248359</td>\n      <td>0.006058</td>\n      <td>-0.317803</td>\n      <td>-0.345614</td>\n      <td>...</td>\n      <td>0.302749</td>\n      <td>0.333932</td>\n      <td>2</td>\n      <td>0.430146</td>\n      <td>-0.237516</td>\n      <td>0.005297</td>\n      <td>-0.514545</td>\n      <td>0.420532</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.015794</td>\n      <td>-0.075416</td>\n      <td>0.098327</td>\n      <td>-0.075660</td>\n      <td>0.240683</td>\n      <td>-0.076148</td>\n      <td>-0.008087</td>\n      <td>...</td>\n      <td>-0.600377</td>\n      <td>-1.052362</td>\n      <td>1</td>\n      <td>0.430146</td>\n      <td>0.094611</td>\n      <td>-0.526005</td>\n      <td>-0.650766</td>\n      <td>-0.008737</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.016454</td>\n      <td>-0.075416</td>\n      <td>-0.323420</td>\n      <td>-0.075660</td>\n      <td>-0.060990</td>\n      <td>-0.076148</td>\n      <td>0.217747</td>\n      <td>...</td>\n      <td>-0.417323</td>\n      <td>-0.607677</td>\n      <td>1</td>\n      <td>0.052724</td>\n      <td>0.426997</td>\n      <td>0.005297</td>\n      <td>0.208660</td>\n      <td>-0.523234</td>\n      <td>7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.016454</td>\n      <td>-0.075416</td>\n      <td>0.205037</td>\n      <td>-0.075660</td>\n      <td>0.006058</td>\n      <td>0.155252</td>\n      <td>0.108884</td>\n      <td>...</td>\n      <td>0.096879</td>\n      <td>-0.338013</td>\n      <td>1</td>\n      <td>0.430146</td>\n      <td>0.248265</td>\n      <td>0.111980</td>\n      <td>-0.514545</td>\n      <td>0.227089</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.016454</td>\n      <td>0.128285</td>\n      <td>0.205037</td>\n      <td>-0.075660</td>\n      <td>-0.060990</td>\n      <td>0.155252</td>\n      <td>0.108884</td>\n      <td>...</td>\n      <td>0.321353</td>\n      <td>-0.468414</td>\n      <td>1</td>\n      <td>0.430146</td>\n      <td>-0.416062</td>\n      <td>-0.526005</td>\n      <td>0.208660</td>\n      <td>0.432324</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>299995</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.015794</td>\n      <td>0.128285</td>\n      <td>0.205037</td>\n      <td>-0.075660</td>\n      <td>0.006058</td>\n      <td>0.249803</td>\n      <td>0.108884</td>\n      <td>...</td>\n      <td>-0.192161</td>\n      <td>0.190831</td>\n      <td>1</td>\n      <td>-0.132258</td>\n      <td>-0.416062</td>\n      <td>0.266667</td>\n      <td>-0.195294</td>\n      <td>0.033989</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>299996</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.016454</td>\n      <td>-0.075416</td>\n      <td>0.098327</td>\n      <td>-0.075660</td>\n      <td>-0.060990</td>\n      <td>-0.076148</td>\n      <td>-0.008087</td>\n      <td>...</td>\n      <td>-0.076648</td>\n      <td>-0.502316</td>\n      <td>2</td>\n      <td>-0.321986</td>\n      <td>-0.416062</td>\n      <td>0.005297</td>\n      <td>0.453411</td>\n      <td>-0.415855</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>299997</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.016454</td>\n      <td>-0.075416</td>\n      <td>-0.323420</td>\n      <td>0.022286</td>\n      <td>0.061193</td>\n      <td>-0.076148</td>\n      <td>0.108884</td>\n      <td>...</td>\n      <td>0.146495</td>\n      <td>0.030982</td>\n      <td>3</td>\n      <td>-0.321986</td>\n      <td>0.248265</td>\n      <td>0.552543</td>\n      <td>-0.650766</td>\n      <td>-0.705175</td>\n      <td>7</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <td>299998</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.016454</td>\n      <td>-0.075416</td>\n      <td>0.098327</td>\n      <td>0.151411</td>\n      <td>0.061193</td>\n      <td>0.041196</td>\n      <td>-0.008087</td>\n      <td>...</td>\n      <td>-0.125022</td>\n      <td>0.288812</td>\n      <td>1</td>\n      <td>0.222693</td>\n      <td>0.248265</td>\n      <td>0.005297</td>\n      <td>0.453411</td>\n      <td>0.572812</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>299999</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.016454</td>\n      <td>-0.075416</td>\n      <td>-0.323420</td>\n      <td>-0.075660</td>\n      <td>-0.305193</td>\n      <td>-0.076148</td>\n      <td>-0.345614</td>\n      <td>...</td>\n      <td>-0.594254</td>\n      <td>0.899483</td>\n      <td>3</td>\n      <td>-0.132258</td>\n      <td>-0.416062</td>\n      <td>0.111980</td>\n      <td>0.208660</td>\n      <td>0.487578</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>300000 rows × 23 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. James-Stein Encoder\n",
    "\n",
    "**James-Stein Encoder** is a target-based encoder.\n",
    "\n",
    "The idea behind James-Stein Encoder is simple. Estimation of the mean target for category k could be calculated according to the following formula:\n",
    "\n",
    "$$\\hat{x}^k = (1-B) * \\frac{n^+}{n} + B * \\frac{y^+}{y} $$\n",
    "\n",
    "One way to select B is to tune it like a hyperparameter via cross-validation, but Charles Stein came up with another solution to the problem:\n",
    "\n",
    "$$B = \\frac{Var[y^k]}{Var[y^k] + Var[y]}$$\n",
    "\n",
    "Seems quite fair, but James-Stein Estimator has a big disadvantage — it is defined only for normal distribution (which is not the case for any classification task). \n",
    "\n",
    "To avoid that, we can either convert binary targets with a log-odds ratio as it was done in WoE Encoder (which is used by default because it is simple) or use beta distribution.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "JSE_encoder = JamesSteinEncoder()\n",
    "train_jse = JSE_encoder.fit_transform(train[feature_list], target)\n",
    "test_jse = JSE_encoder.transform(test[feature_list])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:50:02.767978Z",
     "iopub.execute_input": "2023-06-26T12:50:02.768345Z",
     "iopub.status.idle": "2023-06-26T12:50:12.426390Z",
     "shell.execute_reply.started": "2023-06-26T12:50:02.768290Z",
     "shell.execute_reply": "2023-06-26T12:50:12.425317Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 9.64 s, sys: 1.07 s, total: 10.7 s\nWall time: 9.65 s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_jse"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:50:12.428290Z",
     "iopub.execute_input": "2023-06-26T12:50:12.428930Z",
     "iopub.status.idle": "2023-06-26T12:50:12.565713Z",
     "shell.execute_reply.started": "2023-06-26T12:50:12.428867Z",
     "shell.execute_reply": "2023-06-26T12:50:12.564704Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "execution_count": 15,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        bin_0  bin_1  bin_2     bin_3     bin_4     nom_0     nom_1     nom_2  \\\n0           0      0      0  0.302537  0.290107  0.327145  0.343763  0.306777   \n1           0      1      0  0.302537  0.290107  0.327145  0.294730  0.342564   \n2           0      0      0  0.309384  0.290107  0.241790  0.294730  0.296876   \n3           0      1      0  0.309384  0.290107  0.351052  0.294730  0.306777   \n4           0      0      0  0.309384  0.333773  0.351052  0.294730  0.296876   \n...       ...    ...    ...       ...       ...       ...       ...       ...   \n299995      0      0      0  0.302537  0.333773  0.351052  0.294730  0.306777   \n299996      0      0      0  0.309384  0.290107  0.327145  0.294730  0.296876   \n299997      0      0      0  0.309384  0.290107  0.241790  0.309196  0.315031   \n299998      0      1      0  0.309384  0.290107  0.327145  0.328749  0.315031   \n299999      0      0      0  0.309384  0.290107  0.241790  0.294730  0.262111   \n\n           nom_3     nom_4  ...     nom_8     nom_9  ord_0     ord_1  \\\n0       0.260374  0.248201  ...  0.337649  0.334882      2  0.377845   \n1       0.294658  0.304449  ...  0.238347  0.137804      1  0.377845   \n2       0.294658  0.345642  ...  0.260297  0.227178      1  0.314323   \n3       0.329339  0.325462  ...  0.315328  0.263300      1  0.377845   \n4       0.329339  0.325462  ...  0.339509  0.246247      1  0.377845   \n...          ...       ...  ...       ...       ...    ...       ...   \n299995  0.343989  0.325462  ...  0.279755  0.322701      1  0.285182   \n299996  0.294658  0.304449  ...  0.296697  0.242514      2  0.256848   \n299997  0.294658  0.325462  ...  0.320347  0.302973      3  0.256848   \n299998  0.312025  0.304449  ...  0.291684  0.331289      1  0.342313   \n299999  0.294658  0.248201  ...  0.233576  0.402755      3  0.285182   \n\n           ord_2     ord_3     ord_4     ord_5  day  month  \n0       0.271531  0.306515  0.247588  0.351077    2      2  \n1       0.320078  0.243675  0.232549  0.304868    7      8  \n2       0.372130  0.306515  0.329955  0.249570    7      2  \n3       0.343752  0.319536  0.247588  0.330239    2      1  \n4       0.247048  0.243675  0.329955  0.352552    7      8  \n...          ...       ...       ...       ...  ...    ...  \n299995  0.247048  0.338666  0.283590  0.309435    3      8  \n299996  0.247048  0.306515  0.358728  0.261030    3      2  \n299997  0.343752  0.374913  0.232549  0.229962    7      9  \n299998  0.343752  0.306515  0.358728  0.368007    3      8  \n299999  0.247048  0.319536  0.329955  0.358623    1      3  \n\n[300000 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>nom_0</th>\n      <th>nom_1</th>\n      <th>nom_2</th>\n      <th>nom_3</th>\n      <th>nom_4</th>\n      <th>...</th>\n      <th>nom_8</th>\n      <th>nom_9</th>\n      <th>ord_0</th>\n      <th>ord_1</th>\n      <th>ord_2</th>\n      <th>ord_3</th>\n      <th>ord_4</th>\n      <th>ord_5</th>\n      <th>day</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.302537</td>\n      <td>0.290107</td>\n      <td>0.327145</td>\n      <td>0.343763</td>\n      <td>0.306777</td>\n      <td>0.260374</td>\n      <td>0.248201</td>\n      <td>...</td>\n      <td>0.337649</td>\n      <td>0.334882</td>\n      <td>2</td>\n      <td>0.377845</td>\n      <td>0.271531</td>\n      <td>0.306515</td>\n      <td>0.247588</td>\n      <td>0.351077</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.302537</td>\n      <td>0.290107</td>\n      <td>0.327145</td>\n      <td>0.294730</td>\n      <td>0.342564</td>\n      <td>0.294658</td>\n      <td>0.304449</td>\n      <td>...</td>\n      <td>0.238347</td>\n      <td>0.137804</td>\n      <td>1</td>\n      <td>0.377845</td>\n      <td>0.320078</td>\n      <td>0.243675</td>\n      <td>0.232549</td>\n      <td>0.304868</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.241790</td>\n      <td>0.294730</td>\n      <td>0.296876</td>\n      <td>0.294658</td>\n      <td>0.345642</td>\n      <td>...</td>\n      <td>0.260297</td>\n      <td>0.227178</td>\n      <td>1</td>\n      <td>0.314323</td>\n      <td>0.372130</td>\n      <td>0.306515</td>\n      <td>0.329955</td>\n      <td>0.249570</td>\n      <td>7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.351052</td>\n      <td>0.294730</td>\n      <td>0.306777</td>\n      <td>0.329339</td>\n      <td>0.325462</td>\n      <td>...</td>\n      <td>0.315328</td>\n      <td>0.263300</td>\n      <td>1</td>\n      <td>0.377845</td>\n      <td>0.343752</td>\n      <td>0.319536</td>\n      <td>0.247588</td>\n      <td>0.330239</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.333773</td>\n      <td>0.351052</td>\n      <td>0.294730</td>\n      <td>0.296876</td>\n      <td>0.329339</td>\n      <td>0.325462</td>\n      <td>...</td>\n      <td>0.339509</td>\n      <td>0.246247</td>\n      <td>1</td>\n      <td>0.377845</td>\n      <td>0.247048</td>\n      <td>0.243675</td>\n      <td>0.329955</td>\n      <td>0.352552</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>299995</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.302537</td>\n      <td>0.333773</td>\n      <td>0.351052</td>\n      <td>0.294730</td>\n      <td>0.306777</td>\n      <td>0.343989</td>\n      <td>0.325462</td>\n      <td>...</td>\n      <td>0.279755</td>\n      <td>0.322701</td>\n      <td>1</td>\n      <td>0.285182</td>\n      <td>0.247048</td>\n      <td>0.338666</td>\n      <td>0.283590</td>\n      <td>0.309435</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>299996</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.327145</td>\n      <td>0.294730</td>\n      <td>0.296876</td>\n      <td>0.294658</td>\n      <td>0.304449</td>\n      <td>...</td>\n      <td>0.296697</td>\n      <td>0.242514</td>\n      <td>2</td>\n      <td>0.256848</td>\n      <td>0.247048</td>\n      <td>0.306515</td>\n      <td>0.358728</td>\n      <td>0.261030</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>299997</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.241790</td>\n      <td>0.309196</td>\n      <td>0.315031</td>\n      <td>0.294658</td>\n      <td>0.325462</td>\n      <td>...</td>\n      <td>0.320347</td>\n      <td>0.302973</td>\n      <td>3</td>\n      <td>0.256848</td>\n      <td>0.343752</td>\n      <td>0.374913</td>\n      <td>0.232549</td>\n      <td>0.229962</td>\n      <td>7</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <td>299998</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.327145</td>\n      <td>0.328749</td>\n      <td>0.315031</td>\n      <td>0.312025</td>\n      <td>0.304449</td>\n      <td>...</td>\n      <td>0.291684</td>\n      <td>0.331289</td>\n      <td>1</td>\n      <td>0.342313</td>\n      <td>0.343752</td>\n      <td>0.306515</td>\n      <td>0.358728</td>\n      <td>0.368007</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>299999</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309384</td>\n      <td>0.290107</td>\n      <td>0.241790</td>\n      <td>0.294730</td>\n      <td>0.262111</td>\n      <td>0.294658</td>\n      <td>0.248201</td>\n      <td>...</td>\n      <td>0.233576</td>\n      <td>0.402755</td>\n      <td>3</td>\n      <td>0.285182</td>\n      <td>0.247048</td>\n      <td>0.319536</td>\n      <td>0.329955</td>\n      <td>0.358623</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>300000 rows × 23 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Leave-one-out Encoder (LOO or LOOE)\n",
    "\n",
    "**Leave-one-out Encoding** is another example of target-based encoders.\n",
    "\n",
    "This encoder calculate mean target of category k for observation j if observation j is removed from the dataset:\n",
    "\n",
    "$$\\hat{x}^k_i = \\frac{\\sum_{j \\neq i}(y_j * (x_j == k) ) - y_i }{\\sum_{j \\neq i} x_j == k}$$\n",
    "\n",
    "While encoding the test dataset, a category is replaced with the mean target of the category k in the train dataset:\n",
    "\n",
    "$$\\hat{x}^k = \\frac{\\sum y_j * (x_j == k)  }{\\sum x_j == k}$$\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "LOOE_encoder = LeaveOneOutEncoder()\n",
    "train_looe = LOOE_encoder.fit_transform(train[feature_list], target)\n",
    "test_looe = LOOE_encoder.transform(test[feature_list])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:50:31.603035Z",
     "iopub.execute_input": "2023-06-26T12:50:31.603594Z",
     "iopub.status.idle": "2023-06-26T12:50:41.141790Z",
     "shell.execute_reply.started": "2023-06-26T12:50:31.603510Z",
     "shell.execute_reply": "2023-06-26T12:50:41.140573Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 9.96 s, sys: 1.06 s, total: 11 s\nWall time: 9.53 s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_looe"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:50:41.143296Z",
     "iopub.execute_input": "2023-06-26T12:50:41.143741Z",
     "iopub.status.idle": "2023-06-26T12:50:41.227417Z",
     "shell.execute_reply.started": "2023-06-26T12:50:41.143531Z",
     "shell.execute_reply": "2023-06-26T12:50:41.226439Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "execution_count": 17,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        bin_0  bin_1  bin_2     bin_3     bin_4     nom_0     nom_1     nom_2  \\\n0           0      0      0  0.302539  0.290108  0.327148  0.360990  0.307169   \n1           0      1      0  0.302539  0.290108  0.327148  0.290057  0.359221   \n2           0      0      0  0.309387  0.290108  0.241793  0.290057  0.293087   \n3           0      1      0  0.309380  0.290103  0.351043  0.290047  0.307147   \n4           0      0      0  0.309387  0.333776  0.351056  0.290057  0.293087   \n...       ...    ...    ...       ...       ...       ...       ...       ...   \n299995      0      0      0  0.302539  0.333776  0.351056  0.290057  0.307169   \n299996      0      0      0  0.309387  0.290108  0.327148  0.290057  0.293087   \n299997      0      0      0  0.309380  0.290103  0.241782  0.310612  0.318998   \n299998      0      1      0  0.309380  0.290103  0.327140  0.338918  0.318998   \n299999      0      0      0  0.309387  0.290108  0.241793  0.290057  0.245146   \n\n           nom_3     nom_4  ...     nom_8     nom_9  ord_0     ord_1  \\\n0       0.242820  0.237746  ...  0.374074  0.388889      2  0.403890   \n1       0.289957  0.304167  ...  0.190909  0.083333      1  0.403890   \n2       0.289957  0.353958  ...  0.223827  0.178571      1  0.317188   \n3       0.339780  0.329465  ...  0.321782  0.209302      1  0.403877   \n4       0.339800  0.329476  ...  0.378641  0.205882      1  0.403890   \n...          ...       ...  ...       ...       ...    ...       ...   \n299995  0.361323  0.329476  ...  0.261905  0.348837      1  0.278540   \n299996  0.289957  0.304167  ...  0.289216  0.200000      2  0.242057   \n299997  0.289947  0.329465  ...  0.331034  0.275862      3  0.242049   \n299998  0.314669  0.304155  ...  0.275304  0.333333      1  0.355055   \n299999  0.289957  0.237746  ...  0.186047  0.545455      3  0.278540   \n\n           ord_2     ord_3     ord_4     ord_5  day  month  \n0       0.257885  0.307005  0.208407  0.401980    2      2  \n1       0.326330  0.206605  0.186887  0.303997    7      8  \n2       0.403133  0.307005  0.351885  0.206923    7      2  \n3       0.360951  0.330124  0.208155  0.355736    2      1  \n4       0.225217  0.206605  0.351885  0.404487    7      8  \n...          ...       ...       ...       ...  ...    ...  \n299995  0.225217  0.365225  0.266041  0.313116    3      8  \n299996  0.225217  0.307005  0.409526  0.225034    3      2  \n299997  0.360951  0.433607  0.186832  0.177994    7      9  \n299998  0.360951  0.306965  0.409417  0.437908    3      8  \n299999  0.225217  0.330160  0.351885  0.417883    1      3  \n\n[300000 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>nom_0</th>\n      <th>nom_1</th>\n      <th>nom_2</th>\n      <th>nom_3</th>\n      <th>nom_4</th>\n      <th>...</th>\n      <th>nom_8</th>\n      <th>nom_9</th>\n      <th>ord_0</th>\n      <th>ord_1</th>\n      <th>ord_2</th>\n      <th>ord_3</th>\n      <th>ord_4</th>\n      <th>ord_5</th>\n      <th>day</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.302539</td>\n      <td>0.290108</td>\n      <td>0.327148</td>\n      <td>0.360990</td>\n      <td>0.307169</td>\n      <td>0.242820</td>\n      <td>0.237746</td>\n      <td>...</td>\n      <td>0.374074</td>\n      <td>0.388889</td>\n      <td>2</td>\n      <td>0.403890</td>\n      <td>0.257885</td>\n      <td>0.307005</td>\n      <td>0.208407</td>\n      <td>0.401980</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.302539</td>\n      <td>0.290108</td>\n      <td>0.327148</td>\n      <td>0.290057</td>\n      <td>0.359221</td>\n      <td>0.289957</td>\n      <td>0.304167</td>\n      <td>...</td>\n      <td>0.190909</td>\n      <td>0.083333</td>\n      <td>1</td>\n      <td>0.403890</td>\n      <td>0.326330</td>\n      <td>0.206605</td>\n      <td>0.186887</td>\n      <td>0.303997</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309387</td>\n      <td>0.290108</td>\n      <td>0.241793</td>\n      <td>0.290057</td>\n      <td>0.293087</td>\n      <td>0.289957</td>\n      <td>0.353958</td>\n      <td>...</td>\n      <td>0.223827</td>\n      <td>0.178571</td>\n      <td>1</td>\n      <td>0.317188</td>\n      <td>0.403133</td>\n      <td>0.307005</td>\n      <td>0.351885</td>\n      <td>0.206923</td>\n      <td>7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.309380</td>\n      <td>0.290103</td>\n      <td>0.351043</td>\n      <td>0.290047</td>\n      <td>0.307147</td>\n      <td>0.339780</td>\n      <td>0.329465</td>\n      <td>...</td>\n      <td>0.321782</td>\n      <td>0.209302</td>\n      <td>1</td>\n      <td>0.403877</td>\n      <td>0.360951</td>\n      <td>0.330124</td>\n      <td>0.208155</td>\n      <td>0.355736</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309387</td>\n      <td>0.333776</td>\n      <td>0.351056</td>\n      <td>0.290057</td>\n      <td>0.293087</td>\n      <td>0.339800</td>\n      <td>0.329476</td>\n      <td>...</td>\n      <td>0.378641</td>\n      <td>0.205882</td>\n      <td>1</td>\n      <td>0.403890</td>\n      <td>0.225217</td>\n      <td>0.206605</td>\n      <td>0.351885</td>\n      <td>0.404487</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>299995</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.302539</td>\n      <td>0.333776</td>\n      <td>0.351056</td>\n      <td>0.290057</td>\n      <td>0.307169</td>\n      <td>0.361323</td>\n      <td>0.329476</td>\n      <td>...</td>\n      <td>0.261905</td>\n      <td>0.348837</td>\n      <td>1</td>\n      <td>0.278540</td>\n      <td>0.225217</td>\n      <td>0.365225</td>\n      <td>0.266041</td>\n      <td>0.313116</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>299996</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309387</td>\n      <td>0.290108</td>\n      <td>0.327148</td>\n      <td>0.290057</td>\n      <td>0.293087</td>\n      <td>0.289957</td>\n      <td>0.304167</td>\n      <td>...</td>\n      <td>0.289216</td>\n      <td>0.200000</td>\n      <td>2</td>\n      <td>0.242057</td>\n      <td>0.225217</td>\n      <td>0.307005</td>\n      <td>0.409526</td>\n      <td>0.225034</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>299997</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309380</td>\n      <td>0.290103</td>\n      <td>0.241782</td>\n      <td>0.310612</td>\n      <td>0.318998</td>\n      <td>0.289947</td>\n      <td>0.329465</td>\n      <td>...</td>\n      <td>0.331034</td>\n      <td>0.275862</td>\n      <td>3</td>\n      <td>0.242049</td>\n      <td>0.360951</td>\n      <td>0.433607</td>\n      <td>0.186832</td>\n      <td>0.177994</td>\n      <td>7</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <td>299998</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.309380</td>\n      <td>0.290103</td>\n      <td>0.327140</td>\n      <td>0.338918</td>\n      <td>0.318998</td>\n      <td>0.314669</td>\n      <td>0.304155</td>\n      <td>...</td>\n      <td>0.275304</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>0.355055</td>\n      <td>0.360951</td>\n      <td>0.306965</td>\n      <td>0.409417</td>\n      <td>0.437908</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>299999</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309387</td>\n      <td>0.290108</td>\n      <td>0.241793</td>\n      <td>0.290057</td>\n      <td>0.245146</td>\n      <td>0.289957</td>\n      <td>0.237746</td>\n      <td>...</td>\n      <td>0.186047</td>\n      <td>0.545455</td>\n      <td>3</td>\n      <td>0.278540</td>\n      <td>0.225217</td>\n      <td>0.330160</td>\n      <td>0.351885</td>\n      <td>0.417883</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>300000 rows × 23 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. Catboost Encoder\n",
    "\n",
    "**Catboost** is a recently created target-based categorical encoder. \n",
    "\n",
    "It is intended to overcome target leakage problems inherent in LOO. \n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "CBE_encoder = CatBoostEncoder()\n",
    "train_cbe = CBE_encoder.fit_transform(train[feature_list], target)\n",
    "test_cbe = CBE_encoder.transform(test[feature_list])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:50:54.189435Z",
     "iopub.execute_input": "2023-06-26T12:50:54.189805Z",
     "iopub.status.idle": "2023-06-26T12:51:07.539917Z",
     "shell.execute_reply.started": "2023-06-26T12:50:54.189751Z",
     "shell.execute_reply": "2023-06-26T12:51:07.538677Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 13.9 s, sys: 1.24 s, total: 15.1 s\nWall time: 13.3 s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_cbe"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:51:07.541672Z",
     "iopub.execute_input": "2023-06-26T12:51:07.541993Z",
     "iopub.status.idle": "2023-06-26T12:51:07.623499Z",
     "shell.execute_reply.started": "2023-06-26T12:51:07.541937Z",
     "shell.execute_reply": "2023-06-26T12:51:07.622265Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "execution_count": 19,
     "output_type": "execute_result",
     "data": {
      "text/plain": "        bin_0  bin_1  bin_2     bin_3     bin_4     nom_0     nom_1     nom_2  \\\n0           0      0      0  0.305880  0.305880  0.305880  0.305880  0.305880   \n1           0      1      0  0.152940  0.152940  0.152940  0.305880  0.305880   \n2           0      0      0  0.305880  0.101960  0.305880  0.152940  0.305880   \n3           0      1      0  0.152940  0.076470  0.305880  0.101960  0.152940   \n4           0      0      0  0.435293  0.305880  0.652940  0.326470  0.152940   \n...       ...    ...    ...       ...       ...       ...       ...       ...   \n299995      0      0      0  0.302539  0.333776  0.351056  0.290063  0.307169   \n299996      0      0      0  0.309379  0.290102  0.327142  0.290060  0.293088   \n299997      0      0      0  0.309377  0.290101  0.241786  0.310611  0.318979   \n299998      0      1      0  0.309382  0.290105  0.327140  0.338918  0.318998   \n299999      0      0      0  0.309387  0.290108  0.241793  0.290057  0.245148   \n\n           nom_3     nom_4  ...     nom_8     nom_9  ord_0     ord_1  \\\n0       0.305880  0.305880  ...  0.305880  0.305880      2  0.305880   \n1       0.305880  0.305880  ...  0.305880  0.305880      1  0.152940   \n2       0.152940  0.305880  ...  0.305880  0.305880      1  0.305880   \n3       0.305880  0.305880  ...  0.305880  0.305880      1  0.101960   \n4       0.652940  0.652940  ...  0.305880  0.305880      1  0.326470   \n...          ...       ...  ...       ...       ...    ...       ...   \n299995  0.361322  0.329468  ...  0.262927  0.347861      1  0.278547   \n299996  0.289953  0.304159  ...  0.289297  0.202941      2  0.242051   \n299997  0.289950  0.329465  ...  0.330862  0.276863      3  0.242049   \n299998  0.314669  0.304155  ...  0.275427  0.332235      1  0.355053   \n299999  0.289957  0.237747  ...  0.188770  0.535038      3  0.278540   \n\n           ord_2     ord_3     ord_4     ord_5  day  month  \n0       0.305880  0.305880  0.305880  0.305880    2      2  \n1       0.305880  0.305880  0.305880  0.305880    7      8  \n2       0.305880  0.152940  0.305880  0.305880    7      2  \n3       0.305880  0.305880  0.152940  0.305880    2      1  \n4       0.305880  0.152940  0.152940  0.305880    7      8  \n...          ...       ...       ...       ...  ...    ...  \n299995  0.225222  0.365223  0.266043  0.313113    3      8  \n299996  0.225220  0.306977  0.409450  0.225089    3      2  \n299997  0.360939  0.433596  0.186839  0.178062    7      9  \n299998  0.360950  0.306965  0.409406  0.437765    3      8  \n299999  0.225217  0.330159  0.351882  0.417841    1      3  \n\n[300000 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bin_0</th>\n      <th>bin_1</th>\n      <th>bin_2</th>\n      <th>bin_3</th>\n      <th>bin_4</th>\n      <th>nom_0</th>\n      <th>nom_1</th>\n      <th>nom_2</th>\n      <th>nom_3</th>\n      <th>nom_4</th>\n      <th>...</th>\n      <th>nom_8</th>\n      <th>nom_9</th>\n      <th>ord_0</th>\n      <th>ord_1</th>\n      <th>ord_2</th>\n      <th>ord_3</th>\n      <th>ord_4</th>\n      <th>ord_5</th>\n      <th>day</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>...</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>2</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.152940</td>\n      <td>0.152940</td>\n      <td>0.152940</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>...</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>1</td>\n      <td>0.152940</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.305880</td>\n      <td>0.101960</td>\n      <td>0.305880</td>\n      <td>0.152940</td>\n      <td>0.305880</td>\n      <td>0.152940</td>\n      <td>0.305880</td>\n      <td>...</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>1</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.152940</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>7</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.152940</td>\n      <td>0.076470</td>\n      <td>0.305880</td>\n      <td>0.101960</td>\n      <td>0.152940</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>...</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>1</td>\n      <td>0.101960</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>0.152940</td>\n      <td>0.305880</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.435293</td>\n      <td>0.305880</td>\n      <td>0.652940</td>\n      <td>0.326470</td>\n      <td>0.152940</td>\n      <td>0.652940</td>\n      <td>0.652940</td>\n      <td>...</td>\n      <td>0.305880</td>\n      <td>0.305880</td>\n      <td>1</td>\n      <td>0.326470</td>\n      <td>0.305880</td>\n      <td>0.152940</td>\n      <td>0.152940</td>\n      <td>0.305880</td>\n      <td>7</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>299995</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.302539</td>\n      <td>0.333776</td>\n      <td>0.351056</td>\n      <td>0.290063</td>\n      <td>0.307169</td>\n      <td>0.361322</td>\n      <td>0.329468</td>\n      <td>...</td>\n      <td>0.262927</td>\n      <td>0.347861</td>\n      <td>1</td>\n      <td>0.278547</td>\n      <td>0.225222</td>\n      <td>0.365223</td>\n      <td>0.266043</td>\n      <td>0.313113</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>299996</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309379</td>\n      <td>0.290102</td>\n      <td>0.327142</td>\n      <td>0.290060</td>\n      <td>0.293088</td>\n      <td>0.289953</td>\n      <td>0.304159</td>\n      <td>...</td>\n      <td>0.289297</td>\n      <td>0.202941</td>\n      <td>2</td>\n      <td>0.242051</td>\n      <td>0.225220</td>\n      <td>0.306977</td>\n      <td>0.409450</td>\n      <td>0.225089</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>299997</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309377</td>\n      <td>0.290101</td>\n      <td>0.241786</td>\n      <td>0.310611</td>\n      <td>0.318979</td>\n      <td>0.289950</td>\n      <td>0.329465</td>\n      <td>...</td>\n      <td>0.330862</td>\n      <td>0.276863</td>\n      <td>3</td>\n      <td>0.242049</td>\n      <td>0.360939</td>\n      <td>0.433596</td>\n      <td>0.186839</td>\n      <td>0.178062</td>\n      <td>7</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <td>299998</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.309382</td>\n      <td>0.290105</td>\n      <td>0.327140</td>\n      <td>0.338918</td>\n      <td>0.318998</td>\n      <td>0.314669</td>\n      <td>0.304155</td>\n      <td>...</td>\n      <td>0.275427</td>\n      <td>0.332235</td>\n      <td>1</td>\n      <td>0.355053</td>\n      <td>0.360950</td>\n      <td>0.306965</td>\n      <td>0.409406</td>\n      <td>0.437765</td>\n      <td>3</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <td>299999</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.309387</td>\n      <td>0.290108</td>\n      <td>0.241793</td>\n      <td>0.290057</td>\n      <td>0.245148</td>\n      <td>0.289957</td>\n      <td>0.237747</td>\n      <td>...</td>\n      <td>0.188770</td>\n      <td>0.535038</td>\n      <td>3</td>\n      <td>0.278540</td>\n      <td>0.225217</td>\n      <td>0.330159</td>\n      <td>0.351882</td>\n      <td>0.417841</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>300000 rows × 23 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation\n",
    "\n",
    "Validation proceeds with single lr and lr with cv.\n",
    "\n",
    "- I will add OneHotEncoder, etc later.\n",
    "- More Fold get better score (my experience)\n",
    "- you can try another solver and another parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single LR"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "encoder_list = [ OrdinalEncoder(), WOEEncoder(), TargetEncoder(), MEstimateEncoder(), JamesSteinEncoder(), LeaveOneOutEncoder() ,CatBoostEncoder()]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=97)\n",
    "\n",
    "for encoder in encoder_list:\n",
    "    print(\"Test {} : \".format(str(encoder).split('(')[0]), end=\" \")\n",
    "    train_enc = encoder.fit_transform(X_train[feature_list], y_train)\n",
    "    #test_enc = encoder.transform(test[feature_list])\n",
    "    val_enc = encoder.transform(X_val[feature_list])\n",
    "    lr = LogisticRegression(C=0.1, solver=\"lbfgs\", max_iter=1000)\n",
    "    lr.fit(train_enc, y_train)\n",
    "    lr_pred = lr.predict_proba(val_enc)[:, 1]\n",
    "    score = auc(y_val, lr_pred)\n",
    "    print(\"score: \", score)\n",
    "    del train_enc\n",
    "    del val_enc\n",
    "    gc.collect()\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T12:54:17.639173Z",
     "iopub.execute_input": "2023-06-26T12:54:17.639567Z",
     "iopub.status.idle": "2023-06-26T12:57:22.803719Z",
     "shell.execute_reply.started": "2023-06-26T12:54:17.639491Z",
     "shell.execute_reply": "2023-06-26T12:57:22.802818Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "Test OrdinalEncoder :  ",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "score:  0.6058952245541614\nTest WOEEncoder :  score:  0.7814115175120097\nTest TargetEncoder :  score:  0.7790232741835907\nTest MEstimateEncoder :  score:  0.7792025608680453\nTest JamesSteinEncoder :  score:  0.7719623937439537\nTest LeaveOneOutEncoder :  score:  0.79576549204064\nTest CatBoostEncoder :  score:  0.7917094405644212\nCPU times: user 3min 52s, sys: 2.48 s, total: 3min 55s\nWall time: 3min 5s\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LR with CrossValidation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# CV function original : @Peter Hurford : Why Not Logistic Regression? https://www.kaggle.com/peterhurford/why-not-logistic-regression\n",
    "\n",
    "def run_cv_model(train, test, target, model_fn, params={}, label='model'):\n",
    "    kf = KFold(n_splits=5)\n",
    "    fold_splits = kf.split(train, target)\n",
    "\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0]))\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print('Started {} fold {}/5'.format(label, i))\n",
    "        dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "        dev_y, val_y = target[dev_index], target[val_index]\n",
    "        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test, params)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        cv_score = auc(val_y, pred_val_y)\n",
    "        cv_scores.append(cv_score)\n",
    "        print(label + ' cv score {}: {}'.format(i, cv_score))\n",
    "        i += 1\n",
    "        \n",
    "    print('{} cv scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std score : {}'.format(label, np.std(cv_scores)))\n",
    "    pred_full_test = pred_full_test / 5.0\n",
    "    results = {'label': label, 'train': pred_train, 'test': pred_full_test, 'cv': cv_scores}\n",
    "    return results\n",
    "\n",
    "\n",
    "def runLR(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)[:, 1]\n",
    "    pred_test_y2 = model.predict_proba(test_X2)[:, 1]\n",
    "    return pred_test_y, pred_test_y2\n"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2023-06-26T13:10:01.426511Z",
     "iopub.execute_input": "2023-06-26T13:10:01.426986Z",
     "iopub.status.idle": "2023-06-26T13:10:01.438379Z",
     "shell.execute_reply.started": "2023-06-26T13:10:01.426921Z",
     "shell.execute_reply": "2023-06-26T13:10:01.436819Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 37 µs, sys: 1e+03 ns, total: 38 µs\nWall time: 41 µs\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "TEST = True"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T13:10:46.205044Z",
     "iopub.execute_input": "2023-06-26T13:10:46.205376Z",
     "iopub.status.idle": "2023-06-26T13:10:46.210031Z",
     "shell.execute_reply.started": "2023-06-26T13:10:46.205325Z",
     "shell.execute_reply": "2023-06-26T13:10:46.208864Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if TEST:\n",
    "\n",
    "    lr_params = {'solver': 'lbfgs', 'C': 0.1}\n",
    "\n",
    "    results = list()\n",
    "\n",
    "    for encoder in  [ OrdinalEncoder(), WOEEncoder(), TargetEncoder(), MEstimateEncoder(), JamesSteinEncoder(), LeaveOneOutEncoder() ,CatBoostEncoder()]:\n",
    "        train_enc = encoder.fit_transform(train[feature_list], target)\n",
    "        test_enc = encoder.transform(test[feature_list])\n",
    "        result = run_cv_model(train_enc, test_enc, target, runLR, lr_params, str(encoder).split('(')[0])\n",
    "        results.append(result)\n",
    "    results = pd.DataFrame(results)\n",
    "    results['cv_mean'] = results['cv'].apply(lambda l : np.mean(l))\n",
    "    results['cv_std'] = results['cv'].apply(lambda l : np.std(l))\n",
    "    results[['label','cv_mean','cv_std']].head(8)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T13:10:46.381398Z",
     "iopub.execute_input": "2023-06-26T13:10:46.382059Z",
     "iopub.status.idle": "2023-06-26T13:13:53.720337Z",
     "shell.execute_reply.started": "2023-06-26T13:10:46.381999Z",
     "shell.execute_reply": "2023-06-26T13:13:53.719563Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "Started OrdinalEncoder fold 1/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "OrdinalEncoder cv score 1: 0.5478087630083479\nStarted OrdinalEncoder fold 2/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "OrdinalEncoder cv score 2: 0.5785368145198041\nStarted OrdinalEncoder fold 3/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "OrdinalEncoder cv score 3: 0.5794711812720389\nStarted OrdinalEncoder fold 4/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "OrdinalEncoder cv score 4: 0.5776419118948795\nStarted OrdinalEncoder fold 5/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "OrdinalEncoder cv score 5: 0.5530765797041557\nOrdinalEncoder cv scores : [0.5478087630083479, 0.5785368145198041, 0.5794711812720389, 0.5776419118948795, 0.5530765797041557]\nOrdinalEncoder cv mean score : 0.5673070500798453\nOrdinalEncoder cv std score : 0.01388216518854039\nStarted WOEEncoder fold 1/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "WOEEncoder cv score 1: 0.8296010840892494\nStarted WOEEncoder fold 2/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "WOEEncoder cv score 2: 0.8276778463251253\nStarted WOEEncoder fold 3/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "WOEEncoder cv score 3: 0.8343609323413513\nStarted WOEEncoder fold 4/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "WOEEncoder cv score 4: 0.8315878216378239\nStarted WOEEncoder fold 5/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "WOEEncoder cv score 5: 0.8307581723870281\nWOEEncoder cv scores : [0.8296010840892494, 0.8276778463251253, 0.8343609323413513, 0.8315878216378239, 0.8307581723870281]\nWOEEncoder cv mean score : 0.8307971713561155\nWOEEncoder cv std score : 0.002213045618434726\nStarted TargetEncoder fold 1/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "TargetEncoder cv score 1: 0.8228825225372465\nStarted TargetEncoder fold 2/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "TargetEncoder cv score 2: 0.819640037288339\nStarted TargetEncoder fold 3/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "TargetEncoder cv score 3: 0.8270373654953843\nStarted TargetEncoder fold 4/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "TargetEncoder cv score 4: 0.8279529191167299\nStarted TargetEncoder fold 5/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "TargetEncoder cv score 5: 0.8270456093348808\nTargetEncoder cv scores : [0.8228825225372465, 0.819640037288339, 0.8270373654953843, 0.8279529191167299, 0.8270456093348808]\nTargetEncoder cv mean score : 0.8249116907545162\nTargetEncoder cv std score : 0.003169511807334321\nStarted MEstimateEncoder fold 1/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "MEstimateEncoder cv score 1: 0.8239353389041686\nStarted MEstimateEncoder fold 2/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "MEstimateEncoder cv score 2: 0.8225460353941083\nStarted MEstimateEncoder fold 3/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "MEstimateEncoder cv score 3: 0.8250872944298472\nStarted MEstimateEncoder fold 4/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "MEstimateEncoder cv score 4: 0.8290057049907481\nStarted MEstimateEncoder fold 5/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "MEstimateEncoder cv score 5: 0.8257572319360416\nMEstimateEncoder cv scores : [0.8239353389041686, 0.8225460353941083, 0.8250872944298472, 0.8290057049907481, 0.8257572319360416]\nMEstimateEncoder cv mean score : 0.8252663211309826\nMEstimateEncoder cv std score : 0.002164601755855072\nStarted JamesSteinEncoder fold 1/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "JamesSteinEncoder cv score 1: 0.825785651337012\nStarted JamesSteinEncoder fold 2/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "JamesSteinEncoder cv score 2: 0.8213704510704314\nStarted JamesSteinEncoder fold 3/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "JamesSteinEncoder cv score 3: 0.8279921606609264\nStarted JamesSteinEncoder fold 4/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "JamesSteinEncoder cv score 4: 0.8144299433590861\nStarted JamesSteinEncoder fold 5/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "JamesSteinEncoder cv score 5: 0.8265248336422055\nJamesSteinEncoder cv scores : [0.825785651337012, 0.8213704510704314, 0.8279921606609264, 0.8144299433590861, 0.8265248336422055]\nJamesSteinEncoder cv mean score : 0.8232206080139323\nJamesSteinEncoder cv std score : 0.004918616364474788\nStarted LeaveOneOutEncoder fold 1/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "LeaveOneOutEncoder cv score 1: 0.7870271221339217\nStarted LeaveOneOutEncoder fold 2/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "LeaveOneOutEncoder cv score 2: 0.7883871851867641\nStarted LeaveOneOutEncoder fold 3/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "LeaveOneOutEncoder cv score 3: 0.7940299539122266\nStarted LeaveOneOutEncoder fold 4/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "LeaveOneOutEncoder cv score 4: 0.7901332335441938\nStarted LeaveOneOutEncoder fold 5/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "LeaveOneOutEncoder cv score 5: 0.7883777050213145\nLeaveOneOutEncoder cv scores : [0.7870271221339217, 0.7883871851867641, 0.7940299539122266, 0.7901332335441938, 0.7883777050213145]\nLeaveOneOutEncoder cv mean score : 0.7895910399596842\nLeaveOneOutEncoder cv std score : 0.0024287055632732923\nStarted CatBoostEncoder fold 1/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "CatBoostEncoder cv score 1: 0.7297166731892779\nStarted CatBoostEncoder fold 2/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "CatBoostEncoder cv score 2: 0.7805421729205533\nStarted CatBoostEncoder fold 3/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "CatBoostEncoder cv score 3: 0.7839670066128355\nStarted CatBoostEncoder fold 4/5\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "CatBoostEncoder cv score 4: 0.7795226775861617\nStarted CatBoostEncoder fold 5/5\nCatBoostEncoder cv score 5: 0.786081368408643\nCatBoostEncoder cv scores : [0.7297166731892779, 0.7805421729205533, 0.7839670066128355, 0.7795226775861617, 0.786081368408643]\nCatBoostEncoder cv mean score : 0.7719659797434942\nCatBoostEncoder cv std score : 0.021255246501015294\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Submit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even CVs did not solve the target based encoder's overfit problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "if TEST:\n",
    "    for idx, label in enumerate(results['label']):\n",
    "        sub_df = pd.DataFrame({'id': test_id, 'target' : results.iloc[idx]['test']})\n",
    "        sub_df.to_csv(\"LR_{}.csv\".format(label), index=False)\n",
    "\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-26T13:15:26.189611Z",
     "iopub.execute_input": "2023-06-26T13:15:26.189993Z",
     "iopub.status.idle": "2023-06-26T13:15:26.230444Z",
     "shell.execute_reply.started": "2023-06-26T13:15:26.189932Z",
     "shell.execute_reply": "2023-06-26T13:15:26.229445Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "execution_count": 25,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                label                                              train  \\\n0      OrdinalEncoder  [0.48505482333487315, 0.4917464196614169, 0.42...   \n1          WOEEncoder  [0.3576322095435018, 0.01605304315035642, 0.02...   \n2       TargetEncoder  [0.40135938852395303, 0.027724824015846892, 0....   \n3    MEstimateEncoder  [0.43698261221403517, 0.023900335752031814, 0....   \n4   JamesSteinEncoder  [0.38073758773828714, 0.01792667770063288, 0.0...   \n5  LeaveOneOutEncoder  [0.39689094271812525, 0.06536588173953757, 0.0...   \n6     CatBoostEncoder  [0.27363762814222814, 0.10252561953328132, 0.0...   \n\n                                                test  \\\n0  [0.3380805366198249, 0.3306798714776061, 0.334...   \n1  [0.23103000402027601, 0.5454760958772756, 0.03...   \n2  [0.24365513471906883, 0.4716806571929366, 0.05...   \n3  [0.24434920814860037, 0.4873201437201871, 0.05...   \n4  [0.26601283716262103, 0.4008785887852655, 0.04...   \n5  [0.32221034919543445, 0.5207705882316336, 0.11...   \n6  [0.33701225943510055, 0.4841749143409252, 0.16...   \n\n                                                  cv   cv_mean    cv_std  \n0  [0.5478087630083479, 0.5785368145198041, 0.579...  0.567307  0.013882  \n1  [0.8296010840892494, 0.8276778463251253, 0.834...  0.830797  0.002213  \n2  [0.8228825225372465, 0.819640037288339, 0.8270...  0.824912  0.003170  \n3  [0.8239353389041686, 0.8225460353941083, 0.825...  0.825266  0.002165  \n4  [0.825785651337012, 0.8213704510704314, 0.8279...  0.823221  0.004919  \n5  [0.7870271221339217, 0.7883871851867641, 0.794...  0.789591  0.002429  \n6  [0.7297166731892779, 0.7805421729205533, 0.783...  0.771966  0.021255  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>train</th>\n      <th>test</th>\n      <th>cv</th>\n      <th>cv_mean</th>\n      <th>cv_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>OrdinalEncoder</td>\n      <td>[0.48505482333487315, 0.4917464196614169, 0.42...</td>\n      <td>[0.3380805366198249, 0.3306798714776061, 0.334...</td>\n      <td>[0.5478087630083479, 0.5785368145198041, 0.579...</td>\n      <td>0.567307</td>\n      <td>0.013882</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>WOEEncoder</td>\n      <td>[0.3576322095435018, 0.01605304315035642, 0.02...</td>\n      <td>[0.23103000402027601, 0.5454760958772756, 0.03...</td>\n      <td>[0.8296010840892494, 0.8276778463251253, 0.834...</td>\n      <td>0.830797</td>\n      <td>0.002213</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>TargetEncoder</td>\n      <td>[0.40135938852395303, 0.027724824015846892, 0....</td>\n      <td>[0.24365513471906883, 0.4716806571929366, 0.05...</td>\n      <td>[0.8228825225372465, 0.819640037288339, 0.8270...</td>\n      <td>0.824912</td>\n      <td>0.003170</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>MEstimateEncoder</td>\n      <td>[0.43698261221403517, 0.023900335752031814, 0....</td>\n      <td>[0.24434920814860037, 0.4873201437201871, 0.05...</td>\n      <td>[0.8239353389041686, 0.8225460353941083, 0.825...</td>\n      <td>0.825266</td>\n      <td>0.002165</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>JamesSteinEncoder</td>\n      <td>[0.38073758773828714, 0.01792667770063288, 0.0...</td>\n      <td>[0.26601283716262103, 0.4008785887852655, 0.04...</td>\n      <td>[0.825785651337012, 0.8213704510704314, 0.8279...</td>\n      <td>0.823221</td>\n      <td>0.004919</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>LeaveOneOutEncoder</td>\n      <td>[0.39689094271812525, 0.06536588173953757, 0.0...</td>\n      <td>[0.32221034919543445, 0.5207705882316336, 0.11...</td>\n      <td>[0.7870271221339217, 0.7883871851867641, 0.794...</td>\n      <td>0.789591</td>\n      <td>0.002429</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>CatBoostEncoder</td>\n      <td>[0.27363762814222814, 0.10252561953328132, 0.0...</td>\n      <td>[0.33701225943510055, 0.4841749143409252, 0.16...</td>\n      <td>[0.7297166731892779, 0.7805421729205533, 0.783...</td>\n      <td>0.771966</td>\n      <td>0.021255</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
