{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analýza textu pomocí Pythonu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rád bych úvodem předeslal, že se v tomto spisku nebude hovořit o analýze textu s pomocí hlubokých neuronových sítí. Není to kvůli tomu, že by to byla problematika nezajímavá nebo vzdálená reálným aplikacím. Právě naopak. Jelikož ale s těmito pokročilými technikami nemám absolutně žádnou zkušenost, netroufám si veřejně prezentovat jakoukoli rešerši na ono téma.\n",
    "Čemu se tedy na následujících řádcích budeme věnovat? Půjde o metody starší, možná ne tak mocné, ale přesto pro určité úlohy stále užitečné. Navíc se bude jednat o věci snáze vysvětlitelné a snáze prezentovatelné i na dýchavičných počítačích. Modely a postupy budou až na určité výjimky pracovat s textem jako s \"bag of words\". To znamená, že při analýze dokumentů budeme pomíjet vzájemné vtahy mezi slovy a budeme sledovat pouze frekvenci výskytů jednotlivých termínů bez ohledu na kontext, ve kterém se nacházejí.  \n",
    "Pro začátek bychom si měli zadefinovat několik pojmů.  \n",
    "**Slovo** - jedná se o základní jednotku dat (textu). Obvykle je zapsáno v nějakém slovníku slov. Na první pohled vypadá tahle definice samozřejmě až zbytečně. Měli bychom ale zmínit, že některé modely (např. LDA) nejsou navázány jenom na text a v takovém případě tak může termín **slovo** označovat i něco, co bychom normálně slovem neoznačili.  \n",
    "**Dokument** - jedná se o posloupnost X slov. Může jím být jedna věta, ale třeba i celý novinový článek.  \n",
    "**Korpus** - Jedná se o sbírku Y dokumentů.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obsah**  \n",
    "[Ruční čištění textu](#Pročištění-textu-normálním-způsobem)  \n",
    "[Čištění textu pomocí Spacy](#Pročištění-textu-pomocí-spacy)  \n",
    "[Stemming a lematizace](#Stemming-a-lematizace)  \n",
    "[Vektorizéry](#Vektorizéry)  \n",
    "[Term frequency - inverse term frequency (TF-IDF)](#TF-IDF)  \n",
    "[Latent Dirichlet allocation (LDA)](#LDA)  \n",
    "[Word2Vec](#Word2Vec)  \n",
    "[Doc2Vec](#Doc2Vec)  \n",
    "[Named entity recognition (NER)](#NER-(Named-entity-recognition))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pročištění textu normálním způsobem\n",
    "Naším cílem je z určitého textu vydolovat informace. Ty jsou zakódovány v některých slovech, resp. v posloupnosti slov. Avšak tato pro naše potřeby užitečná slova jsou obalena slovy nedůležitými, která nemusí z hlediska ML algoritmu nic dalšího přinášet. Též se v okolí slov mohou vyskytovat HTML tagy anebo interpunkce. Slova zejména v češtině mohou díky skloňování a časování nabývat všelijakých tvarů a oku nepoučeného pozorovatele, kterým počítač je, mohou tyto různé tvary připadat jako zcela separátní termíny. Nakonec pak v dokumentech reálně nevidíme samostatná separovaná slova, nýbrž paragrafy. Ty se skládají z vět a ty jsou zase tvořeny slovy. Nicméně mezi těmito lingvistickými jednotkami neleží Mariánský příkop, nýbrž mezery, tečky či čárky. S tím vším by se počítač srovnával dosti obtížně a tudíž je na místě mu jeho práci usnadnit.  \n",
    "V této sekci workshopu budeme pracovat s následujícím textem:\n",
    "> V uplynulých dekádách došlo k razantnímu pokroku v elektronice, k pokroku\n",
    "tak velkému, že narazil na fyzikální meze svých možností. Zlepšování parametrů\n",
    "používaných součástek šlo totiž ruku v ruce s jejich miniaturizací, která se ale\n",
    "z fundamentálních důvodů musí zastavit u rozměrů kolem 10 nm. Na těchto škálách\n",
    "se totiž začínají projevovat kvantové vlastnosti elektronů, což však není kompatibilní\n",
    "se současnou výpočetní technikou. V rámci hledání řešení tohoto problému tak začal\n",
    "v posledních letech značně vzrůstat zájem o spintroniku, což je odvětví elektroniky,\n",
    "ve kterém se klade důraz nejen na náboj, ale i na spin nabitých nosičů. Je nutné\n",
    "podotknout, že ačkoli se jedná o relativně nový vědní obor, jedna z jejích aplikací -\n",
    "čtecí hlavy v pevných discích - je už léta široce rozšířená."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:20:38.875821Z",
     "start_time": "2021-02-27T15:20:38.856012Z"
    }
   },
   "outputs": [],
   "source": [
    "original_text = \"\"\"V uplynulých dekádách došlo k razantnímu pokroku v elektronice, k pokroku\n",
    "tak velkému, že narazil na fyzikální meze svých možností. Zlepšování parametrů\n",
    "používaných součástek šlo totiž ruku v ruce s jejich miniaturizací, která se ale\n",
    "z fundamentálních důvodů musí zastavit u rozměrů kolem 10 nm. Na těchto škálách\n",
    "se totiž začínají projevovat kvantové vlastnosti elektronů, což však není kompatibilní\n",
    "se současnou výpočetní technikou. V rámci hledání řešení tohoto problému tak začal\n",
    "v posledních letech značně vzrůstat zájem o spintroniku, což je odvětví elektroniky,\n",
    "ve kterém se klade důraz nejen na náboj, ale i na spin nabitých nosičů. Je nutné\n",
    "podotknout, že ačkoli se jedná o relativně nový vědní obor, jedna z jejích aplikací -\n",
    "čtecí hlavy v pevných discích - je už léta široce rozšířená.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "První věc, která nás napadne, je převedení všech písmen na malá - koneckonců význam slova \"Zlepšování\" je stejný jako u \"zlepšování\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:20:40.925854Z",
     "start_time": "2021-02-27T15:20:40.906178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v uplynulých dekádách došlo k razantnímu pokroku v elektronice, k pokroku\\ntak velkému, že narazil na fyzikální meze svých možností. zlepšování parametrů\\npoužívaných součástek šlo totiž ruku v ruce s jejich miniaturizací, která se ale\\nz fundamentálních důvodů musí zastavit u rozměrů kolem 10 nm. na těchto škálách\\nse totiž začínají projevovat kvantové vlastnosti elektronů, což však není kompatibilní\\nse současnou výpočetní technikou. v rámci hledání řešení tohoto problému tak začal\\nv posledních letech značně vzrůstat zájem o spintroniku, což je odvětví elektroniky,\\nve kterém se klade důraz nejen na náboj, ale i na spin nabitých nosičů. je nutné\\npodotknout, že ačkoli se jedná o relativně nový vědní obor, jedna z jejích aplikací -\\nčtecí hlavy v pevných discích - je už léta široce rozšířená.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_change_1 = original_text.lower()\n",
    "small_change_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jelikož chceme pracovat s celým textem naráz a neplánujeme jít větu po větě, chtěli bychom odstranit tečky a vlastně i všechnu ostatní interpunkci. Jak to ale realizovat? Člověka by mohlo napadnout, že si všechny tečky a čárky nahradí standardní stringovou operací za prázdný řetězec. To by se nám ale mohlo šeredně vymstít, pokud by autor zdrojového textu občas za interpunkčním znakem omylem vynechal mezeru. Pak bychom totiž např. z \"elektronů,což\", udělali \"elektronůcož\", což by počítač už nerozklíčoval. Nahrazovat interpunkci tudíž budeme za řetězec \" \". Kde ale vzít seznam těchto speciálních znaků? Mohli bychom si je sice zapsat do listu, avšak v takovém případě bychom na některý z nich mohli zapomenout. Naštěstí jejich kompletní seznam se nachází v balíčku *string* v listu *punctuation*.  \n",
    "V tomto kroku taktéž nahradíme znaky nového řádku - ony nevzhledné \"\\n\" - za mezeru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:20:42.516203Z",
     "start_time": "2021-02-27T15:20:42.496183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v uplynulých dekádách došlo k razantnímu pokroku v elektronice  k pokroku tak velkému  že narazil na fyzikální meze svých možností  zlepšování parametrů používaných součástek šlo totiž ruku v ruce s jejich miniaturizací  která se ale z fundamentálních důvodů musí zastavit u rozměrů kolem 10 nm  na těchto škálách se totiž začínají projevovat kvantové vlastnosti elektronů  což však není kompatibilní se současnou výpočetní technikou  v rámci hledání řešení tohoto problému tak začal v posledních letech značně vzrůstat zájem o spintroniku  což je odvětví elektroniky  ve kterém se klade důraz nejen na náboj  ale i na spin nabitých nosičů  je nutné podotknout  že ačkoli se jedná o relativně nový vědní obor  jedna z jejích aplikací   čtecí hlavy v pevných discích   je už léta široce rozšířená '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "small_nonelegant_change_2 = small_change_1\n",
    "\n",
    "interpunction = string.punctuation\n",
    "for interpunction_char in interpunction:\n",
    "    small_nonelegant_change_2 = small_nonelegant_change_2.replace(interpunction_char, \" \")\n",
    "small_nonelegant_change_2 = small_nonelegant_change_2.replace(\"\\n\", \" \")\n",
    "small_nonelegant_change_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výše uvedený kód lze přepsat do trochu hezčí podoby pomocí funkce *translate*. Ta v řetězci, na který je vypuštěna, nahrazuje znaky podle translační tabulky vyrobené díky funkci *maketrans*. Funkce *maketrans* vytváří mapování jedna ku jedné mezi i-tými elementy svého prvního a druhého argumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:20:44.206004Z",
     "start_time": "2021-02-27T15:20:44.186227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v uplynulých dekádách došlo k razantnímu pokroku v elektronice  k pokroku tak velkému  že narazil na fyzikální meze svých možností  zlepšování parametrů používaných součástek šlo totiž ruku v ruce s jejich miniaturizací  která se ale z fundamentálních důvodů musí zastavit u rozměrů kolem 10 nm  na těchto škálách se totiž začínají projevovat kvantové vlastnosti elektronů  což však není kompatibilní se současnou výpočetní technikou  v rámci hledání řešení tohoto problému tak začal v posledních letech značně vzrůstat zájem o spintroniku  což je odvětví elektroniky  ve kterém se klade důraz nejen na náboj  ale i na spin nabitých nosičů  je nutné podotknout  že ačkoli se jedná o relativně nový vědní obor  jedna z jejích aplikací   čtecí hlavy v pevných discích   je už léta široce rozšířená '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "small_change_2 = small_change_1\n",
    "\n",
    "interpunction = string.punctuation\n",
    "replaced_chars = interpunction + \"\\n\"\n",
    "replacing_chars = \" \" * len(replaced_chars)\n",
    "small_change_2 = small_change_2.translate(str.maketrans(replaced_chars, replacing_chars, \"\"))\n",
    "small_change_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máme v plánu text rozkouskovat na jednotlivá slova. Tím se ale ztratí informace o tom, že je desítka vztažena k nanometrům. Proto bychom ji preventivně chtěli odstranit. To se dá udělat například pomocí regulárních výrazů. Konkrétně použijeme funkci *sub*, která v textu (3. argument) nahradí všechny výskyty prvního argumentu za argument druhý. Do prvního argumentu pak vkládáme regexový výraz pro číslovku \"\\d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:20:46.163613Z",
     "start_time": "2021-02-27T15:20:46.145764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v uplynulých dekádách došlo k razantnímu pokroku v elektronice  k pokroku tak velkému  že narazil na fyzikální meze svých možností  zlepšování parametrů používaných součástek šlo totiž ruku v ruce s jejich miniaturizací  která se ale z fundamentálních důvodů musí zastavit u rozměrů kolem  nm  na těchto škálách se totiž začínají projevovat kvantové vlastnosti elektronů  což však není kompatibilní se současnou výpočetní technikou  v rámci hledání řešení tohoto problému tak začal v posledních letech značně vzrůstat zájem o spintroniku  což je odvětví elektroniky  ve kterém se klade důraz nejen na náboj  ale i na spin nabitých nosičů  je nutné podotknout  že ačkoli se jedná o relativně nový vědní obor  jedna z jejích aplikací   čtecí hlavy v pevných discích   je už léta široce rozšířená '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "small_change_3 = re.sub(\"\\d\", \"\", small_change_2)\n",
    "small_change_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že v textu je hodně neužitečných slov (tzv. stopwords) - po rozsekání na jednotlivá slova budou předložky či spojky přinášet snad ještě méně informace než desítka diskutovaná v předchozím odstavci. Jakmile toto vyřešíme, už můžeme podle bílých znaků ono rozsekání - tokenizaci - provést."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:20:47.815867Z",
     "start_time": "2021-02-27T15:20:47.786004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uplynulých',\n",
       " 'dekádách',\n",
       " 'došlo',\n",
       " 'razantnímu',\n",
       " 'pokroku',\n",
       " 'elektronice',\n",
       " 'pokroku',\n",
       " 'velkému',\n",
       " 'narazil',\n",
       " 'fyzikální',\n",
       " 'meze',\n",
       " 'svých',\n",
       " 'možností',\n",
       " 'zlepšování',\n",
       " 'parametrů',\n",
       " 'používaných',\n",
       " 'součástek',\n",
       " 'šlo',\n",
       " 'ruku',\n",
       " 'ruce',\n",
       " 'miniaturizací',\n",
       " 'fundamentálních',\n",
       " 'důvodů',\n",
       " 'musí',\n",
       " 'zastavit',\n",
       " 'rozměrů',\n",
       " 'kolem',\n",
       " 'nm',\n",
       " 'škálách',\n",
       " 'začínají',\n",
       " 'projevovat',\n",
       " 'kvantové',\n",
       " 'vlastnosti',\n",
       " 'elektronů',\n",
       " 'není',\n",
       " 'kompatibilní',\n",
       " 'současnou',\n",
       " 'výpočetní',\n",
       " 'technikou',\n",
       " 'rámci',\n",
       " 'hledání',\n",
       " 'řešení',\n",
       " 'tohoto',\n",
       " 'problému',\n",
       " 'začal',\n",
       " 'posledních',\n",
       " 'letech',\n",
       " 'značně',\n",
       " 'vzrůstat',\n",
       " 'zájem',\n",
       " 'spintroniku',\n",
       " 'je',\n",
       " 'odvětví',\n",
       " 'elektroniky',\n",
       " 'klade',\n",
       " 'důraz',\n",
       " 'nejen',\n",
       " 'náboj',\n",
       " 'spin',\n",
       " 'nabitých',\n",
       " 'nosičů',\n",
       " 'je',\n",
       " 'nutné',\n",
       " 'podotknout',\n",
       " 'ačkoli',\n",
       " 'jedná',\n",
       " 'relativně',\n",
       " 'nový',\n",
       " 'vědní',\n",
       " 'obor',\n",
       " 'jedna',\n",
       " 'jejích',\n",
       " 'aplikací',\n",
       " 'čtecí',\n",
       " 'hlavy',\n",
       " 'pevných',\n",
       " 'discích',\n",
       " 'je',\n",
       " 'léta',\n",
       " 'široce',\n",
       " 'rozšířená']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CZECH_STOPWORDS = [\n",
    "    \"v\", \"k\", \"tak\", \"že\", \"na\", \"totiž\", \"s\", \"která\", \"se\", \"i\", \"už\",\n",
    "    \"ale\", \"z\", \"u\", \"těchto\", \"což\", \"však\", \"o\", \"ve\", \"kterém\", \"jejich\"\n",
    "]\n",
    "\n",
    "small_change_4 = [word for word in small_change_3.split() if word not in CZECH_STOPWORDS]\n",
    "small_change_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pročištění textu pomocí spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T20:05:33.192861Z",
     "start_time": "2020-03-26T20:05:33.186877Z"
    }
   },
   "source": [
    "Spacy je pythoní balíček na zpracování textu. Klade si za cíl poskytnout uživateli rychlé a kvalitní výsledky bez nutnosti do podrobna zkoumat dokumentaci balíčku (citujme jednoho z tvůrců: \"spaCy is written to help you get things done\"). Oproti NLTK tak  nenabízí myriádu algoritmů, nýbrž jeho autoři se snažili vybrat ty postupy, které jim přišly pro určitý účel nejlepší.  \n",
    "Krom samotné instalace balíčku přes pip se musí nainstalovat i modul pro určitý jazyk (viz níže). Bohužel pro češtinu modul prozatím neexistuje. \n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Při použití musíme nejprve naimportovat balíček a pak si s pomocí jazykového modulu vytvořit nlp objekt. Ten obsahuje metody umožňující zpracovávat text podle pravidel daného jazyku. Mimo jiné sem patří oddělování slov podle bílých znaků či separace čísel a interpunkce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:23:53.506043Z",
     "start_time": "2021-02-27T15:23:52.075617Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po vytvoření *nlp* objektu ho můžeme vypustit na náš text. Vidíme, že výsledný dokument vypadá na první pohled stejně jako dokument originální. Zdání zde ale klame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:23:56.655467Z",
     "start_time": "2021-02-27T15:23:55.125659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V uplynulých dekádách došlo k razantnímu pokroku v elektronice, k pokroku\n",
       "tak velkému, že narazil na fyzikální meze svých možností. Zlepšování parametrů\n",
       "používaných součástek šlo totiž ruku v ruce s jejich miniaturizací, která se ale\n",
       "z fundamentálních důvodů musí zastavit u rozměrů kolem 10 nm. Na těchto škálách\n",
       "se totiž začínají projevovat kvantové vlastnosti elektronů, což však není kompatibilní\n",
       "se současnou výpočetní technikou. V rámci hledání řešení tohoto problému tak začal\n",
       "v posledních letech značně vzrůstat zájem o spintroniku, což je odvětví elektroniky,\n",
       "ve kterém se klade důraz nejen na náboj, ale i na spin nabitých nosičů. Je nutné\n",
       "podotknout, že ačkoli se jedná o relativně nový vědní obor, jedna z jejích aplikací -\n",
       "čtecí hlavy v pevných discích - je už léta široce rozšířená."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = nlp(original_text)\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nově vytvořený objekt je už totiž rozsekán na tokeny - víceméně slova a interpunkční znaky. Tyto tokeny mají pak atributy nesoucí o nich užitečné informace.\n",
    "> *.text* ukazuje, jak token vlastně vypadá  \n",
    "*.lower_* je to samé jako  .text, ale převedené na malá písmena (existuje i .lower, tj. bez podtržítka na konci; to ale ukazuje **asi** ID lowercasovaného tokenu) \n",
    "*.is_alpha* je True, pokud se token skládá pouze z písmen  \n",
    "*.is_punct* je True, pokud je tokenem interpunkční znak  \n",
    "*.like_num* je True, pokud má token význam čísla. To znaméná, že v případě užití anglického modelu bude True nejen u tokenu \"9\", ale i u tokenu \"nine\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:24:00.545589Z",
     "start_time": "2021-02-27T15:24:00.485719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token text:  V ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  uplynulých ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  dekádách ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  došlo ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  k ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  razantnímu ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  pokroku ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  v ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  elektronice ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  , ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  k ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  pokroku ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  \n",
      " ***Is word:  False ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  tak ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  velkému ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  , ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  že ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  narazil ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  na ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  fyzikální ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  meze ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  svých ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  možností ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  . ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  Zlepšování ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  parametrů ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  \n",
      " ***Is word:  False ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  používaných ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  součástek ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  šlo ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  totiž ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  ruku ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  v ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  ruce ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  s ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  jejich ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  miniaturizací ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  , ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  která ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  se ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  ale ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  \n",
      " ***Is word:  False ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  z ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  fundamentálních ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  důvodů ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  musí ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  zastavit ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  u ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  rozměrů ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  kolem ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  10 ***Is word:  False ***Is interpuction:  False ***Is number:  True\n",
      "Token text:  nm ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  . ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  Na ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  těchto ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  škálách ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  \n",
      " ***Is word:  False ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  se ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  totiž ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  začínají ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  projevovat ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  kvantové ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  vlastnosti ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  elektronů ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  , ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  což ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  však ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  není ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  kompatibilní ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  \n",
      " ***Is word:  False ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  se ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  současnou ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  výpočetní ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  technikou ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  . ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  V ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  rámci ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  hledání ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  řešení ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  tohoto ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  problému ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  tak ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  začal ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  \n",
      " ***Is word:  False ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  v ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  posledních ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  letech ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  značně ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  vzrůstat ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  zájem ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  o ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  spintroniku ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  , ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  což ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  je ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  odvětví ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  elektroniky ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  , ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  \n",
      " ***Is word:  False ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  ve ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  kterém ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  se ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  klade ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  důraz ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  nejen ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  na ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  náboj ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  , ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  ale ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  i ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  na ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  spin ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  nabitých ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  nosičů ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  . ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  Je ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  nutné ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  \n",
      " ***Is word:  False ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  podotknout ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  , ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  že ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  ačkoli ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  se ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  jedná ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  o ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  relativně ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  nový ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  vědní ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  obor ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  , ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  jedna ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  z ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  jejích ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  aplikací ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  - ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  \n",
      " ***Is word:  False ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  čtecí ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  hlavy ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  v ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  pevných ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  discích ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  - ***Is word:  False ***Is interpuction:  True ***Is number:  False\n",
      "Token text:  je ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  už ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  léta ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  široce ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  rozšířená ***Is word:  True ***Is interpuction:  False ***Is number:  False\n",
      "Token text:  . ***Is word:  False ***Is interpuction:  True ***Is number:  False\n"
     ]
    }
   ],
   "source": [
    "for token in document:\n",
    "    print(\"Token text: \", token.text, \n",
    "          \"***Is word: \", token.is_alpha, \n",
    "          \"***Is interpuction: \", token.is_punct, \n",
    "          \"***Is number: \", token.like_num,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:24:02.325470Z",
     "start_time": "2021-02-27T15:24:02.305517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nine True False True\n",
      "ten True False True\n",
      "dot True False False\n"
     ]
    }
   ],
   "source": [
    "short_text = nlp(\"nine ten dot\")\n",
    "for elem in short_text:\n",
    "    print(elem.text, elem.is_alpha, elem.is_punct, elem.like_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokud chceme tedy začistit český text, lze použít následující kód. Tady využíváme té skutečnosti, že oddělování slov je v angličtině stejné jako v češtině, takže můžeme použít i anglický model.  \n",
    "Bacha - v případě chybějící mezery za tečkou, např. u řetězce  \"prvni.druhe\", k oddělení slov nedojde (u čárky ale už ano)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:24:03.935612Z",
     "start_time": "2021-02-27T15:24:03.825623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uplynulých',\n",
       " 'dekádách',\n",
       " 'došlo',\n",
       " 'razantnímu',\n",
       " 'pokroku',\n",
       " 'elektronice',\n",
       " 'pokroku',\n",
       " 'velkému',\n",
       " 'narazil',\n",
       " 'fyzikální',\n",
       " 'meze',\n",
       " 'svých',\n",
       " 'možností',\n",
       " 'zlepšování',\n",
       " 'parametrů',\n",
       " 'používaných',\n",
       " 'součástek',\n",
       " 'šlo',\n",
       " 'ruku',\n",
       " 'ruce',\n",
       " 'miniaturizací',\n",
       " 'fundamentálních',\n",
       " 'důvodů',\n",
       " 'musí',\n",
       " 'zastavit',\n",
       " 'rozměrů',\n",
       " 'kolem',\n",
       " 'nm',\n",
       " 'škálách',\n",
       " 'začínají',\n",
       " 'projevovat',\n",
       " 'kvantové',\n",
       " 'vlastnosti',\n",
       " 'elektronů',\n",
       " 'není',\n",
       " 'kompatibilní',\n",
       " 'současnou',\n",
       " 'výpočetní',\n",
       " 'technikou',\n",
       " 'rámci',\n",
       " 'hledání',\n",
       " 'řešení',\n",
       " 'tohoto',\n",
       " 'problému',\n",
       " 'začal',\n",
       " 'posledních',\n",
       " 'letech',\n",
       " 'značně',\n",
       " 'vzrůstat',\n",
       " 'zájem',\n",
       " 'spintroniku',\n",
       " 'je',\n",
       " 'odvětví',\n",
       " 'elektroniky',\n",
       " 'klade',\n",
       " 'důraz',\n",
       " 'nejen',\n",
       " 'náboj',\n",
       " 'spin',\n",
       " 'nabitých',\n",
       " 'nosičů',\n",
       " 'je',\n",
       " 'nutné',\n",
       " 'podotknout',\n",
       " 'ačkoli',\n",
       " 'jedná',\n",
       " 'relativně',\n",
       " 'nový',\n",
       " 'vědní',\n",
       " 'obor',\n",
       " 'jedna',\n",
       " 'jejích',\n",
       " 'aplikací',\n",
       " 'čtecí',\n",
       " 'hlavy',\n",
       " 'pevných',\n",
       " 'discích',\n",
       " 'je',\n",
       " 'léta',\n",
       " 'široce',\n",
       " 'rozšířená']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = nlp(original_text)\n",
    "result_list = []\n",
    "for word in document:\n",
    "    if ( \n",
    "        (not word.like_num) \n",
    "        and (not word.is_punct) \n",
    "        and (word.text != \"\\n\") \n",
    "        and (word.lower_ not in CZECH_STOPWORDS)\n",
    "       ):\n",
    "        result_list.append(word.lower_)\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:24:06.655908Z",
     "start_time": "2021-02-27T15:24:06.635641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prvni.druhe\n",
      "treti\n",
      ",\n",
      "ctvrte\n"
     ]
    }
   ],
   "source": [
    "another_short_text = nlp(\"prvni.druhe treti,ctvrte\")\n",
    "for elem in another_short_text:\n",
    "    print(elem.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T21:33:51.577862Z",
     "start_time": "2020-03-26T21:33:51.573870Z"
    }
   },
   "source": [
    "### Stemming a lematizace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V běžném textu najdeme celou řadu slov, které jsou časováním, skloňováním atd. odvozeny z nějakého \"mateřského\" slova. Počítač ale tato příbuzná slova bude za normálních okolností brát jako zcela separátní jednotky. Tohle je značný problém u češtiny, ale v menší míře se vyskytuje i u angličtiny. Například slova \"working\", \"works\", \"worked\" atd. jsou všechna nějak napojena na slovo \"work\" a nemá tudíž smysl, aby při analýze byla od sebe oddělena. Jak lze ale slova převést na slovo původní?  \n",
    "Jedním možným řešením je stemming. De facto se jedná o osekávání koncovek s cílem dostat kořen slova. Problém ale spočívá ve skutečnosti, že pokud tento kořen bude stát samostatně mimo kontext okolních slov, nemusí být jasné, o jaké slovo se vlastně původně jednalo.  \n",
    "Cílem lemmatizace je oproti tomu najít slovo, ze kterého byla slova odvozená vytvořena. Snáze se tudíž interpretuje, ale o to obtížnější je lemmatizační model vytvořit. Mimo jiné je totiž často potřeba, aby algoritmus dokázal určit slovní druh jednotlivých slov ve větě. Například \"broken\" jako příslovce má lemma \"broken\", zatímco jako sloveso by mělo mít lemma \"break\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:24:08.275593Z",
     "start_time": "2021-02-27T15:24:08.247685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This work is dedicated to the study of two antiferromagnetic materials that are suitable for use in spintronic devices. In series of FeRh samples we studied the transition temperature between the antiferromagnetic and ferromagnetic phases. We developed a method based on material optical response for a quick determination of this temperature, which enabled us to study with a spatial resolution of 1 μm a magnetic inhomogeneity of prepared samples.We also developed a method for a determination of the Néel temperature and the magnetization easy axis position in thin films prepared from compensated antiferromagnetic metal. We successfully applied this method on an uniaxial sample of CuMnAs and we discussed its applicability for a research of samples with a biaxial magnetic anisotropy.\n",
      "Tato práce se zabývá studiem dvou antiferomagnetických materiálů, které jsou principielně použitelné pro spintronické aplikace. V sérii vzorků FeRh jsme studovali teplotu přechodu mezi antiferomagnetickou a feromagnetickou fází. Vyvinuli jsme na optické odezvě založenou metodu na rychlé určení této teploty, která nám následně umožnila studovat s prostorovým rozlišením 1 μm magnetickou nehomogenity připravených vzorků. Dále jsme vyvinuli metodu pro nalezení Néelovy teploty a snadné osy magnetizace v tenkých filmech připravených z kovového kompenzovaného antiferomagnetu. Tuto metodu jsme úspěšně aplikovali na uniaxiální vzorek CuMnAs a diskutovali jsme její použitelnost i pro vzorky s biaxiální magnetickou anizotropií.\n"
     ]
    }
   ],
   "source": [
    "english_text = \"This work is dedicated to the study of two antiferromagnetic materials that are suitable for use in spintronic devices. In series of FeRh samples we studied the transition temperature between the antiferromagnetic and ferromagnetic phases. We developed a method based on material optical response for a quick determination of this temperature, which enabled us to study with a spatial resolution of 1 μm a magnetic inhomogeneity of prepared samples.We also developed a method for a determination of the Néel temperature and the magnetization easy axis position in thin films prepared from compensated antiferromagnetic metal. We successfully applied this method on an uniaxial sample of CuMnAs and we discussed its applicability for a research of samples with a biaxial magnetic anisotropy.\"\n",
    "czech_text = \"Tato práce se zabývá studiem dvou antiferomagnetických materiálů, které jsou principielně použitelné pro spintronické aplikace. V sérii vzorků FeRh jsme studovali teplotu přechodu mezi antiferomagnetickou a feromagnetickou fází. Vyvinuli jsme na optické odezvě založenou metodu na rychlé určení této teploty, která nám následně umožnila studovat s prostorovým rozlišením 1 μm magnetickou nehomogenity připravených vzorků. Dále jsme vyvinuli metodu pro nalezení Néelovy teploty a snadné osy magnetizace v tenkých filmech připravených z kovového kompenzovaného antiferomagnetu. Tuto metodu jsme úspěšně aplikovali na uniaxiální vzorek CuMnAs a diskutovali jsme její použitelnost i pro vzorky s biaxiální magnetickou anizotropií.\"\n",
    "print(english_text)\n",
    "print(czech_text)\n",
    "\n",
    "interpunction = string.punctuation\n",
    "replacing_chars = \" \" * len(interpunction)\n",
    "english_text = english_text.translate(str.maketrans(interpunction, replacing_chars, \"\"))\n",
    "czech_text = czech_text.translate(str.maketrans(interpunction, replacing_chars, \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro stemming anglického textu použijeme PorterStemmer z balíčku gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:25:15.676232Z",
     "start_time": "2021-02-27T15:25:02.945715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thi work is dedic to the studi of two antiferromagnet materi that ar suitabl for us in spintron devic in seri of ferh sampl we studi the transit temperatur between the antiferromagnet and ferromagnet phase we develop a method base on materi optic respons for a quick determin of thi temperatur which enabl us to studi with a spatial resolut of 1 μm a magnet inhomogen of prepar sampl we also develop a method for a determin of the néel temperatur and the magnet easi axi posit in thin film prepar from compens antiferromagnet metal we successfulli appli thi method on an uniaxi sampl of cumna and we discuss it applic for a research of sampl with a biaxial magnet anisotropi'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "p = PorterStemmer()\n",
    "p.stem_sentence(english_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Všimněme si zde toho, že slova \"is\" a \"are\" byla převedena na \"is\" a \"ar\", nikoli na jedno sjednocující \"be\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T11:31:27.229518Z",
     "start_time": "2020-03-28T11:31:27.223539Z"
    }
   },
   "source": [
    "Co se českého stemmeru týče, našel jsem 10 let starý od Luisa Gomese a to sice na https://github.com/UFAL-DSG/alex/blob/master/alex/utils/czech_stemmer.py. Mimo jiné je z odkazu vidět, že stemmery jsou víceméně souborem heuristik, které musel znalec příslušného jazyka převést do počítačového kódu.  \n",
    "Jelikož je stemmer zjevně psaný pro Python 2, musí se:\n",
    "- změnit řádka 34 z \"    if isinstance(l, str) or isinstance(l, unicode):\" na \"    if isinstance(l, str):\"\n",
    "- na řádku 172 dát argument printu do závorek.  \n",
    "\n",
    "Pozn.: Pokud vám spuštění následující buňky povede k chybě, tak musíte do stejného adresáře, ve kterém bydlí notebook, stáhnout z Githubu soubor czech_stemmer.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:27:25.665353Z",
     "start_time": "2021-02-27T15:27:25.645255Z"
    }
   },
   "outputs": [],
   "source": [
    "import czech_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:27:29.285373Z",
     "start_time": "2021-02-27T15:27:29.265473Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tat',\n",
       " 'prák',\n",
       " 'se',\n",
       " 'zabýv',\n",
       " 'studi',\n",
       " 'dvo',\n",
       " 'antiferomagnetick',\n",
       " 'materiál',\n",
       " 'kter',\n",
       " 'jso',\n",
       " 'principieln',\n",
       " 'použiteln',\n",
       " 'pro',\n",
       " 'spintronick',\n",
       " 'aplikak',\n",
       " 'V',\n",
       " 'séri',\n",
       " 'vzork',\n",
       " 'FeRh',\n",
       " 'jsm',\n",
       " 'studoval',\n",
       " 'teplot',\n",
       " 'přechod',\n",
       " 'meh',\n",
       " 'antiferomagnetick',\n",
       " 'a',\n",
       " 'feromagnetick',\n",
       " 'fáz',\n",
       " 'Vyvinul',\n",
       " 'jsm',\n",
       " 'na',\n",
       " 'optick',\n",
       " 'odezv',\n",
       " 'založen',\n",
       " 'metod',\n",
       " 'na',\n",
       " 'rychl',\n",
       " 'určen',\n",
       " 'tét',\n",
       " 'teplot',\n",
       " 'kter',\n",
       " 'nám',\n",
       " 'následn',\n",
       " 'umožnil',\n",
       " 'stud',\n",
       " 's',\n",
       " 'prostor',\n",
       " 'rozliše',\n",
       " '1',\n",
       " 'μm',\n",
       " 'magnetick',\n",
       " 'nehomogenit',\n",
       " 'připraven',\n",
       " 'vzork',\n",
       " 'Dál',\n",
       " 'jsm',\n",
       " 'vyvinul',\n",
       " 'metod',\n",
       " 'pro',\n",
       " 'nalezen',\n",
       " 'Néel',\n",
       " 'teplot',\n",
       " 'a',\n",
       " 'snadn',\n",
       " 'osy',\n",
       " 'magnetizak',\n",
       " 'v',\n",
       " 'tenk',\n",
       " 'film',\n",
       " 'připraven',\n",
       " 'z',\n",
       " 'kovov',\n",
       " 'kompenzovan',\n",
       " 'antiferomagnet',\n",
       " 'Tut',\n",
       " 'metod',\n",
       " 'jsm',\n",
       " 'úspěšn',\n",
       " 'aplikoval',\n",
       " 'na',\n",
       " 'uniaxiáln',\n",
       " 'vzorek',\n",
       " 'CuMnAs',\n",
       " 'a',\n",
       " 'diskutoval',\n",
       " 'jsm',\n",
       " 'jej',\n",
       " 'použitelnost',\n",
       " 'i',\n",
       " 'pro',\n",
       " 'vzork',\n",
       " 's',\n",
       " 'biaxiáln',\n",
       " 'magnetick',\n",
       " 'anizotropi']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "czech_stemmer.cz_stem(czech_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co se týče lemmatizace anglického textu, je asi nejsnazší použít balíček spacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:28:17.905359Z",
     "start_time": "2021-02-27T15:28:17.355416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this this\n",
      "work work\n",
      "is be\n",
      "dedicated dedicate\n",
      "to to\n",
      "the the\n",
      "study study\n",
      "of of\n",
      "two two\n",
      "antiferromagnetic antiferromagnetic\n",
      "materials material\n",
      "that that\n",
      "are be\n",
      "suitable suitable\n",
      "for for\n",
      "use use\n",
      "in in\n",
      "spintronic spintronic\n",
      "devices device\n",
      "   \n",
      "in in\n",
      "series series\n",
      "of of\n",
      "ferh FeRh\n",
      "samples sample\n",
      "we we\n",
      "studied study\n",
      "the the\n",
      "transition transition\n",
      "temperature temperature\n",
      "between between\n",
      "the the\n",
      "antiferromagnetic antiferromagnetic\n",
      "and and\n",
      "ferromagnetic ferromagnetic\n",
      "phases phase\n",
      "   \n",
      "we we\n",
      "developed develop\n",
      "a a\n",
      "method method\n",
      "based base\n",
      "on on\n",
      "material material\n",
      "optical optical\n",
      "response response\n",
      "for for\n",
      "a a\n",
      "quick quick\n",
      "determination determination\n",
      "of of\n",
      "this this\n",
      "temperature temperature\n",
      "   \n",
      "which which\n",
      "enabled enable\n",
      "us we\n",
      "to to\n",
      "study study\n",
      "with with\n",
      "a a\n",
      "spatial spatial\n",
      "resolution resolution\n",
      "of of\n",
      "1 1\n",
      "μm μm\n",
      "a a\n",
      "magnetic magnetic\n",
      "inhomogeneity inhomogeneity\n",
      "of of\n",
      "prepared prepared\n",
      "samples sample\n",
      "we we\n",
      "also also\n",
      "developed develop\n",
      "a a\n",
      "method method\n",
      "for for\n",
      "a a\n",
      "determination determination\n",
      "of of\n",
      "the the\n",
      "néel Néel\n",
      "temperature temperature\n",
      "and and\n",
      "the the\n",
      "magnetization magnetization\n",
      "easy easy\n",
      "axis axis\n",
      "position position\n",
      "in in\n",
      "thin thin\n",
      "films film\n",
      "prepared prepare\n",
      "from from\n",
      "compensated compensated\n",
      "antiferromagnetic antiferromagnetic\n",
      "metal metal\n",
      "   \n",
      "we we\n",
      "successfully successfully\n",
      "applied apply\n",
      "this this\n",
      "method method\n",
      "on on\n",
      "an an\n",
      "uniaxial uniaxial\n",
      "sample sample\n",
      "of of\n",
      "cumnas cumnas\n",
      "and and\n",
      "we we\n",
      "discussed discuss\n",
      "its its\n",
      "applicability applicability\n",
      "for for\n",
      "a a\n",
      "research research\n",
      "of of\n",
      "samples sample\n",
      "with with\n",
      "a a\n",
      "biaxial biaxial\n",
      "magnetic magnetic\n",
      "anisotropy anisotropy\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "document = nlp(english_text)\n",
    "\n",
    "for word in document:\n",
    "    print(word.lower_, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že značný počet slov vůbec lematizací změněn nebyl. Občas se odsekne přípona -s spojená s množným číslem či -ed zajišťjící časování slovesa. Také ale vidíme, že se ze zájmen stalo -PRON- (to je z technických důvodů - https://github.com/explosion/spaCy/issues/962) a \"is\" se proměnilo v \"be\". Každopádně interpretace lemmat je vskutku mnohem snazší než interpretace stemmů."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro lemmatizaci českého textu lze použít balíček morphodita. Pro jeho fungování je nutno načíst soubor \"czech-morfflex-161115.dict\" - ten lze stáhnout [odtud](http://ufal.mff.cuni.cz/morphodita#download). Pro některé uživatele ale může být problematické, že zdrojová data modelů (tipoval bych, že https://github.com/UniversalDependencies/UD_Czech-PDT), se kterými balíček pracuje, jsou pod licencí [CC BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/4.0/) - tady ona písmena NC znamenajíc non-commercial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:30:45.755119Z",
     "start_time": "2021-02-27T15:30:43.645429Z"
    }
   },
   "outputs": [],
   "source": [
    "import ufal.morphodita as ufm\n",
    "morpho = ufm.Morpho.load(\"czech-morfflex-161115.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:30:48.365137Z",
     "start_time": "2021-02-27T15:30:48.335080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tato tata_,h_,n_^(tatínek)\n",
      "práce práce_^(jako_činnost_i_místo)\n",
      "se se_^(zvr._zájmeno/částice)\n",
      "zabývá zabývat_:T\n",
      "studiem studio\n",
      "dvou dva`2\n",
      "antiferomagnetických antiferomagnetický\n",
      "materiálů materiál\n",
      "které který\n",
      "jsou být\n",
      "principielně principielně_^(*4ální)\n",
      "použitelné použitelný_^(*6ít)\n",
      "pro pro-1\n",
      "spintronické spintronický\n",
      "aplikace aplikace\n",
      "v v-1\n",
      "sérii série\n",
      "vzorků vzorek\n",
      "ferh ferh\n",
      "jsme být\n",
      "studovali studovat_:T\n",
      "teplotu teplota\n",
      "přechodu přechod\n",
      "mezi mez\n",
      "antiferomagnetickou antiferomagneticka\n",
      "a a-1\n",
      "feromagnetickou feromagnetický\n",
      "fází fáze\n",
      "vyvinuli vyvinout\n",
      "jsme být\n",
      "na na-1\n",
      "optické optický\n",
      "odezvě odezva\n",
      "založenou založený_^(*3it)\n",
      "metodu metoda\n",
      "na na-1\n",
      "rychlé rychlý\n",
      "určení určený_^(*3it)\n",
      "této tento\n",
      "teploty teplota\n",
      "která který\n",
      "nám já\n",
      "následně následně_^(*1ý)\n",
      "umožnila umožnit_:W\n",
      "studovat studovat_:T\n",
      "s s-1\n",
      "prostorovým prostorový\n",
      "rozlišením rozlišení_^(*3it)\n",
      "1 1\n",
      "μm μ\n",
      "magnetickou magnetický\n",
      "nehomogenity nehomogenita\n",
      "připravených připravený_^(*3it)\n",
      "vzorků vzorek\n",
      "dále dál\n",
      "jsme být\n",
      "vyvinuli vyvinout\n",
      "metodu metoda\n",
      "pro pro-1\n",
      "nalezení nalezený-2_^(*7ézt-2)\n",
      "néelovy néelův_;S_^(*2)\n",
      "teploty teplota\n",
      "a a-1\n",
      "snadné snadný\n",
      "osy osa\n",
      "magnetizace magnetizace\n",
      "v v-1\n",
      "tenkých tenký\n",
      "filmech film\n",
      "připravených připravený_^(*3it)\n",
      "z z-1\n",
      "kovového kovový\n",
      "kompenzovaného kompenzovaný_^(*2t)\n",
      "antiferomagnetu antiferomagnet\n",
      "tuto tento\n",
      "metodu metoda\n",
      "jsme být\n",
      "úspěšně úspěšně_^(*1ý)\n",
      "aplikovali aplikovat_:T_:W\n",
      "na na-1\n",
      "uniaxiální uniaxiální\n",
      "vzorek vzorek\n",
      "cumnas cumnas_;S\n",
      "a a-1\n",
      "diskutovali diskutovat_:T\n",
      "jsme být\n",
      "její jeho_^(přivlast.)\n",
      "použitelnost použitelnost_^(*3ý)\n",
      "i i-1\n",
      "pro pro-1\n",
      "vzorky vzorek\n",
      "s s-1\n",
      "biaxiální biaxiální\n",
      "magnetickou magnetický\n",
      "anizotropií anizotropie\n"
     ]
    }
   ],
   "source": [
    "lemmas = ufm.TaggedLemmas()\n",
    "tokens = czech_text.lower().split()\n",
    "for token in tokens:\n",
    "    morpho.analyze(token, morpho.GUESSER, lemmas)\n",
    "    print(token, lemmas[0].lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co máme ale dělat, pokud chceme lemmatizátor českého textu a náš use-case je \"commercial\"? V takovém případě lze využít balíček majka a na něj navázané [jazykové modely](https://nlp.fi.muni.cz/ma/), které jsou pod volnější [CC BY-SA licencí](https://creativecommons.org/licenses/by-sa/3.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:03:09.068088Z",
     "start_time": "2021-02-27T15:03:07.277843Z"
    }
   },
   "outputs": [],
   "source": [
    "import majka\n",
    "morph_majka = majka.Majka(\"majka.w-lt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defaultní výstupem majky není jedno lemma, ale soubor všech možných lemmat včetně informace o pádu, rodu, negaci apod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:07:19.187136Z",
     "start_time": "2021-02-27T15:07:19.162264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lemma': 'vhodný',\n",
       "  'tags': {'pos': 'adjective',\n",
       "   'negation': True,\n",
       "   'gender': 'feminine',\n",
       "   'plural': True,\n",
       "   'case': 1,\n",
       "   'degree': 3}},\n",
       " {'lemma': 'vhodný',\n",
       "  'tags': {'pos': 'adjective',\n",
       "   'negation': True,\n",
       "   'gender': 'feminine',\n",
       "   'plural': True,\n",
       "   'case': 4,\n",
       "   'degree': 3}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_majka.find(\"nejnevhodnější\")[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To častokrát nechceme, naštěstí se však dá upovídaný výstup omezit. Lze nastavit, že se člověku zobrazí jen první lemma s příslušenstvím (flag *first_only*), že se přídavné informace nezobrazí (flag *tags*) a že se negace lemmatu projeví přidáním předpony \"ne\" (flag *negative*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:09:20.377038Z",
     "start_time": "2021-02-27T15:09:20.366991Z"
    }
   },
   "outputs": [],
   "source": [
    "morph_majka.first_only = True \n",
    "morph_majka.tags = False\n",
    "morph_majka.negative = \"ne\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pak u normálního českého slova vidíme něco na tento způsob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:09:40.206961Z",
     "start_time": "2021-02-27T15:09:40.196844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lemma': 'nevhodný'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_majka.find(\"nejnevhodnější\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Když se podíváme na velikosti jazykových modelů pro morphoditu a majku, vidíme, že ten první má trojnásobnou velikost. To znamená, že majka nezná lemmata vzácnějších slov. Výše jsme viděli, že morphodita správně lemmatizovala slovo \"antiferomagnetu\". Majka si s ním bohužel neporadí a dokonce ani nevrátí původní slovo, nýbrž jen prázdný list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:11:39.476919Z",
     "start_time": "2021-02-27T15:11:39.446818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_majka.find(\"antiferomagnetu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na to se pak musí myslet při lemmatizaci celého textu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T15:14:02.256671Z",
     "start_time": "2021-02-27T15:14:02.236827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tata\n",
      "práce\n",
      "se\n",
      "zabývat\n",
      "studio\n",
      "dva\n",
      "antiferomagnetických\n",
      "materiál\n",
      "který\n",
      "být\n",
      "principielně\n",
      "použitelný\n",
      "pro\n",
      "spintronické\n",
      "aplikace\n",
      "v\n",
      "série\n",
      "vzorek\n",
      "ferh\n",
      "být\n",
      "studovat\n",
      "teplota\n",
      "přechod\n",
      "mezi\n",
      "antiferomagnetickou\n",
      "a\n",
      "feromagnetickou\n",
      "fáze\n",
      "vyvinout\n",
      "být\n",
      "na\n",
      "optický\n",
      "odezva\n",
      "založený\n",
      "metoda\n",
      "na\n",
      "rychlý\n",
      "určení\n",
      "tento\n",
      "teplota\n",
      "který\n",
      "my\n",
      "následně\n",
      "umožnit\n",
      "studovat\n",
      "s\n",
      "prostorový\n",
      "rozlišení\n",
      "1\n",
      "μm\n",
      "magnetický\n",
      "nehomogenity\n",
      "připravený\n",
      "vzorek\n",
      "dále\n",
      "být\n",
      "vyvinout\n",
      "metoda\n",
      "pro\n",
      "nalezení\n",
      "néelovy\n",
      "teplota\n",
      "a\n",
      "snadný\n",
      "osa\n",
      "magnetizace\n",
      "v\n",
      "tenký\n",
      "film\n",
      "připravený\n",
      "z\n",
      "kovový\n",
      "kompenzovaný\n",
      "antiferomagnetu\n",
      "tento\n",
      "metoda\n",
      "být\n",
      "úspěšně\n",
      "aplikovat\n",
      "na\n",
      "uniaxiální\n",
      "vzorek\n",
      "cumnas\n",
      "a\n",
      "diskutovat\n",
      "být\n",
      "její\n",
      "použitelnost\n",
      "i\n",
      "pro\n",
      "vzorek\n",
      "s\n",
      "biaxiální\n",
      "magnetický\n",
      "anizotropií\n"
     ]
    }
   ],
   "source": [
    "tokens = czech_text.lower().split()\n",
    "for token in tokens:\n",
    "    lemma_list = morph_majka.find(token)\n",
    "    if lemma_list:\n",
    "        print(lemma_list[0][\"lemma\"])\n",
    "    else:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vektorizéry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejme tomu, že jsme už text vyčistili. Nicméně předat ho nějakému algoritmu, dejme tomu logistické regresi, ještě nemůžeme. Většina algoritmů by si totiž se slovy neporadila - očekávají totiž vstup v podobě čísel. Převod slov na čísla tak musíme zajistit vektorizéry (přesněji jejich funkcí *fit*). Ty vezmou nějakou sadu vět (obvykle trénovací data, ale není to bezpodmíněčně nutné) a každému slovu z těchto vět přiřadí unikátní IDčko.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T16:02:16.782240Z",
     "start_time": "2021-02-27T16:02:12.787085Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T16:02:23.052342Z",
     "start_time": "2021-02-27T16:02:21.177528Z"
    }
   },
   "outputs": [],
   "source": [
    "source_documents = [\n",
    "\"This work is dedicated to the study of two antiferromagnetic materials that are suitable for use in spintronic devices.\",\n",
    "\"In series of FeRh samples we studied the transition temperature between the antiferromagnetic and ferromagnetic phases.\",\n",
    "\"We developed a method based on material optical response for a quick determination of this temperature.\",\n",
    "\"This enabled us to study with a spatial resolution of 1 μm a magnetic inhomogeneity of prepared samples.\",\n",
    "\"We also developed a method for a determination of the Néel temperature and the magnetization easy axis position in thin films prepared from compensated antiferromagnetic metal.\",\n",
    "\"We successfully applied this method on an uniaxial sample of CuMnAs and we discussed its applicability for a research of samples with a biaxial magnetic anisotropy.\"]\n",
    "commmon_vectorizer = CountVectorizer()\n",
    "commmon_vectorizer.fit(source_documents);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokud chceme znát mapování mezi slovy a čísly, podíváme se na proměnnou vocabulary_. Ta je významově i implementačně slovníkem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:15:14.716897Z",
     "start_time": "2020-04-12T10:15:14.707923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 60,\n",
       " 'work': 69,\n",
       " 'is': 28,\n",
       " 'dedicated': 14,\n",
       " 'to': 61,\n",
       " 'the': 58,\n",
       " 'study': 53,\n",
       " 'of': 37,\n",
       " 'two': 63,\n",
       " 'antiferromagnetic': 4,\n",
       " 'materials': 33,\n",
       " 'that': 57,\n",
       " 'are': 7,\n",
       " 'suitable': 55,\n",
       " 'for': 24,\n",
       " 'use': 66,\n",
       " 'in': 26,\n",
       " 'spintronic': 51,\n",
       " 'devices': 17,\n",
       " 'series': 49,\n",
       " 'ferh': 21,\n",
       " 'samples': 48,\n",
       " 'we': 67,\n",
       " 'studied': 52,\n",
       " 'transition': 62,\n",
       " 'temperature': 56,\n",
       " 'between': 10,\n",
       " 'and': 2,\n",
       " 'ferromagnetic': 22,\n",
       " 'phases': 40,\n",
       " 'developed': 16,\n",
       " 'method': 35,\n",
       " 'based': 9,\n",
       " 'on': 38,\n",
       " 'material': 32,\n",
       " 'optical': 39,\n",
       " 'response': 46,\n",
       " 'quick': 43,\n",
       " 'determination': 15,\n",
       " 'enabled': 20,\n",
       " 'us': 65,\n",
       " 'with': 68,\n",
       " 'spatial': 50,\n",
       " 'resolution': 45,\n",
       " 'μm': 70,\n",
       " 'magnetic': 30,\n",
       " 'inhomogeneity': 27,\n",
       " 'prepared': 42,\n",
       " 'also': 0,\n",
       " 'néel': 36,\n",
       " 'magnetization': 31,\n",
       " 'easy': 19,\n",
       " 'axis': 8,\n",
       " 'position': 41,\n",
       " 'thin': 59,\n",
       " 'films': 23,\n",
       " 'from': 25,\n",
       " 'compensated': 12,\n",
       " 'metal': 34,\n",
       " 'successfully': 54,\n",
       " 'applied': 6,\n",
       " 'an': 1,\n",
       " 'uniaxial': 64,\n",
       " 'sample': 47,\n",
       " 'cumnas': 13,\n",
       " 'discussed': 18,\n",
       " 'its': 29,\n",
       " 'applicability': 5,\n",
       " 'research': 44,\n",
       " 'biaxial': 11,\n",
       " 'anisotropy': 3}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commmon_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pakliže bychom chtěli znát pouze ve slovníku uvedená slova:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:15:24.893752Z",
     "start_time": "2020-04-12T10:15:24.885811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['also',\n",
       " 'an',\n",
       " 'and',\n",
       " 'anisotropy',\n",
       " 'antiferromagnetic',\n",
       " 'applicability',\n",
       " 'applied',\n",
       " 'are',\n",
       " 'axis',\n",
       " 'based',\n",
       " 'between',\n",
       " 'biaxial',\n",
       " 'compensated',\n",
       " 'cumnas',\n",
       " 'dedicated',\n",
       " 'determination',\n",
       " 'developed',\n",
       " 'devices',\n",
       " 'discussed',\n",
       " 'easy',\n",
       " 'enabled',\n",
       " 'ferh',\n",
       " 'ferromagnetic',\n",
       " 'films',\n",
       " 'for',\n",
       " 'from',\n",
       " 'in',\n",
       " 'inhomogeneity',\n",
       " 'is',\n",
       " 'its',\n",
       " 'magnetic',\n",
       " 'magnetization',\n",
       " 'material',\n",
       " 'materials',\n",
       " 'metal',\n",
       " 'method',\n",
       " 'néel',\n",
       " 'of',\n",
       " 'on',\n",
       " 'optical',\n",
       " 'phases',\n",
       " 'position',\n",
       " 'prepared',\n",
       " 'quick',\n",
       " 'research',\n",
       " 'resolution',\n",
       " 'response',\n",
       " 'sample',\n",
       " 'samples',\n",
       " 'series',\n",
       " 'spatial',\n",
       " 'spintronic',\n",
       " 'studied',\n",
       " 'study',\n",
       " 'successfully',\n",
       " 'suitable',\n",
       " 'temperature',\n",
       " 'that',\n",
       " 'the',\n",
       " 'thin',\n",
       " 'this',\n",
       " 'to',\n",
       " 'transition',\n",
       " 'two',\n",
       " 'uniaxial',\n",
       " 'us',\n",
       " 'use',\n",
       " 'we',\n",
       " 'with',\n",
       " 'work',\n",
       " 'μm']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commmon_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní bychom chtěli podle výše uvedeného slovníku provést převod textových dokumentů na vektory. To zrealizujeme pomocí funkce *transform*. V případě, že bychom takto chtěli transformovat trénovací data, museli bych *trasform* vypustit na ty samá data, na která byla vypuštěna i funkce *fit*. Pro zefektivnění tohoto častého případu proto existuje funkce *fit_transform*, která v sobě oba diskutované procesy spojuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:18:37.160544Z",
     "start_time": "2020-04-12T10:18:37.155558Z"
    }
   },
   "outputs": [],
   "source": [
    "new_document = [\"some antiferromagnetic materials can be useful, another antiferromagnetic materials are useless\",\n",
    "\"are plastic bottles antiferromagnetic\"]\n",
    "new_document_vector = commmon_vectorizer.transform(new_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Délka vektorů bude odpovídat počtu elementů ve slovníku. Pakliže slovo s IDčkem i v trasformovaném dokumentu neexistuje, bude i-tý prvek vektoru nulový. Pakliže je takových slov v dokumentu dejme tomu pět a vektorizér, který byl použit, je CountVectorizer, byde na i-té pozici vektoru pětka.  \n",
    "Při printění vektoru se pro zvýšení přehlednosti neukazuje celý vektor, ale pouze jeho nenulové prvky. První číslo v tuplu udává index dokumentu. Druhé číslo tuplu odpovídá pořadovému číslo elementu vektoru, který není nulový. Vpravo pak stojí hodnota na tomto indexu alias počet výskytů slova v dokumentu.  \n",
    "Pakliže se v dokumentu objevilo slovo, které není ve slovníku, vůbec nijak se ve vektoru nepromítne.  \n",
    "Jako příklad si vezměme větu \"are plastic bottles antiferromagnetic\". Slova \"platic\" a \"bottles\" ve slovníku nejsou, takže je ignorujeme. Slovo \"are\" má ve slovníku ID 7, slovo \"antiferromagnetic\" má ID 4. Obě slova se v dokumentu objevují pouze jednou. Vektor dokumentu má tak tvar (0, 0, 0, 0, 1, 0, 0, 1, 0, ... , 0). Pro zjednodušení se vypíší jen nenulové elementy. Jelikož se jedná o druhý předaný dokument (a indexace začíná od nuly), píšeme  \n",
    "(1, 4)  1  \n",
    "(1, 7)  1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:19:16.170860Z",
     "start_time": "2020-04-12T10:19:16.044364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector matrix\n",
      "(2, 71)\n",
      "1st document vector\n",
      "  (0, 4)\t2\n",
      "  (0, 7)\t1\n",
      "  (0, 33)\t2\n",
      "2nd document vector\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "Both vectors\n",
      "  (0, 4)\t2\n",
      "  (0, 7)\t1\n",
      "  (0, 33)\t2\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "2nd document vector not in compressed but in normal representation\n",
      "[[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of vector matrix\")\n",
    "print(new_document_vector.shape)\n",
    "print(\"1st document vector\")\n",
    "print(new_document_vector[0])\n",
    "print(\"2nd document vector\")\n",
    "print(new_document_vector[1])\n",
    "print(\"Both vectors\")\n",
    "print(new_document_vector)\n",
    "print(\"2nd document vector not in compressed but in normal representation\")\n",
    "print(new_document_vector[1].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokud se chceme rychle podívat, jaká slova vektor vlastně zachycuje, použijeme funkce *inverse_transform*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:19:24.578787Z",
     "start_time": "2020-04-12T10:19:24.476638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['antiferromagnetic', 'are', 'materials'], dtype='<U17'),\n",
       " array(['antiferromagnetic', 'are'], dtype='<U17')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commmon_vectorizer.inverse_transform(new_document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní jsme tedy převedli jednotlivé dokumenty na poslopunosti čísel, které mohou být vstupem dejme tomu pro logisticku regresi. Nicméně použití je i jiné - lze vypočítat, jak moc se mezi sebou jednotlivé dokumenty liší. Asi nejjednodušší metrika toto umožňující je kosinová podobnost K definovaná jako    \n",
    "$K(X, Y) = \\frac {<X, Y>}{(||X||*||Y||)}$   \n",
    "Zde <X,Y> je skalární součin mezi vektory X a Y, ||X|| je velikost vektoru X (tj. odmocnina ze sumy druhých mocnin jednotlivých složek vektoru).  \n",
    "Kosinovou podobnost si můžeme spočítat sami, anebo lze použít funkci z sci-kit learnu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:21:17.839362Z",
     "start_time": "2020-04-12T10:21:17.681957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First vector\n",
      "  (0, 4)\t2\n",
      "  (0, 7)\t1\n",
      "  (0, 33)\t2\n",
      "Second vector\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "Magnitude of the first and the second vector\n",
      "3.0 1.4142135623730951\n",
      "Scalar product of vectors\n",
      "[[3]]\n",
      "Cosine similarity\n",
      "[[0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"First vector\")\n",
    "print(new_document_vector[0])\n",
    "print(\"Second vector\")\n",
    "print(new_document_vector[1])\n",
    "print(\"Magnitude of the first and the second vector\")\n",
    "vector_abs_value_1 = np.linalg.norm(new_document_vector[0].toarray())\n",
    "vector_abs_value_2 = np.linalg.norm(new_document_vector[1].toarray())\n",
    "print(vector_abs_value_1, vector_abs_value_2)\n",
    "print(\"Scalar product of vectors\")\n",
    "scalar_product = new_document_vector[0].toarray()@new_document_vector[1].toarray().transpose()\n",
    "print(scalar_product)\n",
    "print(\"Cosine similarity\")\n",
    "print(scalar_product/(vector_abs_value_1*vector_abs_value_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:22:52.165817Z",
     "start_time": "2020-04-12T10:22:51.518697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(new_document_vector[0],new_document_vector[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check - podobnost vektoru k sobě samému je 1 (alias 100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:22:54.803068Z",
     "start_time": "2020-04-12T10:22:54.795125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(new_document_vector[1],new_document_vector[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K čemu to může být dobré? Například k tomu, že člověk zadá nějaký dokument (větu) a posléze nalezne v množině jiných dokumentů ten, který bude tomu původnímu nejpodobnější.  \n",
    "Níže vidíme, že dokument \"are plastic bottles antiferromagnetic\" má nejblíže k \"This work is dedicated to the study of two antiferromagnetic materials that are suitable for use in spintronic devices.\" Oproti tomu s \"We developed a method based on material optical response for a quick determination of this temperature.\" nemá nic společného (není tu ani \"are\", ani \"antiferromagnetic\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:24:03.157643Z",
     "start_time": "2020-04-12T10:24:03.142685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32444284]]\n",
      "[[0.16666667]]\n",
      "[[0.]]\n",
      "[[0.]]\n",
      "[[0.13867505]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "user_document = new_document_vector[1]\n",
    "transformed_documents = commmon_vectorizer.transform(source_documents)\n",
    "no_of_original_docs = transformed_documents.shape[0]\n",
    "for index in range(no_of_original_docs):\n",
    "    similarity = cosine_similarity(transformed_documents[index],user_document)\n",
    "    print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Může se stát, že v dokumentech, s jejichž pomocí slovník vytváříme, budou velice vzácná slova. To by samo o sobě nebyl problém, koneckonců takové údaje pak mohou určitou skupinu dokumentů charakterizovat. Komplikace nastávají v situaci, kdy se určité slovo vyskytuje pouze v jednom dokumentu (a je jedno, jestli jednou anebo desetkrát). Pak je na zvážení, jestli by se takové slovo nemělo z dokumentu vyhodit, neboť se v dokumentech později transformovaných slovníkem na vektory asi také nevyskytne. Pouze tak zbytečně zvětší délku vektorů. Proto lze při vytváření slovníku vložit do parametrů funkce atribut *min_df*, který tyto typy slov vyřadí.  \n",
    "V příkladu níže vyhazujeme všecchna slova, která se nevyskytují aspoň ve dvou dokumentech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:30:56.877672Z",
     "start_time": "2020-04-12T10:30:56.870731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'antiferromagnetic',\n",
       " 'determination',\n",
       " 'developed',\n",
       " 'for',\n",
       " 'in',\n",
       " 'magnetic',\n",
       " 'method',\n",
       " 'of',\n",
       " 'on',\n",
       " 'prepared',\n",
       " 'samples',\n",
       " 'study',\n",
       " 'temperature',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'we',\n",
       " 'with']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_documents = [\n",
    "\"This work is dedicated to the study of two antiferromagnetic materials that are suitable for use in spintronic devices.\",\n",
    "\"In series of FeRh samples we studied the transition temperature between the antiferromagnetic and ferromagnetic phases.\",\n",
    "\"We developed a method based on material optical response for a quick determination of this temperature.\",\n",
    "\"This enabled us to study with a spatial resolution of 1 μm a magnetic inhomogeneity of prepared samples.\",\n",
    "\"We also developed a method for a determination of the Néel temperature and the magnetization easy axis position in thin films prepared from compensated antiferromagnetic metal.\"\n",
    "\"We successfully applied this method on an uniaxial sample of CuMnAs and we discussed its applicability for a research of samples with a biaxial magnetic anisotropy.\"]\n",
    "commmon_vectorizer = CountVectorizer(min_df=2)\n",
    "commmon_vectorizer.fit(source_documents)\n",
    "commmon_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slovník se razantně zmenšil. Na druhou stranu vidíme, že je v něm hromada stopwords. Ty buď odstraníme postupy popsanými v předchozí sekci, anebo aplikujeme parametr *max_df=X*. Díky jemu se vyhodí všechny slova, která se vyskytují ve více než X dokumentech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:31:07.862538Z",
     "start_time": "2020-04-12T10:31:07.854560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'determination',\n",
       " 'developed',\n",
       " 'magnetic',\n",
       " 'method',\n",
       " 'on',\n",
       " 'prepared',\n",
       " 'study',\n",
       " 'to',\n",
       " 'with']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_documents = [\n",
    "\"This work is dedicated to the study of two antiferromagnetic materials that are suitable for use in spintronic devices.\",\n",
    "\"In series of FeRh samples we studied the transition temperature between the antiferromagnetic and ferromagnetic phases.\",\n",
    "\"We developed a method based on material optical response for a quick determination of this temperature.\",\n",
    "\"This enabled us to study with a spatial resolution of 1 μm a magnetic inhomogeneity of prepared samples.\",\n",
    "\"We also developed a method for a determination of the Néel temperature and the magnetization easy axis position in thin films prepared from compensated antiferromagnetic metal.\"\n",
    "\"We successfully applied this method on an uniaxial sample of CuMnAs and we discussed its applicability for a research of samples with a biaxial magnetic anisotropy.\"]\n",
    "commmon_vectorizer = CountVectorizer(min_df=2, max_df=2)\n",
    "commmon_vectorizer.fit(source_documents)\n",
    "commmon_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jiným způsobem, jak redukovat velikost slovníku, je do funkce fit  vložit *max_meatures=X*. Do slovníku se tak dostane jen X nejčastěji zastoupených slov v korpusu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:31:13.306685Z",
     "start_time": "2020-04-12T10:31:13.197335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'antiferromagnetic',\n",
       " 'for',\n",
       " 'in',\n",
       " 'method',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'the',\n",
       " 'this',\n",
       " 'we']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_documents = [\n",
    "\"This work is dedicated to the study of two antiferromagnetic materials that are suitable for use in spintronic devices.\",\n",
    "\"In series of FeRh samples we studied the transition temperature between the antiferromagnetic and ferromagnetic phases.\",\n",
    "\"We developed a method based on material optical response for a quick determination of this temperature.\",\n",
    "\"This enabled us to study with a spatial resolution of 1 μm a magnetic inhomogeneity of prepared samples.\",\n",
    "\"We also developed a method for a determination of the Néel temperature and the magnetization easy axis position in thin films prepared from compensated antiferromagnetic metal.\"\n",
    "\"We successfully applied this method on an uniaxial sample of CuMnAs and we discussed its applicability for a research of samples with a biaxial magnetic anisotropy.\"]\n",
    "commmon_vectorizer = CountVectorizer(max_features=10)\n",
    "commmon_vectorizer.fit(source_documents)\n",
    "commmon_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vektorizéry počítají se zjednodušujícím předpokladem, že uspořádání slov v dokumentech nehraje roli. Může se ale často stávat, že dvě slova často stojící vedle sebe mají specifický význam, např. \"Česká republika\". Těmto dvojicím se říká bigramy, trojice by se nazývaly trigramy, samostatně stojící slovo je unigram a název pro N slov stojících u sebe je N-gram.   Vektorizéry umožňují do slovníku přidat i tyto konstrukce a to sice pomocí parametru *ngram_range=(1, 2)*. První číslo udává minimální velikost N-gramu, druhé číslo jeho velikost maximální. Tj. v uvedeném příkladu se do slovníku dostanou unigramy a bigramy.  \n",
    "Použití N-gramů bez dalších opatření vede ke zvětšování slovníku o víceméně náhodná sousloví. Proto je potřeba slovník nějak omezit, např. pomocí *min_df*.  \n",
    "Pozn.: v příkladu níže vyplavaly víceméně díky náhodě dva bigramy, u kterých o žádné ustálené slovní spojení nejde. Tomu se dá předejít dostatečně velkým korpusem a dostatečně velkou hodnotou min_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:33:57.281321Z",
     "start_time": "2020-04-12T10:33:57.273377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'antiferromagnetic',\n",
       " 'determination',\n",
       " 'determination of',\n",
       " 'developed',\n",
       " 'developed method',\n",
       " 'for',\n",
       " 'in',\n",
       " 'magnetic',\n",
       " 'method',\n",
       " 'of',\n",
       " 'on',\n",
       " 'prepared',\n",
       " 'samples',\n",
       " 'study',\n",
       " 'temperature',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'we',\n",
       " 'with']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_documents = [\n",
    "\"This work is dedicated to the study of two antiferromagnetic materials that are suitable for use in spintronic devices.\",\n",
    "\"In series of FeRh samples we studied the transition temperature between the antiferromagnetic and ferromagnetic phases.\",\n",
    "\"We developed a method based on material optical response for a quick determination of this temperature.\",\n",
    "\"This enabled us to study with a spatial resolution of 1 μm a magnetic inhomogeneity of prepared samples.\",\n",
    "\"We also developed a method for a determination of the Néel temperature and the magnetization easy axis position in thin films prepared from compensated antiferromagnetic metal.\"\n",
    "\"We successfully applied this method on an uniaxial sample of CuMnAs and we discussed its applicability for a research of samples with a biaxial magnetic anisotropy.\"]\n",
    "commmon_vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "commmon_vectorizer.fit(source_documents)\n",
    "commmon_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Vypuštění natrénovaného count vektorizéru na dokument povede k vytvoření vektoru, jehož jednotlivé složky budou odpovídat počtu výskytu určitého slova v dokumentu. Je tak tedy zajištěno, že pokud je termín X v dokumentu víckrát, bude mít i příslušná složka vektoru větší váhu. Tudíž pak bude tento vektor více podobný jinému vektoru zastupujícímu dokument z podobné oblasti. Co ale když je termín X typický pro všechny dokumenty? Potom nám jeho velká četnost pro při určování podobnosti dokumentů moc nepomůže. Chtělo by to do vektorů zahrnout informace nejen o zastoupení termínu v jednom dokumentu, ale i o četnosti termínu ve všech studovaných dokumentech. Právě to zajišťuje Tf-Idf metrika.  \n",
    "*Tf* značí *term-frequency*. Samotný počet výskytů termínu by nebyl dostatečně průkazný - přeci jen je rozdíl, jestli se určité slovo vyskytuje dvakrát ve větě složené z 5 anebo z 50 slov. Term frequency už v sobě proto toto normování na délku věty zahrnuje. Pro *Tf* slova X platí vztah  \n",
    "> Tf(X) = (počet výskytů slova X v dokumentu)/(počet slov v dokumentu)  \n",
    "\n",
    "Zkratka *Idf* znamená *inverse document frequency*. Vystihuje, jak je určité slovo z hlediska množiny zkoumaných dokumentů informačně hodnotné. Například pokud zkoumáne recenze filmů, bude slovo \"film\" téměř v každém dokumentu a tak se na nějaké určování podobnosti textů použít nedá. Naopak užitečné pro nás budou slova vyskytující se jen ve výseku dokumentů.\n",
    "Platí rovnice  \n",
    "> Idf(X) = log(počet dokumentů/počet dokumentů obsahující slovo X)+1  \n",
    "\n",
    "Jednička se v rovnici vyskytuje proto, aby nebylo ani slovo, které se vyskytujue v každém dokumentu a tudíž je pro něj logaritmus nulový, úplně opomenuto. V takovémto případu máme Idf = log(1)+1=0+1=1. Naopak pokud je slovo obsaženo jen v jednom z 10000 dokumentů, dostaneme po dosazení Idf = log(10000)+1 = 5  \n",
    "Ideální případ by nastal, pokud by klíčové slovo bylo jen v jednom dokumentu, ale mnohokrát. Proto pro výslednou metriku spolu Tf a Idf pronásobíme.  \n",
    "Samozřejmě ručně nemusíme počítat nic, stačí na text vypustit *TfidfVectorizer*. Jedná se víceméně o *CountVectorizer*, za který je napojen *TfidfTransformer* provádějící výše popsané operace.  \n",
    "Asi je ale nutné poznamentat, že sci-kit learn nepoužívá výše uvedené \"učebnicové\" vzorce. Tf totiž na počet slov v dokumentu nenormuje. Defaultní vztah sci-kit learnu pro Idf má zase tvar\n",
    "> Idf(X) = ln( (počet dokumentů+1)/(počet dokumentů obsahující slovo X+1) )+1  \n",
    "\n",
    "Zde ln je přirozený logaritmus (tj. základem je eulerovo číslo o velikosti cca 2,72).  \n",
    "Výsledný vektor Tf-Idf pro určitý dokument pak normuje dělením každého elementu vektoru velikostí vektoru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:37:55.134733Z",
     "start_time": "2020-04-12T10:37:54.991624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 33)\t0.7604405547137439\n",
      "  (0, 7)\t0.38022027735687197\n",
      "  (0, 4)\t0.5264624425667197\n",
      "  (1, 7)\t0.8221903715494888\n",
      "  (1, 4)\t0.5692126078464125\n"
     ]
    }
   ],
   "source": [
    "source_documents = [\n",
    "\"This work is dedicated to the study of two antiferromagnetic materials that are suitable for use in spintronic devices.\",\n",
    "\"In series of FeRh samples we studied the transition temperature between the antiferromagnetic and ferromagnetic phases.\",\n",
    "\"We developed a method based on material optical response for a quick determination of this temperature.\",\n",
    "\"This enabled us to study with a spatial resolution of 1 μm a magnetic inhomogeneity of prepared samples.\",\n",
    "\"We also developed a method for a determination of the Néel temperature and the magnetization easy axis position in thin films prepared from compensated antiferromagnetic metal.\",\n",
    "\"We successfully applied this method on an uniaxial sample of CuMnAs and we discussed its applicability for a research of samples with a biaxial magnetic anisotropy.\"]\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(source_documents)\n",
    "new_document = [\"some antiferromagnetic materials can be useful, another antiferromagnetic materials are useless\",\n",
    "\"are plastic bottles antiferromagnetic\"]\n",
    "new_document_vector= tfidf_vectorizer.transform(new_document)\n",
    "print(new_document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co máme udělat, pokud si přeci jen budeme chtít spočítat Tf-Idf ručně, třeba pro druhý nový vektor? Nejprve si spočítáme Idf. To nebere v úvahu nové vektory, nýbrž jen trénovací korpus, na který byla použita funce *fit*. Nové vektory jsou použity jen pro určení Tf jednotlivých slov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:38:22.255299Z",
     "start_time": "2020-04-12T10:38:22.245286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idf for word 'antiferromangetic' (i.e. word 4):  1.5596157879354227\n",
      "Idf for word 'are' (i.e. word 7):  2.252762968495368\n",
      "Term frequencies for both words in 2nd new vector are equal to 1. Thus Tf-Idf = Idf*1.\n",
      "Tf-idf after normalization\n",
      "   for word 'antiferromangetic' (i.e. word 4):  0.5692126078464125\n",
      "   for word 'are' (i.e. word 7):  0.8221903715494888\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "number_of_documents = 6\n",
    "doc_frequency_antiferromagnetic = 3\n",
    "doc_frequency_are = 1\n",
    "\n",
    "idf_antiferromagnetic = math.log((1+number_of_documents)/(1+doc_frequency_antiferromagnetic))+1\n",
    "idf_are = math.log((1+number_of_documents)/(1+doc_frequency_are))+1\n",
    "\n",
    "print(\"Idf for word 'antiferromangetic' (i.e. word 4): \", idf_antiferromagnetic)\n",
    "print(\"Idf for word 'are' (i.e. word 7): \", idf_are)\n",
    "print(\"Term frequencies for both words in 2nd new vector are equal to 1. Thus Tf-Idf = Idf*1.\")\n",
    "norm_denominator = math.sqrt(idf_antiferromagnetic*idf_antiferromagnetic+idf_are*idf_are)\n",
    "print(\"Tf-idf after normalization\")\n",
    "print(\"   for word 'antiferromangetic' (i.e. word 4): \", idf_antiferromagnetic/norm_denominator)\n",
    "print(\"   for word 'are' (i.e. word 7): \", idf_are/norm_denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T20:32:24.758584Z",
     "start_time": "2020-04-01T20:32:24.754651Z"
    }
   },
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf je užitečná v tom, že kvantifikuje důležitost slova pro případnou klasifikaci dokumentu. Každý vektor dokumenty zastupující má ale stále velikost odpovídající velikosti slovníku, což úplně optimální není. Navíc metoda mnoho neprozrazuje o vnitřní podstatě dokumetu. Tyto problémy se snaží reflektovat latentní Dirichletova alokace (LDA).    \n",
    "I LDA pracuje s bag-of-words přístupem, tedy nezáleží ji na pořadí slov v dokumentu. Mluví se tak o předpokladu jejich zaměnitelnosti. To ale neznamená, že výskyt určitého slova v dokumentu je náhodný, na ničem nezávislý. Ve skutečnosti je *podmíněně* nezávislý, tj. závisí na \"neviditelném\" latentním parametru - tématu. LDA je postavena na tom, že dokumenty mohou být reprezentovány jako směs těchto témat, přičemž každé téma je charakterizováno jako pravděpodobnostní rozdělení přes množinu slov. To v praxi znamená, že dokument se může skládat z 10% z tématu A, z 40 % z tématu B a z 50% z tématu C. V každém tématu se pak můžou vyskytovat jak slova pro dané téma unikátní, tak slova tématy sdílená. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Práci s LDA si vyzkoušíme na datasetu Reuters. Jedná se o sbírku cca 10000 zpráv, které stejnojmenná agentura publikovala v roce 1987. Jelikož jsem ale líný a nechtělo se mi extrahovat texty z xml souborů(ke stažení [zde](http://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection)), namísto toho jsem si Reuters data převedl do csv z korpusu spojeného s balíčkem nltl pomocí následujícího kódu:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import reuters\n",
    "text_id_list = []\n",
    "text_categories_list = []\n",
    "text_string_list = []\n",
    "for file_id in reuters.fileids():\n",
    "    text_id_list.append(file_id)\n",
    "    text_categories_list.append(reuters.categories(file_id))\n",
    "    text_string_list.append(\" \".join(reuters.words(file_id)))\n",
    "texts_tab = pd.DataFrame({\n",
    "    \"id\":text_id_list, \n",
    "    \"categories\": text_categories_list, \n",
    "    \"text\":text_string_list\n",
    "})\n",
    "texts_tab.to_csv(\"reuters_text.csv\", index=False, sep=\";\", header=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Csv soubor si nyní načteme do pandího datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T16:05:38.521970Z",
     "start_time": "2021-02-27T16:05:30.431672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10788\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>['trade']</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>['grain']</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7 - 12 PCT GRAIN S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>['crude', 'nat-gas']</td>\n",
       "      <td>JAPAN TO REVISE LONG - TERM ENERGY DEMAND DOWN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>['corn', 'grain', 'rice', 'rubber', 'sugar', '...</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>['palm-oil', 'veg-oil']</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY Indone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           keywords  \\\n",
       "0  test/14826                                          ['trade']   \n",
       "1  test/14828                                          ['grain']   \n",
       "2  test/14829                               ['crude', 'nat-gas']   \n",
       "3  test/14832  ['corn', 'grain', 'rice', 'rubber', 'sugar', '...   \n",
       "4  test/14833                            ['palm-oil', 'veg-oil']   \n",
       "\n",
       "                                                text  \n",
       "0  ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPA...  \n",
       "1  CHINA DAILY SAYS VERMIN EAT 7 - 12 PCT GRAIN S...  \n",
       "2  JAPAN TO REVISE LONG - TERM ENERGY DEMAND DOWN...  \n",
       "3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Tha...  \n",
       "4  INDONESIA SEES CPO PRICE RISING SHARPLY Indone...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reuters = pd.read_csv(\"reuters_text.csv\", header=None, names=[\"id\", \"keywords\", \"text\"] ,sep=\";\")\n",
    "print(f\"Number of documents: {len(reuters)}\")\n",
    "reuters.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T19:38:45.141878Z",
     "start_time": "2020-04-10T19:38:45.134931Z"
    }
   },
   "source": [
    "Z dokumentů odstraníme stopwords a dokumenty tokenizujeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T16:05:52.370149Z",
     "start_time": "2021-02-27T16:05:50.991712Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "documents_array = reuters[\"text\"].values.tolist()\n",
    "sentences = []\n",
    "interpunction = string.punctuation\n",
    "stopwords = [\"the\", \"to\", \"of\", \"and\", \"a\", \"in\", \"it\", \"s\", \"for\", \"is\", \"its\", \"that\", \"be\", \"on\", \"will\", \"by\", \"has\",\n",
    "            \"said\", \"u\", \"he\", \"she\", \"at\", \"would\", \"was\", \"with\", \"as\", \"but\", \"not\", \"are\", \"this\", \"from\", \"have\", \"an\",\n",
    "            \"they\", \"had\", \"could\", \"one\", \"two\", \"k\", \"also\",\n",
    "            \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\",\n",
    "             \"an\", \"or\", \"were\", \"which\", \"than\", \"if\" ,\"we\", \"about\", \"their\", \"i\",\n",
    "             \"can\", \"t\", \"say\"\n",
    "            ]\n",
    "\n",
    "for one_document in documents_array:\n",
    "    for interpunction_char in interpunction:\n",
    "        one_document = one_document.replace(interpunction_char, \"\")\n",
    "    sentences.append([word for word in one_document.lower().split() if word not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro následnou analýzu budume chtít vidět nikoli indexy slov, ale jejich znění. Proto modelu předáváme mapování id na token. Toto mapování se dá získat pomocí id2token funkce slovníku. Aby ale ta fungovala, musí se slovník aspoň jednou, byť naprázdno, použít.  \n",
    "Pomineme-li samotný text, je nejdůležitějším parametrem modelu počet témat, které má model najít. Zde je potřeba mít znalost o problematice, resp. o zdrojovém textu. Jinak může totiž být výsledek neuspokojivý. Pro malý počet témat budou tato témata ve skutečnosti slepeninou několika tématických okruhů. Pro velký počet témat zase budou tato témata příliš specifická, určující třeba jen pro jeden konkrétní dokument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T16:07:06.212194Z",
     "start_time": "2021-02-27T16:05:55.451902Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "sentences_dictionary = Dictionary(sentences)\n",
    "sentences_corpus = [sentences_dictionary.doc2bow(text) for text in sentences]\n",
    "\n",
    "sentences_dictionary[1] #necessary for initializatiob id2token\n",
    "id2word = sentences_dictionary.id2token\n",
    "\n",
    "lda = LdaModel(sentences_corpus, num_topics=250, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívejme se na jednotlivá témata a nejcharakterističtější slova pro ně. Přiznám se, že ať jsem se snažil měnit počet témat při trénování jakkoli, nikdy jsem nedostal témata, která by se dala většinově snadno interpretovat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T16:07:37.202012Z",
     "start_time": "2021-02-27T16:07:35.421471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.12184751, 'vs'),\n",
       "   (0.07795902, 'mln'),\n",
       "   (0.061171815, '000'),\n",
       "   (0.05532412, 'cts'),\n",
       "   (0.05354625, 'net'),\n",
       "   (0.043428607, 'loss'),\n",
       "   (0.03603616, 'shr'),\n",
       "   (0.03054762, 'dlrs'),\n",
       "   (0.022166532, 'profit'),\n",
       "   (0.020658478, 'revs'),\n",
       "   (0.020623323, 'qtr'),\n",
       "   (0.018104678, 'lt'),\n",
       "   (0.017325902, 'year'),\n",
       "   (0.012130544, 'note'),\n",
       "   (0.011660022, 'oper'),\n",
       "   (0.010092052, '4th'),\n",
       "   (0.009577685, 'shrs'),\n",
       "   (0.009538614, 'avg'),\n",
       "   (0.009400835, 'inc'),\n",
       "   (0.0075341626, 'sales')],\n",
       "  -1.1933851573863017),\n",
       " ([(0.068227105, 'quarter'),\n",
       "   (0.053438403, 'first'),\n",
       "   (0.051464166, 'dlrs'),\n",
       "   (0.04920716, 'sales'),\n",
       "   (0.04410495, 'year'),\n",
       "   (0.041594345, 'earnings'),\n",
       "   (0.03606402, 'sees'),\n",
       "   (0.034928583, 'expects'),\n",
       "   (0.03039722, 'earned'),\n",
       "   (0.027145015, 'revenues'),\n",
       "   (0.021645484, 'ended'),\n",
       "   (0.020934321, 'pct'),\n",
       "   (0.017067246, '000'),\n",
       "   (0.014357404, 'lower'),\n",
       "   (0.013927337, '1986'),\n",
       "   (0.012483135, 'company'),\n",
       "   (0.012040385, 'number'),\n",
       "   (0.011614681, 'mln'),\n",
       "   (0.011062149, 'lt'),\n",
       "   (0.010525047, 'last')],\n",
       "  -1.4369363763551177),\n",
       " ([(0.060392383, 'trade'),\n",
       "   (0.015776169, 'japan'),\n",
       "   (0.011212534, 'countries'),\n",
       "   (0.009806884, 'imports'),\n",
       "   (0.009684371, 'government'),\n",
       "   (0.0096785175, 'gatt'),\n",
       "   (0.009652479, 'states'),\n",
       "   (0.009432541, 'agreement'),\n",
       "   (0.009316986, 'told'),\n",
       "   (0.009142805, 'foreign'),\n",
       "   (0.009102229, 'united'),\n",
       "   (0.008077631, 'officials'),\n",
       "   (0.007924347, 'last'),\n",
       "   (0.007880139, 'talks'),\n",
       "   (0.007207887, 'import'),\n",
       "   (0.006820085, 'exports'),\n",
       "   (0.0061351475, 'more'),\n",
       "   (0.0058475146, 'general'),\n",
       "   (0.005668277, 'world'),\n",
       "   (0.0054381713, 'industry')],\n",
       "  -1.5597468755198485)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topics = lda.top_topics(sentences_corpus)\n",
    "top_topics[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro určení, jaké témata se v dokumentu (i v tom nepoužitém při trénování) používají, se aplikuje funkce *get_document_topics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:09:57.057759Z",
     "start_time": "2020-04-11T12:09:57.042795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(96, 0.1303343),\n",
       " (101, 0.15189825),\n",
       " (138, 0.25834855),\n",
       " (184, 0.103110984),\n",
       " (196, 0.108891085),\n",
       " (248, 0.125397)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document = \"weather is bad farmers are worried corn harvest will be bad\"\n",
    "new_doc_split = new_document.split()\n",
    "new_doc_bow = sentences_dictionary.doc2bow(new_doc_split)\n",
    "lda.get_document_topics(new_doc_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Některé témata vypadají rozumně."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:36:15.927505Z",
     "start_time": "2020-04-11T12:36:15.846193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('corn', 0.093121625),\n",
       " ('soybean', 0.048433058),\n",
       " ('tonnes', 0.02456417),\n",
       " ('bushels', 0.021541953),\n",
       " ('licences', 0.020016486),\n",
       " ('mln', 0.019897716),\n",
       " ('barley', 0.019527292),\n",
       " ('wheat', 0.019477654),\n",
       " ('week', 0.015098821),\n",
       " ('argentina', 0.012399861)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topic(96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jiná ale s textem nemají nic společného."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T12:36:58.151853Z",
     "start_time": "2020-04-11T12:36:58.068042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pesetas', 0.03557521),\n",
       " ('spain', 0.033057638),\n",
       " ('insured', 0.03294756),\n",
       " ('omits', 0.032092497),\n",
       " ('schedule', 0.03171639),\n",
       " ('premiums', 0.02993515),\n",
       " ('encouraged', 0.029780192),\n",
       " ('levy', 0.023633527),\n",
       " ('philippines', 0.022682484),\n",
       " ('content', 0.022029344)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topic(248)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existuje i příbuzná technika - hierarchický Dirichletův proces (HDP). Ten narozdíl od LDA dokáže z trénovacích dat určit, kolik témat se v nich vyskytuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:41:29.259992Z",
     "start_time": "2020-04-12T10:40:48.413384Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "sentences_dictionary = Dictionary(sentences)\n",
    "sentences_corpus = [sentences_dictionary.doc2bow(text) for text in sentences]\n",
    "\n",
    "sentences_dictionary[1] #necessary for initializatiob id2token\n",
    "id2word = sentences_dictionary.id2token\n",
    "\n",
    "hdp = HdpModel(sentences_corpus, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokud chceme získat matici témat, použijeme funkci *get_topics*. Mimo jiné jsou u ní zajímavá i metadata, konkrétně informace o jejích rozměrech. Ty jsou totiž určeny jako počet témat krát velikost slovníku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:42:06.355219Z",
     "start_time": "2020-04-12T10:42:06.305390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 30889)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp.get_topics().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokud chceme vytisknout několik nejdůležitějších témat, použijeme *print_topics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:42:07.861165Z",
     "start_time": "2020-04-12T10:42:07.388257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*pct + 0.007*dlrs + 0.007*mln + 0.006*year + 0.005*lt + 0.005*trade + 0.004*oil + 0.004*billion + 0.004*bank + 0.004*last'),\n",
       " (1,\n",
       "  '0.031*mln + 0.030*vs + 0.019*dlrs + 0.019*000 + 0.016*cts + 0.014*net + 0.012*lt + 0.011*loss + 0.010*pct + 0.009*year'),\n",
       " (2,\n",
       "  '0.014*mln + 0.011*pct + 0.009*dlrs + 0.008*lt + 0.007*000 + 0.007*year + 0.006*vs + 0.005*company + 0.005*billion + 0.004*cts'),\n",
       " (3,\n",
       "  '0.036*vs + 0.030*mln + 0.023*cts + 0.018*000 + 0.016*net + 0.015*dlrs + 0.012*loss + 0.011*shr + 0.010*lt + 0.007*nil'),\n",
       " (4,\n",
       "  '0.013*vs + 0.012*mln + 0.010*cts + 0.010*lt + 0.008*dlrs + 0.006*000 + 0.006*net + 0.005*pct + 0.005*inc + 0.005*year'),\n",
       " (5,\n",
       "  '0.009*lt + 0.006*dlrs + 0.005*mln + 0.005*shares + 0.005*company + 0.005*inc + 0.004*corp + 0.004*pct + 0.004*stock + 0.004*cts'),\n",
       " (6,\n",
       "  '0.004*mln + 0.004*pct + 0.004*bank + 0.003*lt + 0.003*market + 0.003*japan + 0.002*shares + 0.002*dlrs + 0.002*dollar + 0.002*dome'),\n",
       " (7,\n",
       "  '0.022*vs + 0.014*cts + 0.011*mln + 0.011*000 + 0.009*net + 0.006*shr + 0.006*lt + 0.004*qtr + 0.004*87 + 0.004*dlrs'),\n",
       " (8,\n",
       "  '0.003*mln + 0.003*pct + 0.002*dlrs + 0.002*year + 0.002*lt + 0.002*tonnes + 0.002*sales + 0.001*1986 + 0.001*week + 0.001*inc'),\n",
       " (9,\n",
       "  '0.018*vs + 0.010*cts + 0.010*mln + 0.009*000 + 0.008*net + 0.006*shr + 0.006*loss + 0.004*lt + 0.004*qtr + 0.004*profit'),\n",
       " (10,\n",
       "  '0.004*pct + 0.002*000 + 0.002*1986 + 0.002*year + 0.002*mln + 0.002*billion + 0.002*1987 + 0.001*dlrs + 0.001*company + 0.001*crop'),\n",
       " (11,\n",
       "  '0.004*fed + 0.003*dlrs + 0.002*000 + 0.002*billion + 0.002*mln + 0.002*says + 0.002*vs + 0.002*lt + 0.002*cts + 0.001*repurchase'),\n",
       " (12,\n",
       "  '0.002*japan + 0.002*dollar + 0.002*trade + 0.001*last + 0.001*dlrs + 0.001*csr + 0.001*year + 0.001*monier + 0.001*markets + 0.001*lt'),\n",
       " (13,\n",
       "  '0.005*87 + 0.004*mln + 0.004*09 + 0.003*nil + 0.002*03 + 0.002*04 + 0.002*lt + 0.002*stocks + 0.002*ltd + 0.002*00'),\n",
       " (14,\n",
       "  '0.003*mln + 0.002*shares + 0.002*lt + 0.002*company + 0.002*corp + 0.001*pct + 0.001*marks + 0.001*last + 0.001*1986 + 0.001*inc'),\n",
       " (15,\n",
       "  '0.006*mln + 0.004*tonnes + 0.003*pct + 0.002*last + 0.002*crop + 0.002*87 + 0.002*1986 + 0.001*forecast + 0.001*month + 0.001*vs'),\n",
       " (16,\n",
       "  '0.002*dlrs + 0.001*year + 0.001*billion + 0.001*surplus + 0.001*debt + 0.001*pct + 0.001*1986 + 0.001*south + 0.001*foreign + 0.001*tonnes'),\n",
       " (17,\n",
       "  '0.002*000 + 0.002*tonnes + 0.001*policy + 0.001*trade + 0.001*oil + 0.001*mln + 0.001*vs + 0.001*meeting + 0.001*year + 0.001*exchange'),\n",
       " (18,\n",
       "  '0.004*87 + 0.003*09 + 0.002*last + 0.002*month + 0.002*vs + 0.002*exports + 0.002*1986 + 0.002*stocks + 0.002*1985 + 0.002*03'),\n",
       " (19,\n",
       "  '0.002*mln + 0.002*dlrs + 0.002*vs + 0.001*000 + 0.001*lt + 0.001*company + 0.001*pct + 0.001*tax + 0.001*net + 0.001*1986')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info = hdp.print_topics(num_topics=20, num_words=10)\n",
    "topic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nové dokumenty se do témat rozloží následujícím postupem. Bohužel mi nepřijde, že by nalezená témata byla pro ony dokumenty zrovna moc určující."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:42:10.266596Z",
     "start_time": "2020-04-12T10:42:10.253595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.010636344780183422), (1, 0.22939590297793294), (5, 0.4625137666332447), (28, 0.28625733053084734)]\n",
      "[(0, 0.8955147695426963), (1, 0.06215680482353509), (2, 0.017599295456717932)]\n"
     ]
    }
   ],
   "source": [
    "madeira_document = \"madeira in letter of intent to be acquired lt madeira inc said it signed a letter of intent to be acquired by tradevest inc through a stock for stock exchange after completion of the transaction tradevest would own 90 pct of the issued outstanding stock of madeira\"\n",
    "harvest_document = \"weather is bad farmers are worried corn harvest will be bad\"\n",
    "madeira_document_split = madeira_document.split()\n",
    "harvest_document_split = harvest_document.split()\n",
    "madeira_document_bow = sentences_dictionary.doc2bow(madeira_document_split)\n",
    "harvest_document_bow = sentences_dictionary.doc2bow(harvest_document_split)\n",
    "print(hdp[madeira_document_bow])\n",
    "print(hdp[harvest_document_bow])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T20:49:17.487159Z",
     "start_time": "2020-03-31T20:49:17.482229Z"
    }
   },
   "source": [
    "# Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedním z problémů bag-of-words přístupu je skutečnost, že se nesnaží získat informaci o významu slov. Vzdálenost vektorů pak charakterizuje, do jaké míry se ve dvou dokumentech vyskytují stejná slova, ale nikoli nutně to, zda mají dokumenty podobný obsah. Tuto otázku řeší model v word2vec.  \n",
    "Word2vec mapuje slova do vektorového prostoru, ve kterém jsou na podobných místech slova týkající se podobné tématiky (bacha - to může znamenat, že jsou blízko sebe i opozita týkající se jedné věci).  \n",
    "Jak je toho dosáhnuto? Používá se shallow neuronová síť (tj. síť s jednou skrytou vrstvou), konkrétně (a) skip-gram či (b) continuous bag-of-words. Tak či tak se jedná o  unsupervised techniku založenou na příbuznosti slov.\n",
    "\n",
    "(a) Skip-Gram znamená, že se neuronová sít snaží na základě určitého slova odhadnou slova v jeho blízkém okolí. Velikost tohoto okolí je parametr určený uživatelem.    \n",
    " ![skip-gram](pomocne_soubory/skip_gram_small.png)  \n",
    "(b) CBAG (continuous bag-of-words) je opak předchozího - je známo okolí slova a dotčené slovo síť pokouší určit.  \n",
    " ![cbow](pomocne_soubory/cbow_small.png)  \n",
    "\n",
    "Z hlediska přesnosti/použitelnosti jsou oba přístupy údajně zhruba stejně dobré.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:48:08.307631Z",
     "start_time": "2020-04-07T18:48:08.301682Z"
    }
   },
   "source": [
    "Pro otestování algoritmu opět použijeme retures dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:50:33.606597Z",
     "start_time": "2020-04-07T18:50:28.102209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10788\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>test/14826</td>\n",
       "      <td>['trade']</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>test/14828</td>\n",
       "      <td>['grain']</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7 - 12 PCT GRAIN S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>test/14829</td>\n",
       "      <td>['crude', 'nat-gas']</td>\n",
       "      <td>JAPAN TO REVISE LONG - TERM ENERGY DEMAND DOWN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>test/14832</td>\n",
       "      <td>['corn', 'grain', 'rice', 'rubber', 'sugar', '...</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>test/14833</td>\n",
       "      <td>['palm-oil', 'veg-oil']</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY Indone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           keywords  \\\n",
       "0  test/14826                                          ['trade']   \n",
       "1  test/14828                                          ['grain']   \n",
       "2  test/14829                               ['crude', 'nat-gas']   \n",
       "3  test/14832  ['corn', 'grain', 'rice', 'rubber', 'sugar', '...   \n",
       "4  test/14833                            ['palm-oil', 'veg-oil']   \n",
       "\n",
       "                                                text  \n",
       "0  ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPA...  \n",
       "1  CHINA DAILY SAYS VERMIN EAT 7 - 12 PCT GRAIN S...  \n",
       "2  JAPAN TO REVISE LONG - TERM ENERGY DEMAND DOWN...  \n",
       "3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Tha...  \n",
       "4  INDONESIA SEES CPO PRICE RISING SHARPLY Indone...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reuters = pd.read_csv(\"reuters_text.csv\", header=None, names=[\"id\", \"keywords\", \"text\"] ,sep=\";\")\n",
    "print(f\"Number of documents: {len(reuters)}\")\n",
    "reuters.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro potřeby word2vec bylo nutné nejprve sloupec text konvertovat na list jednotlivých dokumentů. Z každého dokumentu se posléze odstranila všechna interpunkční znaménka a následně se dokumenty tokenizovaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T10:42:40.566394Z",
     "start_time": "2020-04-12T10:42:40.191522Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "documents_array = reuters[\"text\"].values.tolist()\n",
    "tokenized_documents = []\n",
    "interpunction = string.punctuation\n",
    "\n",
    "for one_document in documents_array:\n",
    "    for interpunction_char in interpunction:\n",
    "        one_document = one_document.replace(interpunction_char, \"\")\n",
    "    tokenized_documents.append([word for word in one_document.lower().split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samotné natrénování word2vec modelu proběhne v rámci konstruktoru Word2vec objektu. Tomu stačí v minimalistické formě stačí předat tokenizované dokumenty. Případně zde můžeme například nastavit \n",
    "  - parametrem **size** velikost vektorů reprezentujících jednotlivá slova (default 100)\n",
    "  - parametrem **min_words** prahový počet slov pro vstup do slovníku; slova s menším výskytem se do slovníku nedostanou (default je 5)\n",
    "  - parametrem **sg** trénovací algoritmus - pro hodnotu 1 jím bude skip-gram, default je to CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T18:51:05.270711Z",
     "start_time": "2020-04-07T18:50:58.870379Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:03:02.334255Z",
     "start_time": "2020-04-07T19:03:02.300345Z"
    }
   },
   "source": [
    "Pro výpis slov ve slovníku lze použít *model.wv.vocab*. Pro zobrazení hodnot vektoru pro určité slovo se použije \"slovníková\" notace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T17:33:00.872335Z",
     "start_time": "2020-04-06T17:33:00.863326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2995348 ,  0.1629063 , -0.49440357,  0.54054   , -0.11973411,\n",
       "        0.04009064, -0.04892669, -0.42443478, -0.56929314, -0.17048904,\n",
       "        0.05814625,  0.18764156, -0.17550497, -0.18162726, -0.35992086,\n",
       "        0.00925083, -0.19654445,  0.16989894,  0.1866993 , -0.5971897 ,\n",
       "        0.05828685, -0.18655527,  0.19490187,  0.0075747 , -0.45888034,\n",
       "        0.19208083, -0.00520514, -0.0751219 ,  0.27900982,  0.03434771,\n",
       "       -0.07149681,  0.19865024, -0.16785796,  0.13689639, -0.08421236,\n",
       "       -0.2657021 , -0.293619  , -0.31063342,  0.20969257, -0.02541368,\n",
       "       -0.40483776,  0.03907657,  0.5432602 , -0.03630711,  0.11985175,\n",
       "        0.03115314,  0.00672338,  0.01863982,  0.18898779, -0.38364056,\n",
       "       -0.33235678,  0.42246315, -0.08392639,  0.24356903,  0.17339723,\n",
       "        0.02745501, -0.49006423, -0.48680982,  0.219192  ,  0.4667836 ,\n",
       "       -0.0016112 ,  0.05923849,  0.16883454, -0.41551962, -0.03346179,\n",
       "       -0.02700867,  0.05961036,  0.11947125,  0.34579685, -0.22749276,\n",
       "       -0.34917575, -0.09522516, -0.10533693, -0.43992126,  0.24299583,\n",
       "        0.10428517,  0.10423314, -0.00810526,  0.0191727 ,  0.23637776,\n",
       "        0.04090035,  0.09307571,  0.10730579,  0.54588246, -0.26618513,\n",
       "       -0.07069638, -0.0360704 , -0.10869559,  0.03407193, -0.3390449 ,\n",
       "       -0.29851228,  0.06441103, -0.0370464 ,  0.17917676,  0.24439351,\n",
       "       -0.2774787 ,  0.4962383 ,  0.13126932,  0.16646393,  0.39099905],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_rain = model.wv[\"rain\"]\n",
    "vector_rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co se ale s takovýmito vektory dá dělat? Jednou z možností je výpočet cosinové vzdálenosti dvou vektorů. Tím se určí, jak (z hlediska trénovacího korpusu) si jsou významy dvou slov jimi reprezentovanými podobné. Něco podobného jsme dělali výše u dokumentů vektorizovaných s bag-of-words přístupem. Nicméně tehdy jsme blízkost dvou dokumentů určovali na základě toho, že se v nich vyskytovala stejná slova. Nyní blízkost slov určujeme podle číselné reprezentace získané podle postavvení oněch slov v trénovacích větách.  \n",
    "Podobnost se vypočítá s pomocí funkce *model.wv.similarity(vektor_1, vektor_2)*. Vidíme, že děšť má blízko bouři a vodě. S ohněm už je to horší, ale pořád se jedná o živel. Podobnost s daněmi coby naprosto nesouvisející entitou už pak nestojí za řeč. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:47:38.642641Z",
     "start_time": "2020-04-07T19:47:38.635656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of words rain and storm is 0.75\n",
      "Similarity of words rain and water is 0.72\n",
      "Similarity of words rain and fire is 0.59\n",
      "Similarity of words rain and taxes is 0.18\n"
     ]
    }
   ],
   "source": [
    "first_word = \"rain\"\n",
    "compared_words = [\"storm\", \"water\", \"fire\", \"taxes\"]\n",
    "for second_word in compared_words:\n",
    "    print(f\"Similarity of words {first_word} and {second_word} is {model.wv.similarity(first_word, second_word):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lze si i vypsat slova s nejpodobnějšími vektory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:20:39.338960Z",
     "start_time": "2020-04-07T19:20:39.330980Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\newnotebook\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('expenditure', 0.7712256908416748),\n",
       " ('tax', 0.7419455051422119),\n",
       " ('items', 0.7265673875808716),\n",
       " ('costs', 0.7023390531539917),\n",
       " ('gains', 0.6953080892562866),\n",
       " ('losses', 0.6873643398284912),\n",
       " ('spending', 0.6683705449104309),\n",
       " ('expenses', 0.6584079265594482),\n",
       " ('returns', 0.6527901887893677),\n",
       " ('profits', 0.6491448879241943)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['taxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je možné hledat nejpodobnější vektor k aritmetické operaci mezi vektory. Kód níže ukazuje, co se stane, když od války odečteme zbraně."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T17:35:23.635501Z",
     "start_time": "2020-04-06T17:35:23.626557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\newnotebook\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bill', 0.5575625896453857),\n",
       " ('close', 0.5529700517654419),\n",
       " ('range', 0.5302945375442505),\n",
       " ('dispute', 0.5291434526443481),\n",
       " ('measure', 0.5084378719329834)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['war'], negative=['weapons'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:22:35.710069Z",
     "start_time": "2020-04-07T19:22:35.703085Z"
    }
   },
   "source": [
    "A zde vidíme, jak vypadá součet firmy a daní."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T17:36:12.269163Z",
     "start_time": "2020-04-06T17:36:12.261228Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\newnotebook\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('earnings', 0.6709654331207275),\n",
       " ('costs', 0.6585673093795776),\n",
       " ('revenue', 0.6408575177192688),\n",
       " ('expenses', 0.6370643377304077),\n",
       " ('expenditure', 0.6307579278945923)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['company', \"taxes\"],topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výše uvedenými funkcemi jsme se dívali na vlastnosti jednoho či několika málo vektorů. S pomocí tSNE (více o této technice např. [zde](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1?gi=a516b019f8c)) si lze zobrazit projekci celého prostoru vektorů do 2D obrázku. Jelikož s tSNE moc seznámen nejsem, raději sem zkopíruji kód Radima Řehůřka (originál [tady](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) na spodku stránky), autora gensimu. Pozn.: pokud se notebook tváří, že už vše úspěšně dopočítal, ale vy vidíte jen bílou plochu a máte v prohlížeči zapnuté rozšíření NoScript, musíte povolit localhosta.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:38:03.688858Z",
     "start_time": "2020-04-07T19:36:14.054449Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE - this code was copied from https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = [] # positions in vector space\n",
    "    labels = [] # keep track of words to label our data again later\n",
    "    for word in model.wv.vocab:\n",
    "        vectors.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=False):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "S pomocí word2vec se dá do jisté míry porovnávat podobnost slov. Co máme ale dělat, když chceme porovnávat celé dokumenty? Hypoteticky by se daly spočítat vektory dokumentů jako průměr vektorů jednotlivých slov - nicméně lepší bude použít alogritmus doc2vec. S extrémně velkou dávkou zjednodušení to funguje tak, že se ve výše ukázaných neuronových sítích objevuje další vektor, tentokráte zastupující celý dokument.  \n",
    "V případě architektury PV-DM (Distributed Memory Model of Paragraph Vector) je vektor dokumentu de facto dodatečným vstupem pro CBAG. Mimo jiné je jeho výhodou, že v sobě mimo jiné uchovává i určitou informaci o pořadí slov.\n",
    " ![pv_dm](pomocne_soubory/pv_dm_small.png)  \n",
    "Existuje ale i jiná architektura nazvaná Distributed Bag of Words version of Paragraph Vector (PV-DBOW). Zde je cílem donutit model, aby na základě vektoru dokumenty odhadl slova, která se v dokumentu nalézají.\n",
    " ![pv_dbow](pomocne_soubory/pv_dbow_small.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S pomocí doc2vec se nyní pokusíme o unsupervised klasifikaci. Než ale začneme, měl bych čtenáře [odkázat](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py) na velice pěkný tutoriál od tvůrců gensimu, podle kterého jsem se řídil.  \n",
    "Stejně jako u word2vec budeme zkoumat dataset Reuters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:48:33.575514Z",
     "start_time": "2020-04-07T19:48:33.466775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10788\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>test/14826</td>\n",
       "      <td>['trade']</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>test/14828</td>\n",
       "      <td>['grain']</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7 - 12 PCT GRAIN S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>test/14829</td>\n",
       "      <td>['crude', 'nat-gas']</td>\n",
       "      <td>JAPAN TO REVISE LONG - TERM ENERGY DEMAND DOWN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>test/14832</td>\n",
       "      <td>['corn', 'grain', 'rice', 'rubber', 'sugar', '...</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>test/14833</td>\n",
       "      <td>['palm-oil', 'veg-oil']</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY Indone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           keywords  \\\n",
       "0  test/14826                                          ['trade']   \n",
       "1  test/14828                                          ['grain']   \n",
       "2  test/14829                               ['crude', 'nat-gas']   \n",
       "3  test/14832  ['corn', 'grain', 'rice', 'rubber', 'sugar', '...   \n",
       "4  test/14833                            ['palm-oil', 'veg-oil']   \n",
       "\n",
       "                                                text  \n",
       "0  ASIAN EXPORTERS FEAR DAMAGE FROM U . S .- JAPA...  \n",
       "1  CHINA DAILY SAYS VERMIN EAT 7 - 12 PCT GRAIN S...  \n",
       "2  JAPAN TO REVISE LONG - TERM ENERGY DEMAND DOWN...  \n",
       "3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Tha...  \n",
       "4  INDONESIA SEES CPO PRICE RISING SHARPLY Indone...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reuters = pd.read_csv(\"reuters_text.csv\", header=None, names=[\"id\", \"keywords\", \"text\"] ,sep=\";\")\n",
    "print(f\"Number of documents: {len(reuters)}\")\n",
    "reuters.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Než texty vypustíme na doc2vec, měli bychom je převést do stavu, ve kterém bude algoritmus schopný a ochotný je přebrat. Z dokumentů odebereme interpunkci, všechna velká písmena převedeme na malá a provedeme roztržení vět podle bílých znaků. Navíc dokumenty otagujeme indexem, abychom je mohli později identifikovat. To zrealizujeme jejich převodem na TaggedDocument.  Jedná se o objekt s atributy words - původní dokument převedený na jednotlivá slova coby elementy listu - a tags, ve kterém je v našem případě číslovka indexu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:48:36.054002Z",
     "start_time": "2020-04-07T19:48:35.694264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['china', 'daily', 'says', 'vermin', 'eat', '7', '12', 'pct', 'grain', 'stocks', 'a', 'survey', 'of', '19', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', '12', 'pct', 'of', 'china', 's', 'grain', 'stocks', 'the', 'china', 'daily', 'said', 'it', 'also', 'said', 'that', 'each', 'year', '1', '575', 'mln', 'tonnes', 'or', '25', 'pct', 'of', 'china', 's', 'fruit', 'output', 'are', 'left', 'to', 'rot', 'and', '2', '1', 'mln', 'tonnes', 'or', 'up', 'to', '30', 'pct', 'of', 'its', 'vegetables', 'the', 'paper', 'blamed', 'the', 'waste', 'on', 'inadequate', 'storage', 'and', 'bad', 'preservation', 'methods', 'it', 'said', 'the', 'government', 'had', 'launched', 'a', 'national', 'programme', 'to', 'reduce', 'waste', 'calling', 'for', 'improved', 'technology', 'in', 'storage', 'and', 'preservation', 'and', 'greater', 'production', 'of', 'additives', 'the', 'paper', 'gave', 'no', 'further', 'details'], tags=[1]),\n",
       " TaggedDocument(words=['japan', 'to', 'revise', 'long', 'term', 'energy', 'demand', 'downwards', 'the', 'ministry', 'of', 'international', 'trade', 'and', 'industry', 'miti', 'will', 'revise', 'its', 'long', 'term', 'energy', 'supply', 'demand', 'outlook', 'by', 'august', 'to', 'meet', 'a', 'forecast', 'downtrend', 'in', 'japanese', 'energy', 'demand', 'ministry', 'officials', 'said', 'miti', 'is', 'expected', 'to', 'lower', 'the', 'projection', 'for', 'primary', 'energy', 'supplies', 'in', 'the', 'year', '2000', 'to', '550', 'mln', 'kilolitres', 'kl', 'from', '600', 'mln', 'they', 'said', 'the', 'decision', 'follows', 'the', 'emergence', 'of', 'structural', 'changes', 'in', 'japanese', 'industry', 'following', 'the', 'rise', 'in', 'the', 'value', 'of', 'the', 'yen', 'and', 'a', 'decline', 'in', 'domestic', 'electric', 'power', 'demand', 'miti', 'is', 'planning', 'to', 'work', 'out', 'a', 'revised', 'energy', 'supply', 'demand', 'outlook', 'through', 'deliberations', 'of', 'committee', 'meetings', 'of', 'the', 'agency', 'of', 'natural', 'resources', 'and', 'energy', 'the', 'officials', 'said', 'they', 'said', 'miti', 'will', 'also', 'review', 'the', 'breakdown', 'of', 'energy', 'supply', 'sources', 'including', 'oil', 'nuclear', 'coal', 'and', 'natural', 'gas', 'nuclear', 'energy', 'provided', 'the', 'bulk', 'of', 'japan', 's', 'electric', 'power', 'in', 'the', 'fiscal', 'year', 'ended', 'march', '31', 'supplying', 'an', 'estimated', '27', 'pct', 'on', 'a', 'kilowatt', 'hour', 'basis', 'followed', 'by', 'oil', '23', 'pct', 'and', 'liquefied', 'natural', 'gas', '21', 'pct', 'they', 'noted'], tags=[2])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "documents_array = reuters[\"text\"].values\n",
    "tagged_documents = []\n",
    "interpunction = string.punctuation\n",
    "\n",
    "for index, one_document in enumerate(documents_array):\n",
    "    for interpunction_char in interpunction:\n",
    "        one_document = one_document.replace(interpunction_char, \"\")\n",
    "    tagged_documents.append(TaggedDocument(words=(one_document.lower().split()), tags=[index]))\n",
    "\n",
    "tagged_documents[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:33:23.139122Z",
     "start_time": "2020-04-05T19:33:23.133136Z"
    }
   },
   "source": [
    "Nyní je na čase vytvořit model. Jaké do něj vložit parametry? \n",
    "  - **vector_size** udává dimenzi (de facto velikost) vektorů reprezentujících jednotlivé dokumenty. Obecně platí, že čím větší číslo, tím lepší - tedy do určitého bodu. Internet radí použít číslo mezi 100 a 300. Bacha - větší číslo znamená větší nároky na paměť.\n",
    "  - **min_count** říká, že se mají ignorovat slova vyskytující se méně často než je hodnota tohoto parametru.\n",
    "  - **epochs** stanovuje, kolikrát má trénovací algoritmus proběhnout přes celá trénovací data.\n",
    "  - **dm** definuje trénovací algoritmus. Pro *dm*=1 je použit PV-DM, jinak je aplikován PV-DBOW.\n",
    "  \n",
    "Nicméně na trénování v této fázi ještě nedojde. Pouze se vytvoří slovník slov použitých v korpusu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:48:44.090956Z",
     "start_time": "2020-04-07T19:48:37.940732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(count:255, index:783, sample_int:4294967296)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300,\n",
    "                min_count=2,\n",
    "                epochs =30)\n",
    "model.build_vocab(tagged_documents)\n",
    "print(model.wv.vocab[\"exporters\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trénování nastává až nyní s použitím stejnojmenné funkce. Doba trvání při 30 epochách a vektorech o velikosti 300 činí cca minutu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:49:45.207797Z",
     "start_time": "2020-04-07T19:48:46.668771Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vezměme si nyní jeden z dokumentů z trénovacího korpusu a vytvořme jeho vektorovou reprezentaci (pozn.: vstupem pro *infer_vector* je list, jehož prvky jsou jednotlivá slova). Vidíme, že vektor je při opakovaném volání *infer_vector* na ten samý zdrojový text lehce odlišný."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:50:13.455264Z",
     "start_time": "2020-04-07T19:50:13.435353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07451984, -0.05164814,  0.00956431,  0.01069472,  0.02165848,\n",
       "        0.01807858,  0.05508804, -0.0125634 , -0.0981988 , -0.09312281,\n",
       "        0.08336763,  0.04845664,  0.11088966,  0.01605684, -0.00684044,\n",
       "       -0.10053208, -0.02189706,  0.03713731,  0.10363684, -0.00636772,\n",
       "        0.08618021,  0.09903043,  0.01540163, -0.09869087,  0.06386721,\n",
       "        0.13111216,  0.13974446, -0.1166614 , -0.07641425,  0.03143704,\n",
       "        0.08749971,  0.03210148, -0.0280312 , -0.09442003,  0.06866841,\n",
       "        0.01781315,  0.01498127,  0.00536366,  0.08387755,  0.04357964,\n",
       "       -0.09910098, -0.08393571,  0.06036806,  0.00760484,  0.02143488,\n",
       "       -0.03227936, -0.03926966,  0.0413909 ,  0.14798   ,  0.03751749,\n",
       "       -0.18519834, -0.00798994,  0.04597524,  0.03042793,  0.00982597,\n",
       "        0.07898626,  0.00251488, -0.01733068,  0.06914633, -0.0131802 ,\n",
       "       -0.09107514,  0.04086854, -0.03847832,  0.05112374,  0.03103903,\n",
       "       -0.15402824,  0.02857238, -0.07292553, -0.01044352, -0.01149495,\n",
       "       -0.0131012 , -0.00860867, -0.04816945,  0.04496521, -0.00860649,\n",
       "       -0.06139817, -0.02345462,  0.08311534,  0.02775866,  0.08328691,\n",
       "        0.17679194,  0.01245852,  0.13866001,  0.0199801 , -0.11218169,\n",
       "       -0.11696137,  0.07290797,  0.08109778,  0.01176924,  0.03486842,\n",
       "        0.07493843, -0.11710529,  0.05090477, -0.0519899 ,  0.08422135,\n",
       "        0.03865039,  0.09265502,  0.05770908, -0.10491312,  0.08417779,\n",
       "       -0.04407611,  0.08132438,  0.02641617,  0.01047719,  0.09942457,\n",
       "       -0.00289629, -0.0195632 , -0.08837668, -0.07668851,  0.0388038 ,\n",
       "       -0.01977539,  0.00295657, -0.19263366, -0.01303107,  0.01162732,\n",
       "        0.07291462,  0.0009903 ,  0.11589168,  0.05382258,  0.04432014,\n",
       "        0.06385696,  0.11591406,  0.0141471 , -0.02184661,  0.04290953,\n",
       "       -0.01929529, -0.05001286, -0.02462205, -0.05213428, -0.03081991,\n",
       "       -0.01126041,  0.15223923, -0.03325818, -0.0501207 ,  0.06400825,\n",
       "        0.05946526,  0.045066  , -0.06656283,  0.01303938,  0.087089  ,\n",
       "       -0.07026269, -0.02288389, -0.08513683, -0.07204837,  0.03631589,\n",
       "        0.09261912,  0.06694347,  0.01874307, -0.11461601,  0.00338175,\n",
       "        0.00054041,  0.0094846 ,  0.01087359,  0.01538928, -0.110762  ,\n",
       "        0.02374247,  0.01037025, -0.01076621,  0.06171961,  0.01600444,\n",
       "       -0.06464364,  0.0190773 , -0.07799973, -0.05731946,  0.03601258,\n",
       "        0.05258308,  0.01138982, -0.06829679,  0.02653146, -0.01577672,\n",
       "        0.02457717,  0.02281358,  0.08698031, -0.06745216, -0.08828074,\n",
       "        0.031451  ,  0.08708706,  0.06887519,  0.10128796, -0.03175055,\n",
       "       -0.06858654,  0.08088386, -0.18267402, -0.03157389, -0.00318918,\n",
       "       -0.06793316,  0.02696235, -0.16189122, -0.04443396,  0.02812776,\n",
       "       -0.0699666 , -0.08707067,  0.09349076,  0.01765832, -0.01551547,\n",
       "       -0.0406018 ,  0.11982882, -0.09684337, -0.07395375,  0.02222303,\n",
       "        0.10140032,  0.00563824, -0.07233191,  0.09119558, -0.01271813,\n",
       "       -0.01329273, -0.133571  , -0.01716857,  0.07163887, -0.01666071,\n",
       "        0.15692133,  0.010293  ,  0.01050077, -0.03198272, -0.05717474,\n",
       "        0.09245288,  0.04742124, -0.0270865 , -0.10925449, -0.03448727,\n",
       "        0.08191053, -0.11366701, -0.00846745, -0.09363075, -0.01976168,\n",
       "       -0.0200851 , -0.06727821,  0.13106745,  0.00838909,  0.07941264,\n",
       "       -0.04575561,  0.03847685,  0.00452186,  0.0919044 , -0.01670016,\n",
       "        0.04108307, -0.08295116,  0.1291455 , -0.05624174,  0.01661475,\n",
       "       -0.10059384,  0.13851452, -0.08330055, -0.01946255, -0.05432618,\n",
       "        0.02360481, -0.12427811, -0.09897044, -0.04389614, -0.02731602,\n",
       "       -0.07654108, -0.02669942,  0.13085544, -0.01274826, -0.30071276,\n",
       "        0.08574489, -0.08138755, -0.02310761,  0.02634668, -0.03454664,\n",
       "        0.09354179,  0.01195094, -0.18907279, -0.11713447, -0.03972411,\n",
       "        0.04590239, -0.00135367,  0.01542273,  0.02665549,  0.02740562,\n",
       "       -0.07817198,  0.00553137, -0.0207172 ,  0.12426771, -0.00348726,\n",
       "       -0.14433129, -0.06887661, -0.02457809, -0.0488038 ,  0.02256683,\n",
       "       -0.03238795, -0.00414453, -0.24892536, -0.1639519 , -0.13398992,\n",
       "        0.06176766, -0.01875776,  0.04643957,  0.0130851 ,  0.14374651,\n",
       "       -0.05306279,  0.06264417, -0.09562841,  0.06492279, -0.28681326,\n",
       "       -0.02560449,  0.03618449, -0.06040709, -0.05247499,  0.05547684],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madeira_vector = model.infer_vector(tagged_documents[150].words)\n",
    "madeira_vector_2= model.infer_vector(tagged_documents[150].words)\n",
    "madeira_vector-madeira_vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokumenty příbuzné našemu dokumentu 150 nalezneme pomocí funkce *most_similar*. Ta v nejjednodušší formě stejně jako u word2vecu spočítá cosinovou vzdálenost mezi vektorem v argumentu (bacha, vektor je v listu!) a vektory trénovacích dokumentů. Pak ukáže topn (defaultně 10) nejpodobnějších z nich. Podle očekávání je na prvním místě dokument původní."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:50:16.817048Z",
     "start_time": "2020-04-07T19:50:16.787130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(150, 0.8874098062515259),\n",
       " (7219, 0.7152369022369385),\n",
       " (9269, 0.6809947490692139),\n",
       " (4765, 0.6635549068450928),\n",
       " (4660, 0.658011794090271),\n",
       " (7762, 0.6564493179321289),\n",
       " (264, 0.6486384868621826),\n",
       " (8530, 0.6481114029884338),\n",
       " (8765, 0.6474401950836182),\n",
       " (5355, 0.6470494270324707)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar([madeira_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkontrolujme si, zda dokumenty mají podobnou tématiku. Zdá se, že ano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T19:55:06.291278Z",
     "start_time": "2020-04-05T19:55:06.283335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 150\n",
      "madeira in letter of intent to be acquired lt madeira inc said it signed a letter of intent to be acquired by tradevest inc through a stock for stock exchange after completion of the transaction tradevest would own 90 pct of the issued outstanding stock of madeira\n",
      "Document 7219\n",
      "first granite bancorp inc agrees to be acquired by magna group inc for stock first granite bancorp inc agrees to be acquired by magna group inc for stock\n",
      "Document 9269\n",
      "home savings durham lt hsld sets stock dividend home savings and loan association inc of durham n c said its board declared a 20 pct stock dividend payable april 28 to holders of record april three\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 150\")\n",
    "print(\" \".join(tagged_documents[150].words))\n",
    "print(\"Document 7219\")\n",
    "print(\" \".join(tagged_documents[7219].words))\n",
    "print(\"Document 9269\")\n",
    "print(\" \".join(tagged_documents[9269].words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusme se podívat, jak se model vypořádá s dokumentem, který neměl v době trénování k dispozici. Konkrétně zde máme článek o vypořádávání se s bouřkou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:50:24.045043Z",
     "start_time": "2020-04-07T19:50:24.040091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the storm clean up in sydney will resume in earnest this morning as fresh crews are brought in to replace state emergency service ses personnel who worked through the night the storm hit sydney early yesterday afternoon and two schoolgirls died when tree fell on them at reserve at hornsby heights in the city north number of other people were injured as the storm brought down trees and power poles and lifted roofs new south wales emergency services minister bob debus says welfare and emergency funding arrangements have been put in place with the declaration of natural disaster areas in campbeltown hornsby warringah and kurringai welfare services become available if they are needed local government is refunded any money it spends on the clean up or that it spends on repairing its own infrastructure low interest loans if they are needed are available to small business to help them get back on their feet again mr debus said energy australia says power has been restored to customers and work will continue today to reconnect those still without electricity energy australia peter leete says work will concentrate around the worst hit areas the worst of the problems we have still got are in sydney northern suburbs which seem to be the worst hit of all and that around hornsby st ives turramurra and frenches forest mr leete said four hundred ses volunteers are responding to more than calls for assistance the volunteers have worked throughout the night to remove trees from homes and roads the ses laura goodin says it will take several days before the damage is cleared up while the ses has received fewer calls for help than in the storm two weeks ago many of the jobs in this storm are actually quite complicated involving large trees or extensively damaged homes and businesses we re estimating that most of the tasks will be completed by friday if no new storms develop ms goodin said outside sydney the storms caused damage in north east of the state and the lower hunter scores of homes and farm buildings have been damaged and literally hundreds of trees have been brought down the storms accompanied by gale force winds and hail left large areas around tamworth gunnedah and quirindi without electricity and telephone services\n"
     ]
    }
   ],
   "source": [
    "new_document = \"the storm clean up in sydney will resume in earnest this morning as fresh crews are brought in to replace state emergency service ses personnel who worked through the night the storm hit sydney early yesterday afternoon and two schoolgirls died when tree fell on them at reserve at hornsby heights in the city north number of other people were injured as the storm brought down trees and power poles and lifted roofs new south wales emergency services minister bob debus says welfare and emergency funding arrangements have been put in place with the declaration of natural disaster areas in campbeltown hornsby warringah and kurringai welfare services become available if they are needed local government is refunded any money it spends on the clean up or that it spends on repairing its own infrastructure low interest loans if they are needed are available to small business to help them get back on their feet again mr debus said energy australia says power has been restored to customers and work will continue today to reconnect those still without electricity energy australia peter leete says work will concentrate around the worst hit areas the worst of the problems we have still got are in sydney northern suburbs which seem to be the worst hit of all and that around hornsby st ives turramurra and frenches forest mr leete said four hundred ses volunteers are responding to more than calls for assistance the volunteers have worked throughout the night to remove trees from homes and roads the ses laura goodin says it will take several days before the damage is cleared up while the ses has received fewer calls for help than in the storm two weeks ago many of the jobs in this storm are actually quite complicated involving large trees or extensively damaged homes and businesses we re estimating that most of the tasks will be completed by friday if no new storms develop ms goodin said outside sydney the storms caused damage in north east of the state and the lower hunter scores of homes and farm buildings have been damaged and literally hundreds of trees have been brought down the storms accompanied by gale force winds and hail left large areas around tamworth gunnedah and quirindi without electricity and telephone services\"\n",
    "print(new_document)\n",
    "new_document_list = new_document.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T20:03:49.024590Z",
     "start_time": "2020-04-05T20:03:49.019603Z"
    }
   },
   "source": [
    "Podobnost zrovna velká není..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T20:03:32.055724Z",
     "start_time": "2020-04-05T20:03:32.021814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1255, 0.4617820382118225),\n",
       " (6047, 0.4605138897895813),\n",
       " (1912, 0.42878440022468567),\n",
       " (1555, 0.4264325201511383),\n",
       " (1874, 0.4224299192428589),\n",
       " (6565, 0.42155858874320984),\n",
       " (6069, 0.41900256276130676),\n",
       " (6250, 0.4145764708518982),\n",
       " (3938, 0.40563446283340454),\n",
       " (6720, 0.40191686153411865)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc_vector = model.infer_vector(new_document_list)\n",
    "model.docvecs.most_similar([new_doc_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T20:04:41.300219Z",
     "start_time": "2020-04-05T20:04:41.294203Z"
    }
   },
   "source": [
    "Nicméně i tak vidíme, že byly nalezeny články týkající se pohrom spojených s vodou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T20:00:08.576718Z",
     "start_time": "2020-04-05T20:00:08.570733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some shipping restrictions remain on rhine limited shipping restrictions due to high water remain in force on parts of the west german stretch of the rhine river between the dutch border and the city of mainz but most are expected to be lifted this weekend water authority officials said the restrictions caused by high water levels include speed limits and directives to keep to the middle of the river to prevent damage to the river banks the high water was expected to recede within two days to below levels at which the restrictions come into force traffic was halted briefly late tuesday night wednesday and parts of thursday on stretches of the rhine between bonn and koblenz but the shipping bans were lifted the officials said shipping is now permitted on all parts of the west german section of the rhine with restrictions in some areas\n",
      "oklahoma cleaning up after week of floods rains residents of central oklahoma returned to their homes over the weekend after a week of heavy rains and severe flooding that left two dead and caused more than 20 mln dlrs in damage officials said some 900 people were evacuated from their homes during the rains and flooding last week civil defense officials said many of the shelters set up throughout the state in areas threatened by flooding except those near the washita and red rivers closed as residents returned to their damaged homes farmers who had expected a near record wheat crop now say this year will see one of the largest losses in decades gov henry bellmon who on thursday declared a flooding emergency for central oklahoma was expected to ask president reagan for federal disaster relief for the area in northern texas officials reported several tornadoes on friday a twister in lubbock yesterday damaged six mobile homes and two houses no injuries were reported\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 6047)\n",
    "print(\" \".join(tagged_documents[6047].words))\n",
    "print(\"Document 1255\")\n",
    "print(\" \".join(tagged_documents[1255].words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T20:10:51.236576Z",
     "start_time": "2020-04-05T20:10:51.231588Z"
    }
   },
   "source": [
    "Zkusme něco trochu aktuálnějšího:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T20:14:07.929146Z",
     "start_time": "2020-04-05T20:14:07.912192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4558, 0.4001925587654114),\n",
       " (8621, 0.3915051221847534),\n",
       " (5391, 0.3913812041282654),\n",
       " (4379, 0.38541918992996216),\n",
       " (6227, 0.37975233793258667),\n",
       " (6250, 0.37950408458709717),\n",
       " (7114, 0.37764972448349),\n",
       " (7904, 0.372464656829834),\n",
       " (3142, 0.3719536066055298),\n",
       " (1150, 0.37069180607795715)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korona_doc = (\"at the daily downing street briefing he said it could not go on it comes as the uk death toll reaches 4934 \"\n",
    "\"the department of health said on sunday there had been 621 more coronavirus related deaths in the uk in the past day \"\n",
    "\"the latest deaths include 12 more in wales seven in northern ireland and two in scotland \"\n",
    "\"as of 09 00 bst on sunday 47806 people had tested positive for coronavirus the department of health said\")\n",
    "korona_list = korona_doc.split()\n",
    "korona_vector = model.infer_vector(korona_list)\n",
    "model.docvecs.most_similar([korona_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T20:11:35.003427Z",
     "start_time": "2020-04-05T20:11:34.997478Z"
    }
   },
   "source": [
    "První je trefa do černého, v dalších dvou příkladech se holt algoritmus chytil na čísla..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T20:14:21.637917Z",
     "start_time": "2020-04-05T20:14:21.630975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 4558\n",
      "exotic newcastle disease in md new york usda exotic newcastle a highly contagious disease of pet birds and poultry has been confirmed in two pet bird dealer facilities in maryland and new york the u s agriculture department said the department said no domestic poultry are involved in the outbreak state quarantines have been placed on the two facilities in hunt valley maryland and schenectady new york the most serious u s outbreak of the disease occurred in 1971 73 in southern california where the disease spread from infected pet birds to a dense poultry population and nearly 12 mln birds mostly laying hens were destroyed at a cost of 56 mln dlrs the department said\n",
      "Document 8621\n",
      "ati medical inc lt atim 2nd qtr jan 31 net shr two cts vs eight cts net 118 933 vs 296 272 revs 2 742 731 vs 1 840 129 six mths shr two cts vs 12 cts net 92 372 vs 444 975 revs 4 977 105 vs 3 296 110\n",
      "Document 5391\n",
      "lt mirtone international inc 1st qtr dec 31 net shr two cts vs two cts net 407 396 vs 376 243 revs 5 341 353 vs 4 292 819\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 4558\")\n",
    "print(\" \".join(tagged_documents[4558].words))\n",
    "print(\"Document 8621\")\n",
    "print(\" \".join(tagged_documents[8621].words))\n",
    "print(\"Document 5391\")\n",
    "print(\" \".join(tagged_documents[5391].words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobně jako u word2vec si i u doc2vec můžeme zobrazit 2D projekci vektorů. Popravdě mi to zde přijde ještě lépe fungující než bylo u slov (např. na pěti hodinách jsou články o ropě a OPECu, na čtyřech hodinách články o ropě a Austrálii apod)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T20:04:32.248100Z",
     "start_time": "2020-04-07T20:01:39.614325Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE - this code was copied from https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "# with very minor changes (because of transition from word2vec to doc2vec)\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = [] # positions in vector space\n",
    "    labels = [] # keep track of words to label our data again later\n",
    "    for cislo, dokument in enumerate(model.docvecs):\n",
    "        vectors.append(dokument)\n",
    "        labels.append(str(cislo))\n",
    "        if cislo == 10787: break\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=False):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER (Named entity recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedním z možných požadavků na datovou analytiku může být oštítkování určitého typu slov či sousloví. Sem mohou patřit jména lidí anebo třeba názvy firem. I takováto netriviální funkcionalita je dnes už podporována některými pythoními balíčky. Konkrétně v anglickém jazyce se jedná o dnes již diskutovaný spacy.  \n",
    "Naimportujme si opět jak balíček, tak model, a tokenizujme anglickou větu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T18:19:35.676894Z",
     "start_time": "2020-04-09T18:19:25.720964Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:06:20.021364Z",
     "start_time": "2020-04-06T18:06:19.459264Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Bill Gates, who urged world leaders to prepare for a pandemic in 2015, lays out a 3-point plan on how the US can emerge victorious against COVID-19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívejme se, co všechno se o jednotlivých tokenech můžeme dozvědět. Kromě již diskutovaných věcí se zde například nalézá pos_ atribut a tag_. atribut. První z nich je jednoduchý part-of-speech tag, druhý obsahuje tagování detailní."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:06:21.618790Z",
     "start_time": "2020-04-06T18:06:21.603832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Bill PROPN NNP compound Xxxx True False\n",
      "Gates Gates PROPN NNP nsubj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "who who PRON WP nsubj xxx True True\n",
      "urged urge VERB VBD relcl xxxx True False\n",
      "world world NOUN NN compound xxxx True False\n",
      "leaders leader NOUN NNS dobj xxxx True False\n",
      "to to PART TO aux xx True True\n",
      "prepare prepare VERB VB xcomp xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "a a DET DT det x True True\n",
      "pandemic pandemic NOUN NN pobj xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "2015 2015 NUM CD pobj dddd False False\n",
      ", , PUNCT , punct , False False\n",
      "lays lay VERB VBZ ROOT xxxx True False\n",
      "out out ADP RP prt xxx True True\n",
      "a a DET DT det x True True\n",
      "3-point 3-point NUM CD nummod d-xxxx False False\n",
      "plan plan NOUN NN dobj xxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "how how ADV WRB advmod xxx True True\n",
      "the the DET DT det xxx True True\n",
      "US US PROPN NNP nsubj XX True True\n",
      "can can VERB MD aux xxx True True\n",
      "emerge emerge VERB VB pcomp xxxx True False\n",
      "victorious victorious ADJ JJ acomp xxxx True False\n",
      "against against ADP IN prep xxxx True True\n",
      "COVID-19 covid-19 NOUN NN pobj XXXX-dd False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co to vlastně znamená? Podívejme se například na slovo \"Bill\". Zde pos_ = PROPN a tag_ = NNP. Použijeme funkci explain, abchom se dozvěděli, jaký význam tyto zkratky vlastně mají."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T18:19:38.103522Z",
     "start_time": "2020-04-09T18:19:38.093516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"PROPN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T18:19:49.801046Z",
     "start_time": "2020-04-09T18:19:49.796096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"NNP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zde je třeba vědět, že proper noun je podstatné jméno označující jednu konkrétní entitu (tj. je jím např. řetězec \"Azor\", ale nikoli \"pes\"). Vidímě, že tag_ přinesl navíc informaci o tom, zda se jedná o jednotné či množné číslo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Další zajímavou informaci přináší token.dep_. Tento atribut ukazuje, jaký je vztah mezi slovy ve větě. Asi nejjednoduší je demonstrovat to pomocí obrázku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-06T18:06:46.444335Z",
     "start_time": "2020-04-06T18:06:33.310557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\newnotebook\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py:193: UserWarning:\n",
      "\n",
      "[W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"35ddcc295d72459baa49804e0b4c6233-0\" class=\"displacy\" width=\"4775\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Bill</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Gates,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">who</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">urged</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">world</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">leaders</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">prepare</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">pandemic</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">2015,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">lays</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">out</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">3-point</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">plan</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">how</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">US</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">can</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">emerge</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">victorious</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">against</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4600\">COVID-19</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4600\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,2.0 2325.0,2.0 2325.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,441.5 L412,429.5 428,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-3\" stroke-width=\"2px\" d=\"M245,439.5 C245,264.5 560.0,264.5 560.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560.0,441.5 L568.0,429.5 552.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-4\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-5\" stroke-width=\"2px\" d=\"M595,439.5 C595,264.5 910.0,264.5 910.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,441.5 L918.0,429.5 902.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-6\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,352.0 1255.0,352.0 1255.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-7\" stroke-width=\"2px\" d=\"M595,439.5 C595,177.0 1265.0,177.0 1265.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,441.5 L1273.0,429.5 1257.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-8\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,352.0 1430.0,352.0 1430.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1430.0,441.5 L1438.0,429.5 1422.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-9\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,352.0 1780.0,352.0 1780.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,441.5 L1637,429.5 1653,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-10\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,264.5 1785.0,264.5 1785.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1785.0,441.5 L1793.0,429.5 1777.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-11\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,177.0 1965.0,177.0 1965.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1965.0,441.5 L1973.0,429.5 1957.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-12\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,352.0 2130.0,352.0 2130.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2130.0,441.5 L2138.0,429.5 2122.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-13\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,352.0 2480.0,352.0 2480.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prt</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2480.0,441.5 L2488.0,429.5 2472.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-14\" stroke-width=\"2px\" d=\"M2695,439.5 C2695,264.5 3010.0,264.5 3010.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2695,441.5 L2687,429.5 2703,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-15\" stroke-width=\"2px\" d=\"M2870,439.5 C2870,352.0 3005.0,352.0 3005.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,441.5 L2862,429.5 2878,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-16\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,177.0 3015.0,177.0 3015.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3015.0,441.5 L3023.0,429.5 3007.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-17\" stroke-width=\"2px\" d=\"M3045,439.5 C3045,352.0 3180.0,352.0 3180.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3180.0,441.5 L3188.0,429.5 3172.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-18\" stroke-width=\"2px\" d=\"M3395,439.5 C3395,177.0 4065.0,177.0 4065.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3395,441.5 L3387,429.5 3403,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-19\" stroke-width=\"2px\" d=\"M3570,439.5 C3570,352.0 3705.0,352.0 3705.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3570,441.5 L3562,429.5 3578,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-20\" stroke-width=\"2px\" d=\"M3745,439.5 C3745,264.5 4060.0,264.5 4060.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3745,441.5 L3737,429.5 3753,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-21\" stroke-width=\"2px\" d=\"M3920,439.5 C3920,352.0 4055.0,352.0 4055.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3920,441.5 L3912,429.5 3928,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-22\" stroke-width=\"2px\" d=\"M3220,439.5 C3220,89.5 4070.0,89.5 4070.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4070.0,441.5 L4078.0,429.5 4062.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-23\" stroke-width=\"2px\" d=\"M4095,439.5 C4095,352.0 4230.0,352.0 4230.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4230.0,441.5 L4238.0,429.5 4222.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-24\" stroke-width=\"2px\" d=\"M4095,439.5 C4095,264.5 4410.0,264.5 4410.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4410.0,441.5 L4418.0,429.5 4402.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35ddcc295d72459baa49804e0b4c6233-0-25\" stroke-width=\"2px\" d=\"M4445,439.5 C4445,352.0 4580.0,352.0 4580.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35ddcc295d72459baa49804e0b4c6233-0-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4580.0,441.5 L4588.0,429.5 4572.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stejnou operaci můžeme udělat i po tzv. \"noun chunks\" - zkrátka po správně oddělených částech vět."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T19:10:47.074525Z",
     "start_time": "2020-04-01T19:10:47.068536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Gates Gates nsubj lays\n",
      "who who nsubj urged\n",
      "world leaders leaders dobj urged\n",
      "a pandemic pandemic pobj for\n",
      "a 3-point plan plan dobj lays\n",
      "the US US nsubj emerge\n",
      "COVID-19 COVID-19 pobj against\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak je z nadpisu asi patrné, spacy dokáže rozpoznat typy entit v dokumentu. Entita není vlastnost jednotlivých tokenů, nýbrž se jedná o vlastnost dokumentu. Jelikož to spacy dokáže díky statistice nad modely a silně závisí na příkladech, na kterých byly tyto modely vytvořeny, je nutné v případě problému modely doupravit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T19:15:21.409758Z",
     "start_time": "2020-04-01T19:15:21.404737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill Gates 0 10 PERSON\n",
      "2015 65 69 DATE\n",
      "US 106 108 GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T19:15:43.441385Z",
     "start_time": "2020-04-01T19:15:43.436432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T18:25:54.368058Z",
     "start_time": "2020-04-09T18:25:54.362074Z"
    }
   },
   "source": [
    "Entity si můžeme dokonce v textu graficky zvýraznit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T19:18:03.691870Z",
     "start_time": "2020-04-01T19:17:58.162645Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\newnotebook\\appdata\\local\\programs\\python\\python37\\lib\\runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bill Gates\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", who urged world leaders to prepare for a pandemic in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2015\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", lays out a 3-point plan on how the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " can emerge victorious against COVID-19</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lze vytvořit i nový typ entit a ty pomocí nových příkladů trénováním do modelu dostat (více [zde](https://spacy.io/usage/training/#ner)). To ale vyžaduje mít pro určitý jazyk již připravený model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaké možnosti ale máme v češtině? Lze použít balíček *nametag*, který podobně jako morphodita pochází z matfyzu - viz http://lindat.mff.cuni.cz/services/nametag/. I tento balíček pro své fungování vyžaduje jazykový model a i tento model je pod non-commercial licencí.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T19:23:46.680336Z",
     "start_time": "2020-04-09T19:23:44.995244Z"
    }
   },
   "outputs": [],
   "source": [
    "from ufal.nametag import *\n",
    "ner = Ner.load(\"czech-cnec2.0-140304.ner\")\n",
    "tokenizer = ner.newTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T19:36:16.230449Z",
     "start_time": "2020-04-09T19:36:16.220476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity:  Václav Havel  Type:  P\n",
      "Entity:  Václav  Type:  pf\n",
      "Entity:  Havel  Type:  ps\n",
      "Entity:  České republiky  Type:  gc\n",
      "Entity:  Komerční banka  Type:  if\n",
      "Entity:  Václavského náměstí  Type:  gs\n"
     ]
    }
   ],
   "source": [
    "def sort_entities(entities):\n",
    "    return sorted(entities, key=lambda entity: (entity.start, -entity.length))\n",
    "\n",
    "text = \"Václav Havel byl prezident České republiky. Komerční banka měla sídlo poblíž Václavského náměstí.\"\n",
    "tokenizer.setText(text)\n",
    "forms = Forms()\n",
    "tokens = TokenRanges()\n",
    "entities = NamedEntities()\n",
    "sortedEntities = []\n",
    "openEntities = []\n",
    "while tokenizer.nextSentence(forms, tokens):\n",
    "    ner.recognize(forms, entities)\n",
    "    sortedEntities = sort_entities(entities)\n",
    "    for entity in sortedEntities:\n",
    "        starting_index = tokens[entity.start].start\n",
    "        ending_index = tokens[entity.start+entity.length-1].start + tokens[entity.start+entity.length-1].length\n",
    "        print(\"Entity: \", text[starting_index:ending_index], \" Type: \", entity.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z příkladu je mimo jiné vidět, že si *nametag* poradí i s tvary nepřevedenými do prvního pádu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existuje ještě jiný způsob, jak na českém textu NER realizovat. Jedná se o BERT-like knihovnu/framework DeepPavlov. Jak u něj rozpoznávání entit realizovat? Následující kód\n",
    "```python\n",
    "from deeppavlov import configs, build_model\n",
    "\n",
    "ner_model = build_model(configs.ner.ner_ontonotes_bert_mult)\n",
    "print(ner_model(['Bob Ross lived in Florida']))\n",
    "print(ner_model(['Hostinský Dominik Výčep, vlastník hospody Na Mýtince, má účet u Komerční banky a Monety.']))\n",
    "print(ner_model(['Firma Agrofert prý nyní už nepatří šéfovi ANO Andreji Babišovi.']))\n",
    "print(ner_model(['Josef Vyskočil zaplatil 55 Kč a poté 3. února 2020 získal produkt Můj Účet Plus.']))\n",
    "```\n",
    "vytvoří takovýto výstup:  \n",
    "[[['Bob', 'Ross', 'lived', 'in', 'Florida']], [['B-PERSON', 'I-PERSON', 'O', 'O', 'B-GPE']]]  \n",
    "[[['Hostinský', 'Dominik', 'Výčep', ',', 'vlastník', 'hospody', 'Na', 'Mýtince', ',', 'má', 'účet', 'u', 'Komerční', 'banky', 'a', 'Monety', '.']], [['B-GPE', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-FAC', 'I-FAC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-ORG', 'O']]]  \n",
    "[[['Firma', 'Agrofert', 'prý', 'nyní', 'už', 'nepatří', 'šéfovi', 'ANO', 'Andreji', 'Babišovi', '.']], [['O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-PERSON', 'I-PERSON', 'O']]]  \n",
    "[[['Josef', 'Vyskočil', 'zaplatil', '55', 'Kč', 'a', 'poté', '3', '.', 'února', '2020', 'získal', 'produkt', 'Můj', 'Účet', 'Plus', '.']], [['B-PERSON', 'I-PERSON', 'O', 'B-MONEY', 'I-MONEY', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O']]]  \n",
    "\n",
    "Co řádek, to dvojice původní tokenizovaná věta - list s rozepsáním entit. Vlastní tokeny zde mají i čárky a tečky. Je vidět, že multijazykový model si poradí součaně jak s angličtinou, tak s češtinou. Tokeny, které do žádné entity nepatří, jsou označeny písmenem O. Prefix B- značí počátek entity, I- její pokračování. Význam některých entit je zjevný, snad ale poznamenejme, že GPE značí země a města a FAC budovy. Vidíme, že jména osob, datumy i finanční částky jsou v příkladu výše odhadnuty správně. Slovo \"Hostinský\" bylo bohužel chybně označeno jako entita země/město. Naprosti tomu ANO a obě banky byly správně rozpoznány jako organizace.  \n",
    "Hlavní problém s DeepPalovem (a vlastně obecně s čímkoli založeném na BERTu) je hardwarová náročnost. A to i když člověk neplánuje model přeučovat a opravdu ho chce jenom použít. Několikagigový model se totiž celý musí načíst do RAMky. Jelikož mám na svém počítači jen 8 GB, tak se mi výše uvedený kód nepodařilo rozběhnout v Jupyteru a musel jsem ho spouštět jako samostatný pythoní skript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
